{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9aef9285-11e9-4f71-b799-8ed56270b41b",
   "metadata": {},
   "source": [
    "# Acquire (Web Scraping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14d1e5e3-b926-4734-aa0b-d7901a1f239d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# general imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# for webscraping\n",
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# local modules\n",
    "import acquire as a\n",
    "from env import github_token, github_username"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3a4b6a-5b6d-4756-ba6a-1a0e679bcae4",
   "metadata": {},
   "source": [
    "1. Import the get() function from the requests module, BeautifulSoup from bs4, and pandas.\n",
    "2. Assign the address of the web page to a variable named url.\n",
    "3. Request the server the content of the web page by using get(), and store the server’s response in the variable response.\n",
    "4. Print the response text to ensure you have an html page.\n",
    "5. Take a look at the actual web page contents and inspect the source to understand the structure a bit.\n",
    "6. Use BeautifulSoup to parse the HTML into a variable ('soup').\n",
    "7. Identify the key tags you need to extract the data you are looking for.\n",
    "8. Create a dataframe of the data desired.\n",
    "9. Run some summary stats and inspect the data to ensure you have what you wanted.\n",
    "10. Edit the data structure as needed, especially so that **one column has all the text you want included in this analysis**.\n",
    "11. Create a corpus of the column with the text you want to analyze.\n",
    "12. Store that corpus for use in a future notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cba5ee43-5104-4147-8802-932d374996dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_urls(code_language):\n",
    "    '''\n",
    "    This function takes in a code language name as a string literal'''\n",
    "    search_page = f'https://github.com/trending/{code_language}?since=daily&spoken_language_code=en'\n",
    "    headers = {\"Authorization\": f\"token {github_token}\", \"User-Agent\": github_username}\n",
    "    response = get(search_page, headers=headers)\n",
    "    # print response\n",
    "    print(response)\n",
    "    if response.status_code // 100 == 2:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        urls = soup.find_all('h2')\n",
    "        REPOS = ['.' + url.find('a')['href'] for url in urls if url.find('a') is not None]\n",
    "        print(len(REPOS))\n",
    "        print(REPOS)\n",
    "        return REPOS\n",
    "    else:\n",
    "        print('There was a response error')\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93af5696-b52b-4b70-8b2b-d0ff7df504fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "25\n",
      "['./AUTOMATIC1111/stable-diffusion-webui', './bmaltais/kohya_ss', './donnemartin/system-design-primer', './Z4nzu/hackingtool', './ChanseyIsTheBest/NX-60FPS-RES-GFX-Cheats', './BlinkDL/RWKV-LM', './neonbjb/tortoise-tts', './neuml/txtai', './TapiocaFox/Daijishou', './coqui-ai/TTS', './OpenBB-finance/OpenBBTerminal', './d8ahazard/sd_dreambooth_extension', './catppuccin/gtk', './tinyvision/SOLIDER', './raspberrypi/usbboot', './sdatkinson/NeuralAmpModelerPlugin', './Zero6992/chatGPT-discord-bot', './pittcsc/Summer2023-Internships', './elebumm/RedditVideoMakerBot', './openai/whisper', './InstaPy/InstaPy', './rawandahmad698/PyChatGPT', './Dong-learn9/TVBox-zyjk', './StevenBlack/hosts', './riffusion/riffusion']\n"
     ]
    }
   ],
   "source": [
    "python_urls = get_urls('python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e63d2fca-9ff3-4d7d-a7a3-1da7fd617576",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'repo': './AUTOMATIC1111/stable-diffusion-webui',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '# Stable Diffusion web UI\\r\\nA browser interface based on Gradio library for Stable Diffusion.\\r\\n\\r\\n![](screenshot.png)\\r\\n\\r\\n## Features\\r\\n[Detailed feature showcase with images](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Features):\\r\\n- Original txt2img and img2img modes\\r\\n- One click install and run script (but you still must install python and git)\\r\\n- Outpainting\\r\\n- Inpainting\\r\\n- Color Sketch\\r\\n- Prompt Matrix\\r\\n- Stable Diffusion Upscale\\r\\n- Attention, specify parts of text that the model should pay more attention to\\r\\n    - a man in a `((tuxedo))` - will pay more attention to tuxedo\\r\\n    - a man in a `(tuxedo:1.21)` - alternative syntax\\r\\n    - select text and press `Ctrl+Up` or `Ctrl+Down` to automatically adjust attention to selected text (code contributed by anonymous user)\\r\\n- Loopback, run img2img processing multiple times\\r\\n- X/Y/Z plot, a way to draw a 3 dimensional plot of images with different parameters\\r\\n- Textual Inversion\\r\\n    - have as many embeddings as you want and use any names you like for them\\r\\n    - use multiple embeddings with different numbers of vectors per token\\r\\n    - works with half precision floating point numbers\\r\\n    - train embeddings on 8GB (also reports of 6GB working)\\r\\n- Extras tab with:\\r\\n    - GFPGAN, neural network that fixes faces\\r\\n    - CodeFormer, face restoration tool as an alternative to GFPGAN\\r\\n    - RealESRGAN, neural network upscaler\\r\\n    - ESRGAN, neural network upscaler with a lot of third party models\\r\\n    - SwinIR and Swin2SR ([see here](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/2092)), neural network upscalers\\r\\n    - LDSR, Latent diffusion super resolution upscaling\\r\\n- Resizing aspect ratio options\\r\\n- Sampling method selection\\r\\n    - Adjust sampler eta values (noise multiplier)\\r\\n    - More advanced noise setting options\\r\\n- Interrupt processing at any time\\r\\n- 4GB video card support (also reports of 2GB working)\\r\\n- Correct seeds for batches\\r\\n- Live prompt token length validation\\r\\n- Generation parameters\\r\\n     - parameters you used to generate images are saved with that image\\r\\n     - in PNG chunks for PNG, in EXIF for JPEG\\r\\n     - can drag the image to PNG info tab to restore generation parameters and automatically copy them into UI\\r\\n     - can be disabled in settings\\r\\n     - drag and drop an image/text-parameters to promptbox\\r\\n- Read Generation Parameters Button, loads parameters in promptbox to UI\\r\\n- Settings page\\r\\n- Running arbitrary python code from UI (must run with `--allow-code` to enable)\\r\\n- Mouseover hints for most UI elements\\r\\n- Possible to change defaults/mix/max/step values for UI elements via text config\\r\\n- Tiling support, a checkbox to create images that can be tiled like textures\\r\\n- Progress bar and live image generation preview\\r\\n    - Can use a separate neural network to produce previews with almost none VRAM or compute requirement\\r\\n- Negative prompt, an extra text field that allows you to list what you don\\'t want to see in generated image\\r\\n- Styles, a way to save part of prompt and easily apply them via dropdown later\\r\\n- Variations, a way to generate same image but with tiny differences\\r\\n- Seed resizing, a way to generate same image but at slightly different resolution\\r\\n- CLIP interrogator, a button that tries to guess prompt from an image\\r\\n- Prompt Editing, a way to change prompt mid-generation, say to start making a watermelon and switch to anime girl midway\\r\\n- Batch Processing, process a group of files using img2img\\r\\n- Img2img Alternative, reverse Euler method of cross attention control\\r\\n- Highres Fix, a convenience option to produce high resolution pictures in one click without usual distortions\\r\\n- Reloading checkpoints on the fly\\r\\n- Checkpoint Merger, a tab that allows you to merge up to 3 checkpoints into one\\r\\n- [Custom scripts](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Custom-Scripts) with many extensions from community\\r\\n- [Composable-Diffusion](https://energy-based-model.github.io/Compositional-Visual-Generation-with-Composable-Diffusion-Models/), a way to use multiple prompts at once\\r\\n     - separate prompts using uppercase `AND`\\r\\n     - also supports weights for prompts: `a cat :1.2 AND a dog AND a penguin :2.2`\\r\\n- No token limit for prompts (original stable diffusion lets you use up to 75 tokens)\\r\\n- DeepDanbooru integration, creates danbooru style tags for anime prompts\\r\\n- [xformers](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Xformers), major speed increase for select cards: (add `--xformers` to commandline args)\\r\\n- via extension: [History tab](https://github.com/yfszzx/stable-diffusion-webui-images-browser): view, direct and delete images conveniently within the UI\\r\\n- Generate forever option\\r\\n- Training tab\\r\\n     - hypernetworks and embeddings options\\r\\n     - Preprocessing images: cropping, mirroring, autotagging using BLIP or deepdanbooru (for anime)\\r\\n- Clip skip\\r\\n- Hypernetworks\\r\\n- Loras (same as Hypernetworks but more pretty)\\r\\n- A sparate UI where you can choose, with preview, which embeddings, hypernetworks or Loras to add to your prompt \\r\\n- Can select to load a different VAE from settings screen\\r\\n- Estimated completion time in progress bar\\r\\n- API\\r\\n- Support for dedicated [inpainting model](https://github.com/runwayml/stable-diffusion#inpainting-with-stable-diffusion) by RunwayML\\r\\n- via extension: [Aesthetic Gradients](https://github.com/AUTOMATIC1111/stable-diffusion-webui-aesthetic-gradients), a way to generate images with a specific aesthetic by using clip images embeds (implementation of [https://github.com/vicgalle/stable-diffusion-aesthetic-gradients](https://github.com/vicgalle/stable-diffusion-aesthetic-gradients))\\r\\n- [Stable Diffusion 2.0](https://github.com/Stability-AI/stablediffusion) support - see [wiki](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Features#stable-diffusion-20) for instructions\\r\\n- [Alt-Diffusion](https://arxiv.org/abs/2211.06679) support - see [wiki](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Features#alt-diffusion) for instructions\\r\\n- Now without any bad letters!\\r\\n- Load checkpoints in safetensors format\\r\\n- Eased resolution restriction: generated image\\'s domension must be a multiple of 8 rather than 64\\r\\n- Now with a license!\\r\\n- Reorder elements in the UI from settings screen\\r\\n\\r\\n## Installation and Running\\r\\nMake sure the required [dependencies](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Dependencies) are met and follow the instructions available for both [NVidia](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Install-and-Run-on-NVidia-GPUs) (recommended) and [AMD](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Install-and-Run-on-AMD-GPUs) GPUs.\\r\\n\\r\\nAlternatively, use online services (like Google Colab):\\r\\n\\r\\n- [List of Online Services](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Online-Services)\\r\\n\\r\\n### Automatic Installation on Windows\\r\\n1. Install [Python 3.10.6](https://www.python.org/downloads/release/python-3106/) (Newer version of Python does not support torch), checking \"Add Python to PATH\".\\r\\n2. Install [git](https://git-scm.com/download/win).\\r\\n3. Download the stable-diffusion-webui repository, for example by running `git clone https://github.com/AUTOMATIC1111/stable-diffusion-webui.git`.\\r\\n4. Run `webui-user.bat` from Windows Explorer as normal, non-administrator, user.\\r\\n\\r\\n### Automatic Installation on Linux\\r\\n1. Install the dependencies:\\r\\n```bash\\r\\n# Debian-based:\\r\\nsudo apt install wget git python3 python3-venv\\r\\n# Red Hat-based:\\r\\nsudo dnf install wget git python3\\r\\n# Arch-based:\\r\\nsudo pacman -S wget git python3\\r\\n```\\r\\n2. Navigate to the directory you would like the webui to be installed and execute the following command:\\r\\n```bash\\r\\nbash <(wget -qO- https://raw.githubusercontent.com/AUTOMATIC1111/stable-diffusion-webui/master/webui.sh)\\r\\n```\\r\\n3. Run `webui.sh`.\\r\\n4. Check `webui-user.sh` for options.\\r\\n### Installation on Apple Silicon\\r\\n\\r\\nFind the instructions [here](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Installation-on-Apple-Silicon).\\r\\n\\r\\n## Contributing\\r\\nHere\\'s how to add code to this repo: [Contributing](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Contributing)\\r\\n\\r\\n## Documentation\\r\\nThe documentation was moved from this README over to the project\\'s [wiki](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki).\\r\\n\\r\\n## Credits\\r\\nLicenses for borrowed code can be found in `Settings -> Licenses` screen, and also in `html/licenses.html` file.\\r\\n\\r\\n- Stable Diffusion - https://github.com/CompVis/stable-diffusion, https://github.com/CompVis/taming-transformers\\r\\n- k-diffusion - https://github.com/crowsonkb/k-diffusion.git\\r\\n- GFPGAN - https://github.com/TencentARC/GFPGAN.git\\r\\n- CodeFormer - https://github.com/sczhou/CodeFormer\\r\\n- ESRGAN - https://github.com/xinntao/ESRGAN\\r\\n- SwinIR - https://github.com/JingyunLiang/SwinIR\\r\\n- Swin2SR - https://github.com/mv-lab/swin2sr\\r\\n- LDSR - https://github.com/Hafiidz/latent-diffusion\\r\\n- MiDaS - https://github.com/isl-org/MiDaS\\r\\n- Ideas for optimizations - https://github.com/basujindal/stable-diffusion\\r\\n- Cross Attention layer optimization - Doggettx - https://github.com/Doggettx/stable-diffusion, original idea for prompt editing.\\r\\n- Cross Attention layer optimization - InvokeAI, lstein - https://github.com/invoke-ai/InvokeAI (originally http://github.com/lstein/stable-diffusion)\\r\\n- Sub-quadratic Cross Attention layer optimization - Alex Birch (https://github.com/Birch-san/diffusers/pull/1), Amin Rezaei (https://github.com/AminRezaei0x443/memory-efficient-attention)\\r\\n- Textual Inversion - Rinon Gal - https://github.com/rinongal/textual_inversion (we\\'re not using his code, but we are using his ideas).\\r\\n- Idea for SD upscale - https://github.com/jquesnelle/txt2imghd\\r\\n- Noise generation for outpainting mk2 - https://github.com/parlance-zz/g-diffuser-bot\\r\\n- CLIP interrogator idea and borrowing some code - https://github.com/pharmapsychotic/clip-interrogator\\r\\n- Idea for Composable Diffusion - https://github.com/energy-based-model/Compositional-Visual-Generation-with-Composable-Diffusion-Models-PyTorch\\r\\n- xformers - https://github.com/facebookresearch/xformers\\r\\n- DeepDanbooru - interrogator for anime diffusers https://github.com/KichangKim/DeepDanbooru\\r\\n- Sampling in float32 precision from a float16 UNet - marunine for the idea, Birch-san for the example Diffusers implementation (https://github.com/Birch-san/diffusers-play/tree/92feee6)\\r\\n- Instruct pix2pix - Tim Brooks (star), Aleksander Holynski (star), Alexei A. Efros (no star) - https://github.com/timothybrooks/instruct-pix2pix\\r\\n- Security advice - RyotaK\\r\\n- UniPC sampler - Wenliang Zhao - https://github.com/wl-zhao/UniPC\\r\\n- Initial Gradio script - posted on 4chan by an Anonymous user. Thank you Anonymous user.\\r\\n- (You)\\r\\n'},\n",
       " {'repo': './bmaltais/kohya_ss',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '## リポジトリについて\\nStable Diffusionの学習、画像生成、その他のスクリプトを入れたリポジトリです。\\n\\n[README in English](./README.md) ←更新情報はこちらにあります\\n\\nGUIやPowerShellスクリプトなど、より使いやすくする機能が[bmaltais氏のリポジトリ](https://github.com/bmaltais/kohya_ss)で提供されています（英語です）のであわせてご覧ください。bmaltais氏に感謝します。\\n\\n以下のスクリプトがあります。\\n\\n* DreamBooth、U-NetおよびText Encoderの学習をサポート\\n* fine-tuning、同上\\n* 画像生成\\n* モデル変換（Stable Diffision ckpt/safetensorsとDiffusersの相互変換）\\n\\n## 使用法について\\n\\n当リポジトリ内およびnote.comに記事がありますのでそちらをご覧ください（将来的にはすべてこちらへ移すかもしれません）。\\n\\n* [学習について、共通編](./docs/train_README-ja.md) : データ整備やオプションなど\\n    * [データセット設定](./docs/config_README-ja.md)\\n* [DreamBoothの学習について](./docs/train_db_README-ja.md)\\n* [fine-tuningのガイド](./docs/fine_tune_README_ja.md):\\n* [LoRAの学習について](./docs/train_network_README-ja.md)\\n* [Textual Inversionの学習について](./docs/train_ti_README-ja.md)\\n* [画像生成スクリプト](./docs/gen_img_README-ja.md)\\n* note.com [モデル変換スクリプト](https://note.com/kohya_ss/n/n374f316fe4ad)\\n\\n## Windowsでの動作に必要なプログラム\\n\\nPython 3.10.6およびGitが必要です。\\n\\n- Python 3.10.6: https://www.python.org/ftp/python/3.10.6/python-3.10.6-amd64.exe\\n- git: https://git-scm.com/download/win\\n\\nPowerShellを使う場合、venvを使えるようにするためには以下の手順でセキュリティ設定を変更してください。\\n（venvに限らずスクリプトの実行が可能になりますので注意してください。）\\n\\n- PowerShellを管理者として開きます。\\n- 「Set-ExecutionPolicy Unrestricted」と入力し、Yと答えます。\\n- 管理者のPowerShellを閉じます。\\n\\n## Windows環境でのインストール\\n\\n以下の例ではPyTorchは1.12.1／CUDA 11.6版をインストールします。CUDA 11.3版やPyTorch 1.13を使う場合は適宜書き換えください。\\n\\n（なお、python -m venv～の行で「python」とだけ表示された場合、py -m venv～のようにpythonをpyに変更してください。）\\n\\n通常の（管理者ではない）PowerShellを開き以下を順に実行します。\\n\\n```powershell\\ngit clone https://github.com/kohya-ss/sd-scripts.git\\ncd sd-scripts\\n\\npython -m venv venv\\n.\\\\venv\\\\Scripts\\\\activate\\n\\npip install torch==1.12.1+cu116 torchvision==0.13.1+cu116 --extra-index-url https://download.pytorch.org/whl/cu116\\npip install --upgrade -r requirements.txt\\npip install -U -I --no-deps https://github.com/C43H66N12O12S2/stable-diffusion-webui/releases/download/f/xformers-0.0.14.dev0-cp310-cp310-win_amd64.whl\\n\\ncp .\\\\bitsandbytes_windows\\\\*.dll .\\\\venv\\\\Lib\\\\site-packages\\\\bitsandbytes\\\\\\ncp .\\\\bitsandbytes_windows\\\\cextension.py .\\\\venv\\\\Lib\\\\site-packages\\\\bitsandbytes\\\\cextension.py\\ncp .\\\\bitsandbytes_windows\\\\main.py .\\\\venv\\\\Lib\\\\site-packages\\\\bitsandbytes\\\\cuda_setup\\\\main.py\\n\\naccelerate config\\n```\\n\\n<!-- \\npip install torch==1.13.1+cu117 torchvision==0.14.1+cu117 --extra-index-url https://download.pytorch.org/whl/cu117\\npip install --use-pep517 --upgrade -r requirements.txt\\npip install -U -I --no-deps xformers==0.0.16\\n-->\\n\\nコマンドプロンプトでは以下になります。\\n\\n\\n```bat\\ngit clone https://github.com/kohya-ss/sd-scripts.git\\ncd sd-scripts\\n\\npython -m venv venv\\n.\\\\venv\\\\Scripts\\\\activate\\n\\npip install torch==1.12.1+cu116 torchvision==0.13.1+cu116 --extra-index-url https://download.pytorch.org/whl/cu116\\npip install --upgrade -r requirements.txt\\npip install -U -I --no-deps https://github.com/C43H66N12O12S2/stable-diffusion-webui/releases/download/f/xformers-0.0.14.dev0-cp310-cp310-win_amd64.whl\\n\\ncopy /y .\\\\bitsandbytes_windows\\\\*.dll .\\\\venv\\\\Lib\\\\site-packages\\\\bitsandbytes\\\\\\ncopy /y .\\\\bitsandbytes_windows\\\\cextension.py .\\\\venv\\\\Lib\\\\site-packages\\\\bitsandbytes\\\\cextension.py\\ncopy /y .\\\\bitsandbytes_windows\\\\main.py .\\\\venv\\\\Lib\\\\site-packages\\\\bitsandbytes\\\\cuda_setup\\\\main.py\\n\\naccelerate config\\n```\\n\\n（注:``python -m venv venv`` のほうが ``python -m venv --system-site-packages venv`` より安全そうなため書き換えました。globalなpythonにパッケージがインストールしてあると、後者だといろいろと問題が起きます。）\\n\\naccelerate configの質問には以下のように答えてください。（bf16で学習する場合、最後の質問にはbf16と答えてください。）\\n\\n※0.15.0から日本語環境では選択のためにカーソルキーを押すと落ちます（……）。数字キーの0、1、2……で選択できますので、そちらを使ってください。\\n\\n```txt\\n- This machine\\n- No distributed training\\n- NO\\n- NO\\n- NO\\n- all\\n- fp16\\n```\\n\\n※場合によって ``ValueError: fp16 mixed precision requires a GPU`` というエラーが出ることがあるようです。この場合、6番目の質問（\\n``What GPU(s) (by id) should be used for training on this machine as a comma-separated list? [all]:``）に「0」と答えてください。（id `0`のGPUが使われます。）\\n\\n### PyTorchとxformersのバージョンについて\\n\\n他のバージョンでは学習がうまくいかない場合があるようです。特に他の理由がなければ指定のバージョンをお使いください。\\n\\n### オプション：Lion8bitを使う\\n\\nLion8bitを使う場合には`bitsandbytes`を0.38.0以降にアップグレードする必要があります。`bitsandbytes`をアンインストールし、Windows環境では例えば[こちら](https://github.com/jllllll/bitsandbytes-windows-webui)などからWindows版のwhlファイルをインストールしてください。たとえば以下のような手順になります。\\n\\n```powershell\\npip install https://github.com/jllllll/bitsandbytes-windows-webui/raw/main/bitsandbytes-0.38.1-py3-none-any.whl\\n```\\n\\nアップグレード時には`pip install .`でこのリポジトリを更新し、必要に応じて他のパッケージもアップグレードしてください。\\n\\n## アップグレード\\n\\n新しいリリースがあった場合、以下のコマンドで更新できます。\\n\\n```powershell\\ncd sd-scripts\\ngit pull\\n.\\\\venv\\\\Scripts\\\\activate\\npip install --use-pep517 --upgrade -r requirements.txt\\n```\\n\\nコマンドが成功すれば新しいバージョンが使用できます。\\n\\n## 謝意\\n\\nLoRAの実装は[cloneofsimo氏のリポジトリ](https://github.com/cloneofsimo/lora)を基にしたものです。感謝申し上げます。\\n\\nConv2d 3x3への拡大は [cloneofsimo氏](https://github.com/cloneofsimo/lora) が最初にリリースし、KohakuBlueleaf氏が [LoCon](https://github.com/KohakuBlueleaf/LoCon) でその有効性を明らかにしたものです。KohakuBlueleaf氏に深く感謝します。\\n\\n## ライセンス\\n\\nスクリプトのライセンスはASL 2.0ですが（Diffusersおよびcloneofsimo氏のリポジトリ由来のものも同様）、一部他のライセンスのコードを含みます。\\n\\n[Memory Efficient Attention Pytorch](https://github.com/lucidrains/memory-efficient-attention-pytorch): MIT\\n\\n[bitsandbytes](https://github.com/TimDettmers/bitsandbytes): MIT\\n\\n[BLIP](https://github.com/salesforce/BLIP): BSD-3-Clause\\n\\n\\n'},\n",
       " {'repo': './donnemartin/system-design-primer',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '*[English](README.md) ∙ [日本語](README-ja.md) ∙ [简体中文](README-zh-Hans.md) ∙ [繁體中文](README-zh-TW.md) | [العَرَبِيَّة\\u200e](https://github.com/donnemartin/system-design-primer/issues/170) ∙ [বাংলা](https://github.com/donnemartin/system-design-primer/issues/220) ∙ [Português do Brasil](https://github.com/donnemartin/system-design-primer/issues/40) ∙ [Deutsch](https://github.com/donnemartin/system-design-primer/issues/186) ∙ [ελληνικά](https://github.com/donnemartin/system-design-primer/issues/130) ∙ [עברית](https://github.com/donnemartin/system-design-primer/issues/272) ∙ [Italiano](https://github.com/donnemartin/system-design-primer/issues/104) ∙ [한국어](https://github.com/donnemartin/system-design-primer/issues/102) ∙ [فارسی](https://github.com/donnemartin/system-design-primer/issues/110) ∙ [Polski](https://github.com/donnemartin/system-design-primer/issues/68) ∙ [русский язык](https://github.com/donnemartin/system-design-primer/issues/87) ∙ [Español](https://github.com/donnemartin/system-design-primer/issues/136) ∙ [ภาษาไทย](https://github.com/donnemartin/system-design-primer/issues/187) ∙ [Türkçe](https://github.com/donnemartin/system-design-primer/issues/39) ∙ [tiếng Việt](https://github.com/donnemartin/system-design-primer/issues/127) ∙ [Français](https://github.com/donnemartin/system-design-primer/issues/250) | [Add Translation](https://github.com/donnemartin/system-design-primer/issues/28)*\\n\\n# システム設計入門\\n\\n<p align=\"center\">\\n  <img src=\"images/jj3A5N8.png\">\\n  <br/>\\n</p>\\n\\n## 動機・目的\\n\\n> 大規模システムのシステム設計を学ぶ\\n>\\n> システム設計面接課題に備える\\n\\n### 大規模システムの設計を学ぶ\\n\\nスケーラブルなシステムのシステム設計を学ぶことは、より良いエンジニアになることに資するでしょう。\\n\\nシステム設計はとても広範なトピックを含みます。システム設計原理については **インターネット上には膨大な量の文献が散らばっています。**\\n\\nこのリポジトリは大規模システム構築に必要な知識を学ぶことができる **文献リストを体系的にまとめたもの** です。\\n\\n### オープンソースコミュニティから学ぶ\\n\\nこのプロジェクトは、これからもずっと更新されていくオープンソースプロジェクトの初期段階にすぎません。\\n\\n[Contributions](#contributing) は大歓迎です！\\n\\n### システム設計面接課題に備える\\n\\nコード技術面接に加えて、システム設計に関する知識は、多くのテック企業における **技術採用面接プロセス** で **必要不可欠な要素** です。\\n\\n**システム設計面接での頻出質問に備え**、自分の解答と*模範解答*:ディスカッション、コードそして図表などを*比較*して学びましょう。\\n\\n面接準備に役立つその他のトピック:\\n\\n* [学習指針](#学習指針)\\n* [システム設計面接課題にどのように準備するか](#システム設計面接にどのようにして臨めばいいか)\\n* [システム設計課題例 **とその解答**](#システム設計課題例とその解答)\\n* [オブジェクト指向設計課題例、 **とその解答**](#オブジェクト指向設計問題と解答)\\n* [その他のシステム設計面接課題例](#他のシステム設計面接例題)\\n\\n## 暗記カード\\n\\n<p align=\"center\">\\n  <img src=\"images/zdCAkB3.png\">\\n  <br/>\\n</p>\\n\\nこの[Anki用フラッシュカードデッキ](https://apps.ankiweb.net/) は、間隔反復を活用して、システム設計のキーコンセプトの学習を支援します。\\n\\n* [システム設計デッキ](resources/flash_cards/System%20Design.apkg)\\n* [システム設計練習課題デッキ](resources/flash_cards/System%20Design%20Exercises.apkg)\\n* [オブジェクト指向練習課題デッキ](resources/flash_cards/OO%20Design.apkg)\\n\\n外出先や移動中の勉強に役立つでしょう。\\n\\n### コーディング技術課題用の問題: 練習用インタラクティブアプリケーション\\n\\nコード技術面接用の問題を探している場合は[**こちら**](https://github.com/donnemartin/interactive-coding-challenges)\\n\\n<p align=\"center\">\\n  <img src=\"images/b4YtAEN.png\">\\n  <br/>\\n</p>\\n\\n姉妹リポジトリの [**Interactive Coding Challenges**](https://github.com/donnemartin/interactive-coding-challenges)も見てみてください。追加の暗記デッキカードも入っています。\\n\\n* [Coding deck](https://github.com/donnemartin/interactive-coding-challenges/tree/master/anki_cards/Coding.apkg)\\n\\n## コントリビュート\\n\\n> コミュニティから学ぶ\\n\\nプルリクエスト等の貢献は積極的にお願いします:\\n\\n* エラー修正\\n* セクション内容改善\\n* 新規セクション追加\\n* [翻訳する](https://github.com/donnemartin/system-design-primer/issues/28)\\n\\n現在、内容の改善が必要な作業中のコンテンツは[こちら](#進行中の作業)です。\\n\\nコントリビュートの前に[Contributing Guidelines](CONTRIBUTING.md)を読みましょう。\\n\\n## システム設計目次\\n\\n> 賛否も含めた様々なシステム設計の各トピックの概要。 **全てはトレードオフの関係にあります。**\\n>\\n> それぞれのセクションはより学びを深めるような他の文献へのリンクが貼られています。\\n\\n<p align=\"center\">\\n  <img src=\"images/jrUBAF7.png\">\\n  <br/>\\n</p>\\n\\n* [システム設計トピック: まずはここから](#システム設計トピックス-まずはここから)\\n    * [Step 1: スケーラビリティに関する動画を見る](#ステップ-1-スケーラビリティに関する動画を観て復習する)\\n    * [Step 2: スケーラビリティに関する記事を読む](#ステップ-2-スケーラビリティに関する資料を読んで復習する)\\n    * [次のステップ](#次のステップ)\\n* [パフォーマンス vs スケーラビリティ](#パフォーマンス-vs-スケーラビリティ)\\n* [レイテンシー vs スループット](#レイテンシー-vs-スループット)\\n* [可用性 vs 一貫性](#可用性-vs-一貫性)\\n    * [CAP理論](#cap-理論)\\n        * [CP - 一貫性(consistency)と分割性(partition)耐性](#cp---一貫性と分断耐性consistency-and-partition-tolerance)\\n        * [AP - 可用性(availability)と分割性(partition)耐性](#ap---可用性と分断耐性availability-and-partition-tolerance)\\n* [一貫性 パターン](#一貫性パターン)\\n    * [弱い一貫性](#弱い一貫性)\\n    * [結果整合性](#結果整合性)\\n    * [強い一貫性](#強い一貫性)\\n* [可用性 パターン](#可用性パターン)\\n    * [フェイルオーバー](#フェイルオーバー)\\n    * [レプリケーション](#レプリケーション)\\n* [ドメインネームシステム(DNS)](#ドメインネームシステム)\\n* [コンテンツデリバリーネットワーク(CDN)](#コンテンツデリバリーネットワークcontent-delivery-network)\\n    * [プッシュCDN](#プッシュcdn)\\n    * [プルCDN](#プルcdn)\\n* [ロードバランサー](#ロードバランサー)\\n    * [アクティブ/パッシブ構成](#アクティブパッシブ)\\n    * [アクティブ/アクティブ構成](#アクティブアクティブ)\\n    * [Layer 4 ロードバランシング](#layer-4-ロードバランシング)\\n    * [Layer 7 ロードバランシング](#layer-7-ロードバランシング)\\n    * [水平スケーリング](#水平スケーリング)\\n* [リバースプロキシ (WEBサーバー)](#リバースプロキシwebサーバー)\\n    * [ロードバランサー vs リバースプロキシ](#ロードバランサー-vs-リバースプロキシ)\\n* [アプリケーションレイヤー](#アプリケーション層)\\n    * [マイクロサービス](#マイクロサービス)\\n    * [サービスディスカバリー](#service-discovery)\\n* [データベース](#データベース)\\n    * [リレーショナルデータベースマネジメントシステム (RDBMS)](#リレーショナルデータベースマネジメントシステム-rdbms)\\n        * [マスター/スレーブ レプリケーション](#マスタースレーブ-レプリケーション)\\n        * [マスター/マスター レプリケーション](#マスターマスター-レプリケーション)\\n        * [フェデレーション](#federation)\\n        * [シャーディング](#シャーディング)\\n        * [デノーマライゼーション](#非正規化)\\n        * [SQL チューニング](#sqlチューニング)\\n    * [NoSQL](#nosql)\\n        * [キー/バリューストア](#キーバリューストア)\\n        * [ドキュメントストア](#ドキュメントストア)\\n        * [ワイドカラムストア](#ワイドカラムストア)\\n        * [グラフ データベース](#グラフデータベース)\\n    * [SQL or NoSQL](#sqlかnosqlか)\\n* [キャッシュ](#キャッシュ)\\n    * [クライアントキャッシング](#クライアントキャッシング)\\n    * [CDNキャッシング](#cdnキャッシング)\\n    * [Webサーバーキャッシング](#webサーバーキャッシング)\\n    * [データベースキャッシング](#データベースキャッシング)\\n    * [アプリケーションキャッシング](#アプリケーションキャッシング)\\n    * [データベースクエリレベルでキャッシングする](#データベースクエリレベルでのキャッシング)\\n    * [オブジェクトレベルでキャッシングする](#オブジェクトレベルでのキャッシング)\\n    * [いつキャッシュを更新するのか](#いつキャッシュを更新するか)\\n        * [キャッシュアサイド](#キャッシュアサイド)\\n        * [ライトスルー](#ライトスルー)\\n        * [ライトビハインド (ライトバック)](#ライトビハインド-ライトバック)\\n        * [リフレッシュアヘッド](#リフレッシュアヘッド)\\n* [非同期処理](#非同期処理)\\n    * [メッセージキュー](#メッセージキュー)\\n    * [タスクキュー](#タスクキュー)\\n    * [バックプレッシャー](#バックプレッシャー)\\n* [通信](#通信)\\n    * [伝送制御プロトコル (TCP)](#伝送制御プロトコル-tcp)\\n    * [ユーザデータグラムプロトコル (UDP)](#ユーザデータグラムプロトコル-udp)\\n    * [遠隔手続呼出 (RPC)](#遠隔手続呼出-rpc)\\n    * [Representational state transfer (REST)](#representational-state-transfer-rest)\\n* [セキュリティ](#セキュリティ)\\n* [補遺](#補遺)\\n    * [2の乗数表](#2の乗数表)\\n    * [全てのプログラマーが知るべきレイテンシー値](#全てのプログラマーが知るべきレイテンシー値)\\n    * [他のシステム設計面接例題](#他のシステム設計面接例題)\\n    * [実世界でのアーキテクチャ](#実世界のアーキテクチャ)\\n    * [各企業のアーキテクチャ](#各企業のアーキテクチャ)\\n    * [企業のエンジニアブログ](#企業のエンジニアブログ)\\n* [作業中](#進行中の作業)\\n* [クレジット](#クレジット)\\n* [連絡情報](#contact-info)\\n* [ライセンス](#license)\\n\\n## 学習指針\\n\\n> 学習スパンに応じてみるべきトピックス (short, medium, long)\\n\\n![Imgur](images/OfVllex.png)\\n\\n**Q: 面接のためには、ここにあるものすべてをやらないといけないのでしょうか？**\\n\\n**A: いえ、ここにあるすべてをやる必要はありません。**\\n\\n面接で何を聞かれるかは以下の条件によって変わってきます:\\n\\n* どれだけの技術経験があるか\\n* あなたの技術背景が何であるか\\n* どのポジションのために面接を受けているか\\n* どの企業の面接を受けているか\\n* 運\\n\\nより経験のある候補者は一般的にシステム設計についてより深い知識を有していることを要求されるでしょう。システムアーキテクトやチームリーダーは各メンバーの持つような知識よりは深い見識を持っているべきでしょう。一流テック企業では複数回の設計面接を課されることが多いです。\\n\\nまずは広く始めて、そこからいくつかの分野に絞って深めていくのがいいでしょう。様々なシステム設計のトピックについて少しずつ知っておくことはいいことです。以下の学習ガイドを自分の学習に当てられる時間、技術経験、どの職位、どの会社に応募しているかなどを加味して自分用に調整して使うといいでしょう。\\n\\n* **短期間** - **幅広く** システム設計トピックを学ぶ。**いくつかの** 面接課題を解くことで対策する。\\n* **中期間** - **幅広く** そして **それなりに深く**システム設計トピックを学ぶ。**多くの** 面接課題を解くことで対策する。\\n* **長期間** - **幅広く** そして **もっと深く**システム設計トピックを学ぶ。**ほぼ全ての** 面接課題を解くことで対策する。\\n\\n| | 短期間 | 中期間 | 長期間 |\\n|---|---|---|---|\\n| [システム設計トピック](#システム設計目次) を読み、システム動作機序について広く知る | :+1: | :+1: | :+1: |\\n| 次のリンク先のいくつかのページを読んで [各企業のエンジニアリングブログ](#企業のエンジニアブログ) 応募する会社について知る | :+1: | :+1: | :+1: |\\n| 次のリンク先のいくつかのページを読む [実世界でのアーキテクチャ](#実世界のアーキテクチャ) | :+1: | :+1: | :+1: |\\n| 復習する [システム設計面接課題にどのように準備するか](#システム設計面接にどのようにして臨めばいいか) | :+1: | :+1: | :+1: |\\n| とりあえず一周する [システム設計課題例](#システム設計課題例とその解答) | Some | Many | Most |\\n| とりあえず一周する [オブジェクト指向設計問題と解答](#オブジェクト指向設計問題と解答) | Some | Many | Most |\\n| 復習する [その他システム設計面接での質問例](#他のシステム設計面接例題) | Some | Many | Most |\\n\\n## システム設計面接にどのようにして臨めばいいか\\n\\n> システム設計面接試験問題にどのように取り組むか\\n\\nシステム設計面接は **open-ended conversation(Yes/Noでは答えられない口頭質問)です**。 自分で会話を組み立てることを求められます。\\n\\n以下のステップに従って議論を組み立てることができるでしょう。この過程を確かなものにするために、次のセクション[システム設計課題例とその解答](#system-design-interview-questions-with-solutions) を以下の指針に従って読み込むといいでしょう。\\n\\n### ステップ 1: そのシステム使用例の概要、制約、推計値等を聞き出し、まとめる\\n\\nシステム仕様の要求事項を聞き出し、問題箇所を特定しましょう。使用例と制約を明確にするための質問を投げかけましょう。要求する推計値についても議論しておきましょう。\\n\\n* 誰がそのサービスを使うのか？\\n* どのように使うのか？\\n* 何人のユーザーがいるのか？\\n* システムはどのような機能を果たすのか？\\n* システムへの入力と出力は？\\n* どれだけの容量のデータを捌く必要があるのか？\\n* 一秒間に何リクエストの送信が想定されるか？\\n* 読み書き比率の推定値はいくら程度か？\\n\\n### ステップ 2: より高レベルのシステム設計を組み立てる\\n\\n重要なコンポーネントを全て考慮した高レベルのシステム設計概要を組み立てる。\\n\\n* 主要なコンポーネントと接続をスケッチして書き出す\\n* 考えの裏付けをする\\n\\n### ステップ 3: 核となるコンポーネントを設計する\\n\\nそれぞれの主要なコンポーネントについての詳細を学ぶ。例えば、[url短縮サービス](solutions/system_design/pastebin/README.md)の設計を問われた際には次のようにするといいでしょう:\\n\\n* 元のURLのハッシュ化したものを作り、それを保存する\\n    * [MD5](solutions/system_design/pastebin/README.md) と [Base62](solutions/system_design/pastebin/README.md)\\n    * ハッシュ衝突\\n    * SQL もしくは NoSQL\\n    * データベーススキーマ\\n* ハッシュ化されたURLを元のURLに再翻訳する\\n    * データベース参照\\n* API & オブジェクト指向の設計\\n\\n### ステップ 4: システム設計のスケール\\n\\n与えられた制約条件からボトルネックとなりそうなところを割り出し、明確化する。  例えば、スケーラビリティの問題解決のために以下の要素を考慮する必要があるだろうか？\\n\\n* ロードバランサー\\n* 水平スケーリング\\n* キャッシング\\n* データベースシャーディング\\n\\n取りうる解決策とそのトレードオフについて議論をしよう。全てのことはトレードオフの関係にある。ボトルネックについては[スケーラブルなシステム設計の原理](#システム設計目次)を読むといいでしょう。\\n\\n### ちょっとした暗算問題\\n\\nちょっとした推計値を手計算ですることを求められることもあるかもしれません。[補遺](#補遺)の以下の項目が役に立つでしょう:\\n\\n* [チラ裏計算でシステム設計する](http://highscalability.com/blog/2011/1/26/google-pro-tip-use-back-of-the-envelope-calculations-to-choo.html)\\n* [2の乗数表](#2の乗数表)\\n* [全てのプログラマーが知っておくべきレイテンシの参考値](#全てのプログラマーが知るべきレイテンシー値)\\n\\n### 文献とその他の参考資料\\n\\n以下のリンク先ページを見てどのような質問を投げかけられるか概要を頭に入れておきましょう:\\n\\n* [システム設計面接で成功するには？](https://www.palantir.com/2011/10/how-to-rock-a-systems-design-interview/)\\n* [システム設計面接](http://www.hiredintech.com/system-design)\\n* [アーキテクチャ、システム設計面接への導入](https://www.youtube.com/watch?v=ZgdS0EUmn70)\\n\\n## システム設計課題例とその解答\\n\\n> 頻出のシステム設計面接課題と参考解答、コード及びダイアグラム\\n>\\n> 解答は `solutions/` フォルダ以下にリンクが貼られている\\n\\n| 問題 | |\\n|---|---|\\n| Pastebin.com (もしくは Bit.ly) を設計する| [解答](solutions/system_design/pastebin/README.md) |\\n| Twitterタイムライン (もしくはFacebookフィード)を設計する<br/>Twitter検索(もしくはFacebook検索)機能を設計する | [解答](solutions/system_design/twitter/README.md) |\\n| ウェブクローラーを設計する | [解答](solutions/system_design/web_crawler/README.md) |\\n| Mint.comを設計する | [解答](solutions/system_design/mint/README.md) |\\n| SNSサービスのデータ構造を設計する | [解答](solutions/system_design/social_graph/README.md) |\\n| 検索エンジンのキー/バリュー構造を設計する | [解答](solutions/system_design/query_cache/README.md) |\\n| Amazonのカテゴリ毎の売り上げランキングを設計する | [解答](solutions/system_design/sales_rank/README.md) |\\n| AWS上で100万人規模のユーザーを捌くサービスを設計する | [解答](solutions/system_design/scaling_aws/README.md) |\\n| システム設計問題を追加する | [Contribute](#contributing) |\\n\\n### Pastebin.com (もしくは Bit.ly) を設計する\\n\\n[問題と解答を見る](solutions/system_design/pastebin/README.md)\\n\\n![Imgur](images/4edXG0T.png)\\n\\n### Twitterタイムライン&検索 (もしくはFacebookフィード&検索)を設計する\\n\\n[問題と解答を見る](solutions/system_design/twitter/README.md)\\n\\n![Imgur](images/jrUBAF7.png)\\n\\n### ウェブクローラーの設計\\n\\n[問題と解答を見る](solutions/system_design/web_crawler/README.md)\\n\\n![Imgur](images/bWxPtQA.png)\\n\\n### Mint.comの設計\\n\\n[問題と解答を見る](solutions/system_design/mint/README.md)\\n\\n![Imgur](images/V5q57vU.png)\\n\\n### SNSサービスのデータ構造を設計する\\n\\n[問題と解答を見る](solutions/system_design/social_graph/README.md)\\n\\n![Imgur](images/cdCv5g7.png)\\n\\n### 検索エンジンのキー/バリュー構造を設計する\\n\\n[問題と解答を見る](solutions/system_design/query_cache/README.md)\\n\\n![Imgur](images/4j99mhe.png)\\n\\n### Amazonのカテゴリ毎の売り上げランキングを設計する\\n\\n[問題と解答を見る](solutions/system_design/sales_rank/README.md)\\n\\n![Imgur](images/MzExP06.png)\\n\\n### AWS上で100万人規模のユーザーを捌くサービスを設計する\\n\\n[問題と解答を見る](solutions/system_design/scaling_aws/README.md)\\n\\n![Imgur](images/jj3A5N8.png)\\n\\n## オブジェクト指向設計問題と解答\\n\\n> 頻出のオブジェクト指向システム設計面接課題と参考解答、コード及びダイアグラム\\n>\\n> 解答は `solutions/` フォルダ以下にリンクが貼られている\\n\\n>**備考: このセクションは作業中です**\\n\\n| 問題 | |\\n|---|---|\\n| ハッシュマップの設計 | [解答](solutions/object_oriented_design/hash_table/hash_map.ipynb)  |\\n| LRUキャッシュの設計 | [解答](solutions/object_oriented_design/lru_cache/lru_cache.ipynb)  |\\n| コールセンターの設計 | [解答](solutions/object_oriented_design/call_center/call_center.ipynb)  |\\n| カードのデッキの設計 | [解答](solutions/object_oriented_design/deck_of_cards/deck_of_cards.ipynb)  |\\n| 駐車場の設計 | [解答](solutions/object_oriented_design/parking_lot/parking_lot.ipynb)  |\\n| チャットサーバーの設計 | [解答](solutions/object_oriented_design/online_chat/online_chat.ipynb)  |\\n| 円形配列の設計 | [Contribute](#contributing)  |\\n| オブジェクト指向システム設計問題を追加する | [Contribute](#contributing) |\\n\\n## システム設計トピックス: まずはここから\\n\\nシステム設計の勉強は初めて？\\n\\nまず初めに、よく使われる設計原理について、それらが何であるか、どのように用いられるか、長所短所について基本的な知識を得る必要があります\\n\\n### ステップ 1: スケーラビリティに関する動画を観て復習する\\n\\n[Harvardでのスケーラビリティの講義](https://www.youtube.com/watch?v=-W9F__D3oY4)\\n\\n* ここで触れられているトピックス:\\n    * 垂直スケーリング\\n    * 水平スケーリング\\n    * キャッシング\\n    * ロードバランシング\\n    * データベースレプリケーション\\n    * データベースパーティション\\n\\n### ステップ 2: スケーラビリティに関する資料を読んで復習する\\n\\n[スケーラビリティ](http://www.lecloud.net/tagged/scalability/chrono)\\n\\n* ここで触れられているトピックス:\\n    * [クローン](http://www.lecloud.net/post/7295452622/scalability-for-dummies-part-1-clones)\\n    * [データベース](http://www.lecloud.net/post/7994751381/scalability-for-dummies-part-2-database)\\n    * [キャッシュ](http://www.lecloud.net/post/9246290032/scalability-for-dummies-part-3-cache)\\n    * [非同期](http://www.lecloud.net/post/9699762917/scalability-for-dummies-part-4-asynchronism)\\n\\n### 次のステップ\\n\\n次に、ハイレベルでのトレードオフについてみていく:\\n\\n* **パフォーマンス** vs **スケーラビリティ**\\n* **レイテンシ** vs **スループット**\\n* **可用性** vs **一貫性**\\n\\n**全てはトレードオフの関係にある**というのを肝に命じておきましょう。\\n\\nそれから、より深い内容、DNSやCDNそしてロードバランサーなどについて学習を進めていきましょう。\\n\\n## パフォーマンス vs スケーラビリティ\\n\\nリソースが追加されるのにつれて **パフォーマンス** が向上する場合そのサービスは **スケーラブル** であると言えるでしょう。一般的に、パフォーマンスを向上させるというのはすなわち計算処理を増やすことを意味しますが、データセットが増えた時などより大きな処理を捌けるようになることでもあります。<sup><a href=http://www.allthingsdistributed.com/2006/03/a_word_on_scalability.html>1</a></sup>\\n\\nパフォーマンスvsスケーラビリティをとらえる他の考え方:\\n\\n* **パフォーマンス** での問題を抱えている時、あなたのシステムは一人のユーザーにとって遅いと言えるでしょう。\\n* **スケーラビリティ** での問題を抱えているとき、一人のユーザーにとっては速いですが、多くのリクエストがある時には遅くなってしまうでしょう。\\n\\n### その他の参考資料、ページ\\n\\n* [スケーラビリティについて](http://www.allthingsdistributed.com/2006/03/a_word_on_scalability.html)\\n* [スケーラビリティ、可用性、安定性、パターン](http://www.slideshare.net/jboner/scalability-availability-stability-patterns/)\\n\\n## レイテンシー vs スループット\\n\\n**レイテンシー** とはなにがしかの動作を行う、もしくは結果を算出するのに要する時間\\n\\n**スループット** とはそのような動作や結果算出が単位時間に行われる回数\\n\\n一般的に、 **最大限のスループット** を **許容範囲内のレイテンシー** で実現することを目指すのが普通だ。\\n\\n### その他の参考資料、ページ\\n\\n* [レイテンシー vs スループットを理解する](https://community.cadence.com/cadence_blogs_8/b/sd/archive/2010/09/13/understanding-latency-vs-throughput)\\n\\n## 可用性 vs 一貫性\\n\\n### CAP 理論\\n\\n<p align=\"center\">\\n  <img src=\"images/bgLMI2u.png\">\\n  <br/>\\n  <i><a href=http://robertgreiner.com/2014/08/cap-theorem-revisited>Source: CAP theorem revisited</a></i>\\n</p>\\n\\n分散型コンピュータシステムにおいては下の三つのうち二つまでしか同時に保証することはできない。:\\n\\n* **一貫性** - 全ての読み込みは最新の書き込みもしくはエラーを受け取る\\n* **可用性** - 受け取る情報が最新のものだという保証はないが、全てのリクエストはレスポンスを必ず受け取る\\n* **分断耐性** - ネットワーク問題によって順不同の分断が起きてもシステムが動作を続ける\\n\\n*ネットワークは信頼できないので、分断耐性は必ず保証しなければなりません。つまりソフトウェアシステムとしてのトレードオフは、一貫性を取るか、可用性を取るかを考えなければなりません。*\\n\\n#### CP - 一貫性と分断耐性(consistency and partition tolerance)\\n\\n分断されたノードからのレスポンスを待ち続けているとタイムアウトエラーに陥る可能性があります。CPはあなたのサービスがアトミックな読み書き（不可分操作）を必要とする際にはいい選択肢でしょう。\\n\\n#### AP - 可用性と分断耐性(availability and partition tolerance)\\n\\nレスポンスはノード上にあるデータで最新のものを返します。つまり、最新版のデータが返されるとは限りません。分断が解消された後も、書き込みが反映されるのには時間がかかります。\\n\\n[結果整合性](#結果整合性)\\u3000を求めるサービスの際にはAPを採用するのがいいでしょう。もしくは、外部エラーに関わらずシステムが稼働する必要がある際にも同様です。\\n\\n### その他の参考資料、ページ\\n\\n* [CAP 理論を振り返る](http://robertgreiner.com/2014/08/cap-theorem-revisited/)\\n* [平易な英語でのCAP 理論のイントロ](http://ksat.me/a-plain-english-introduction-to-cap-theorem/)\\n* [CAP FAQ](https://github.com/henryr/cap-faq)\\n\\n## 一貫性パターン\\n\\n同じデータの複製が複数ある状態では、クライアントが一貫したデータ表示を受け取るために、どのようにそれらを同期すればいいのかという課題があります。 [CAP 理論](#cap-理論) における一貫性の定義を思い出してみましょう。全ての読み取りは最新の書き込みデータもしくはエラーを受け取るはずです。\\n\\n### 弱い一貫性\\n\\n書き込み後の読み取りでは、その最新の書き込みを読めたり読めなかったりする。ベストエフォート型のアプローチに基づく。\\n\\nこのアプローチはmemcachedなどのシステムに見られます。弱い一貫性はリアルタイム性が必要なユースケース、例えばVoIP、ビデオチャット、リアルタイムマルチプレイヤーゲームなどと相性がいいでしょう。例えば、電話に出ているときに数秒間音声が受け取れなくなったとしたら、その後に接続が回復してもその接続が切断されていた間に話されていたことは聞き取れないというような感じです。\\n\\n### 結果整合性\\n\\n書き込みの後、読み取りは最終的にはその結果を読み取ることができる(ミリ秒ほど遅れてというのが一般的です)。データは非同期的に複製されます。\\n\\nこのアプローチはDNSやメールシステムなどに採用されています。結果整合性は多くのリクエストを捌くサービスと相性がいいでしょう。\\n\\n### 強い一貫性\\n\\n書き込みの後、読み取りはそれを必ず読むことができます。データは同期的に複製されます。\\n\\nこのアプローチはファイルシステムやRDBMSなどで採用されています。トランザクションを扱うサービスでは強い一貫性が必要でしょう。\\n\\n### その他の参考資料、ページ\\n\\n* [データセンター間でのトランザクション](http://snarfed.org/transactions_across_datacenters_io.html)\\n\\n## 可用性パターン\\n\\n高い可用性を担保するには主に次の二つのパターンがあります: **フェイルオーバー** と **レプリケーション** です。\\n\\n### フェイルオーバー\\n\\n#### アクティブ・パッシブ\\n\\nアクティブ・パッシブフェイルオーバーにおいては、周期信号はアクティブもしくはスタンバイ中のパッシブなサーバーに送られます。周期信号が中断された時には、パッシブだったサーバーがアクティブサーバーのIPアドレスを引き継いでサービスを再開します。\\n\\n起動までのダウンタイムはパッシブサーバーが「ホット」なスタンバイ状態にあるか、「コールド」なスタンバイ状態にあるかで変わります。アクティブなサーバーのみがトラフィックを捌きます。\\n\\nアクティブ・パッシブフェイルオーバーはマスター・スレーブフェイルオーバーと呼ばれることもあります。\\n\\n#### アクティブ・アクティブ\\n\\nアクティブアクティブ構成では両方のサーバーがトラフィックを捌くことで負荷を分散します。\\n\\nこれらのサーバーがパブリックなものの場合、DNSは両方のサーバーのパブリックIPを知っている必要があります。もし、プライベートなものな場合、アプリケーションロジックが両方のサーバーの情報について知っている必要があります。\\n\\nアクティブ・アクティブなフェイルオーバーはマスター・マスターフェイルオーバーと呼ばれることもあります。\\n\\n### 短所: フェイルオーバー\\n\\n* フェイルオーバーではより多くのハードウェアを要し、複雑さが増します。\\n* 最新の書き込みがパッシブサーバーに複製される前にアクティブが落ちると、データ欠損が起きる潜在可能性があります。\\n\\n### レプリケーション\\n\\n#### マスター・スレーブ\\u3000と\\u3000マスター・マスター\\n\\nこのトピックは [データベース](#データベース) セクションにおいてより詳細に解説されています:\\n\\n* [マスター・スレーブ レプリケーション](#マスタースレーブ-レプリケーション)\\n* [マスター・マスター レプリケーション](#マスターマスター-レプリケーション)\\n\\n## ドメインネームシステム\\n\\n<p align=\"center\">\\n  <img src=\"images/IOyLj4i.jpg\">\\n  <br/>\\n  <i><a href=http://www.slideshare.net/srikrupa5/dns-security-presentation-issa>Source: DNS security presentation</a></i>\\n</p>\\n\\nドメインネームシステム (DNS) は www.example.com などのドメインネームをIPアドレスへと翻訳します。\\n\\nDNSは少数のオーソライズされたサーバーが上位に位置する階層的構造です。あなたのルーターもしくはISPは検索をする際にどのDNSサーバーに接続するかという情報を提供します。低い階層のDNSサーバーはその経路マップをキャッシュします。ただ、この情報は伝搬遅延によって陳腐化する可能性があります。DNSの結果はあなたのブラウザもしくはOSに一定期間（[time to live (TTL)](https://en.wikipedia.org/wiki/Time_to_live)に設定された期間）キャッシュされます。\\n\\n* **NS record (name server)** - あなたのドメイン・サブドメインでのDNSサーバーを特定します。\\n* **MX record (mail exchange)** - メッセージを受け取るメールサーバーを特定します。\\n* **A record (address)** - IPアドレスに名前をつけます。\\n* **CNAME (canonical)** - 他の名前もしくは\\u3000`CNAME` (example.com を www.example.com) もしくは `A` recordへと名前を指し示す。\\n\\n[CloudFlare](https://www.cloudflare.com/dns/) や [Route 53](https://aws.amazon.com/route53/) などのサービスはマネージドDNSサービスを提供しています。いくつかのDNSサービスでは様々な手法を使ってトラフィックを捌くことができます:\\n\\n* [加重ラウンドロビン](http://g33kinfo.com/info/archives/2657)\\n    * トラフィックがメンテナンス中のサーバーに行くのを防ぎます\\n    * 様々なクラスターサイズに応じて調整します\\n    * A/B テスト\\n* レイテンシーベース\\n* 地理ベース\\n\\n### 欠点: DNS\\n\\n* 上記で示されているようなキャッシングによって緩和されているとはいえ、DNSサーバーへの接続には少し遅延が生じる。\\n* DNSサーバーは、[政府、ISP企業,そして大企業](http://superuser.com/questions/472695/who-controls-the-dns-servers/472729)に管理されているが、それらの管理は複雑である。\\n* DNSサービスは[DDoS attack](http://dyn.com/blog/dyn-analysis-summary-of-friday-october-21-attack/)の例で、IPアドレスなしにユーザーがTwitterなどにアクセスできなくなったように、攻撃を受ける可能性がある。\\n\\n### その他の参考資料、ページ\\n\\n* [DNS アーキテクチャ](https://technet.microsoft.com/en-us/library/dd197427(v=ws.10).aspx)\\n* [Wikipedia](https://en.wikipedia.org/wiki/Domain_Name_System)\\n* [DNS 記事](https://support.dnsimple.com/categories/dns/)\\n\\n## コンテンツデリバリーネットワーク(Content delivery network)\\n\\n<p align=\"center\">\\n  <img src=\"images/h9TAuGI.jpg\">\\n  <br/>\\n  <i><a href=https://www.creative-artworks.eu/why-use-a-content-delivery-network-cdn/>Source: Why use a CDN</a></i>\\n</p>\\n\\nコンテンツデリバリーネットワーク(CDN)は世界中に配置されたプロキシサーバーのネットワークがユーザーに一番地理的に近いサーバーからコンテンツを配信するシステムのことです。AmazonのCloudFrontなどは例外的にダイナミックなコンテンツも配信しますが、一般的に、HTML/CSS/JS、写真、そして動画などの静的ファイルがCDNを通じて配信されます。そのサイトのDNSがクライアントにどのサーバーと交信するかという情報を伝えます。\\n\\nCDNを用いてコンテンツを配信することで以下の二つの理由でパフォーマンスが劇的に向上します:\\n\\n* ユーザーは近くにあるデータセンターから受信できる\\n* バックエンドサーバーはCDNが処理してくれるリクエストに関しては処理する必要がなくなります\\n\\n### プッシュCDN\\n\\nプッシュCDNではサーバーデータに更新があった時には必ず、新しいコンテンツを受け取る方式です。コンテンツを用意し、CDNに直接アップロードし、URLをCDNを指すように指定するところまで、全て自分で責任を負う形です。コンテンツがいつ期限切れになるのか更新されるのかを設定することができます。コンテンツは新規作成時、更新時のみアップロードされることでトラフィックは最小化される一方、ストレージは最大限消費されてしまいます。\\n\\nトラフィックの少ない、もしくは頻繁にはコンテンツが更新されないサイトの場合にはプッシュCDNと相性がいいでしょう。コンテンツは定期的に再びプルされるのではなく、CDNに一度のみ配置されます。\\n\\n### プルCDN\\n\\nプルCDNでは一人目のユーザーがリクエストした時に、新しいコンテンツをサービスのサーバーから取得します。コンテンツは自分のサーバーに保存して、CDNを指すURLを書き換えます。結果として、CDNにコンテンツがキャッシュされるまではリクエスト処理が遅くなります。\\n\\n[time-to-live (TTL)](https://en.wikipedia.org/wiki/Time_to_live) はコンテンツがどれだけの期間キャッシュされるかを規定します。プルCDNはCDN 上でのストレージスペースを最小化しますが、有効期限が切れたファイルが更新前にプルされてしまうことで冗長なトラフィックに繋がってしまう可能性があります。\\n\\n大規模なトラフィックのあるサイトではプルCDNが相性がいいでしょう。というのも、トラフィックの大部分は最近リクエストされ、CDNに残っているコンテンツにアクセスするものであることが多いからです。\\n\\n### 欠点: CDN\\n\\n* CDNのコストはトラフィック量によって変わります。もちろん、CDNを使わない場合のコストと比較するべきでしょう。\\n* TTLが切れる前にコンテンツが更新されると陳腐化する恐れがあります。\\n* CDNでは静的コンテンツがCDNを指すようにURLを更新する必要があります。\\n\\n### その他の参考資料、ページ\\n\\n* [グローバルに分散されたコンテンツデリバリーネットワーク](http://repository.cmu.edu/cgi/viewcontent.cgi?article=2112&context=compsci)\\n* [プッシュCDNとプルCDNの違い](http://www.travelblogadvice.com/technical/the-differences-between-push-and-pull-cdns/)\\n* [Wikipedia](https://en.wikipedia.org/wiki/Content_delivery_network)\\n\\n## ロードバランサー\\n\\n<p align=\"center\">\\n  <img src=\"images/h81n9iK.png\">\\n  <br/>\\n  <i><a href=http://horicky.blogspot.com/2010/10/scalable-system-design-patterns.html>Source: Scalable system design patterns</a></i>\\n</p>\\n\\nロードバランサーは入力されるクライアントのリクエストをアプリケーションサーバーやデータベースへと分散させる。どのケースでもロードバランサーはサーバー等計算リソースからのレスポンスを適切なクライアントに返す。ロードバランサーは以下のことに効果的です:\\n\\n* リクエストが状態の良くないサーバーに行くのを防ぐ\\n* リクエストを過剰に送るのを防ぐ\\n* 特定箇所の欠陥でサービスが落ちることを防ぐ\\n\\nロードバランサーは (費用の高い) ハードウェアもしくはHAProxyなどのソフトウェアで実現できる。\\n\\n他の利点としては:\\n\\n* **SSL termination** - 入力されるリクエストを解読する、また、サーバーレスポンスを暗号化することでバックエンドのサーバーがこのコストが高くつきがちな処理を請け負わなくていいように肩代わりします。\\n    * [X.509 certificates](https://en.wikipedia.org/wiki/X.509) をそれぞれのサーバーにインストールする必要をなくします\\n* **セッション管理** - クッキーを取り扱うウェブアプリがセッション情報を保持していない時などに、特定のクライアントのリクエストを同じインスタンスへと流します。\\n\\n障害に対応するために、[アクティブ・パッシブ](#アクティブパッシブ) もしくは [アクティブ・アクティブ](#アクティブアクティブ) モードのどちらにおいても、複数のロードバランサーを配置するのが一般的です。\\n\\nロードバランサーは以下のような種々のメトリックを用いてトラフィックルーティングを行うことができます:\\n\\n* ランダム\\n* Least loaded\\n* セッション/クッキー\\n* [ラウンドロビンもしくは加重ラウンドロビン](http://g33kinfo.com/info/archives/2657)\\n* [Layer 4](#layer-4-ロードバランシング)\\n* [Layer 7](#layer-7-ロードバランシング)\\n\\n### Layer 4 ロードバランシング\\n\\nLayer 4 ロードバランサーは [トランスポートレイヤー](#通信) を参照してどのようにリクエストを配分するか判断します。一般的に、トランスポートレイヤーとしては、ソース、送信先IPアドレス、ヘッダーに記述されたポート番号が含まれますが、パケットの中身のコンテンツは含みません。 Layer 4 ロードバランサーはネットワークパケットを上流サーバーへ届け、上流サーバーから配信することでネットワークアドレス変換 [Network Address Translation (NAT)](https://www.nginx.com/resources/glossary/layer-4-load-balancing/) を実現します。\\n\\n### Layer 7 ロードバランシング\\n\\nLayer 7 ロードバランサーは [アプリケーションレイヤー](#通信) を参照してどのようにリクエストを配分するか判断します。ヘッダー、メッセージ、クッキーなどのコンテンツのことです。Layer 7 ロードバランサーはネットワークトラフィックの終端を受け持ち メッセージを読み込み、ロードバランシングの判断をし、選択したサーバーとの接続を繋ぎます。例えば layer 7 ロードバランサーは動画のトラフィックを直接、そのデータをホストしているサーバーにつなぐと同時に、決済処理などのより繊細なトラフィックをセキュリティ強化されたサーバーに流すということもできる。\\n\\n柔軟性とのトレードオフになりますが、 layer 4 ロードバランサーではLayer 7ロードバランサーよりも所要時間、計算リソースを少なく済ませることができます。ただし、昨今の汎用ハードウェアではパフォーマンスは最小限のみしか発揮できないでしょう。\\n\\n### 水平スケーリング\\n\\nロードバランサーでは水平スケーリングによってパフォーマンスと可用性を向上させることができます。手頃な汎用マシンを追加することによってスケールアウトさせる方が、一つのサーバーをより高価なマシンにスケールアップする（**垂直スケーリング**）より費用対効果も高くなり、結果的に可用性も高くなります。また、汎用ハードウェアを扱える人材を雇う方が、特化型の商用ハードウェアを扱える人材を雇うよりも簡単でしょう。\\n\\n#### 欠点: 水平スケーリング\\n\\n* 水平的にスケーリングしていくと、複雑さが増す上に、サーバーのクローニングが必要になる。\\n    * サーバーはステートレスである必要がある: ユーザーに関連するセッションや、プロフィール写真などのデータを持ってはいけない\\n    * セッションは一元的な[データベース](#データベース) (SQL、 NoSQL)などのデータストアにストアされるか [キャッシュ](#キャッシュ) (Redis、 Memcached)に残す必要があります。\\n* キャッシュやデータベースなどの下流サーバーは上流サーバーがスケールアウトするにつれてより多くの同時接続を保たなければなりません。\\n\\n### 欠点: ロードバランサー\\n\\n* ロードバランサーはリソースが不足していたり、設定が適切でない場合、システム全体のボトルネックになる可能性があります。\\n* 単一障害点を除こうとしてロードバランサーを導入した結果、複雑さが増してしまうことになります。\\n* ロードバランサーが一つだけだとそこが単一障害点になってしまいます。一方で、ロードバランサーを複数にすると、さらに複雑さが増してしまいます。\\n\\n### その他の参考資料、ページ\\n\\n* [NGINX アーキテクチャ](https://www.nginx.com/blog/inside-nginx-how-we-designed-for-performance-scale/)\\n* [HAProxy アーキテクチャガイド](http://www.haproxy.org/download/1.2/doc/architecture.txt)\\n* [スケーラビリティ](http://www.lecloud.net/post/7295452622/scalability-for-dummies-part-1-clones)\\n* [Wikipedia](https://en.wikipedia.org/wiki/Load_balancing_(computing))\\n* [Layer 4 ロードバランシング](https://www.nginx.com/resources/glossary/layer-4-load-balancing/)\\n* [Layer 7 ロードバランシング](https://www.nginx.com/resources/glossary/layer-7-load-balancing/)\\n* [ELB listener config](http://docs.aws.amazon.com/elasticloadbalancing/latest/classic/elb-listener-config.html)\\n\\n## リバースプロキシ(webサーバー)\\n\\n<p align=\"center\">\\n  <img src=\"images/n41Azff.png\">\\n  <br/>\\n  <i><a href=https://upload.wikimedia.org/wikipedia/commons/6/67/Reverse_proxy_h2g2bob.svg>Source: Wikipedia</a></i>\\n  <br/>\\n</p>\\n\\nリバースプロキシサーバーは内部サービスをまとめて外部に統一されたインターフェースを提供するウェブサーバーです。クライアントからのリクエストはそれに対応するサーバーに送られて、その後レスポンスをリバースプロキシがクライアントに返します。\\n\\n他には以下のような利点があります:\\n\\n* **より堅牢なセキュリティ** - バックエンドサーバーの情報を隠したり、IPアドレスをブラックリスト化したり、クライアントごとの接続数を制限したりできます。\\n* **スケーラビリティや柔軟性が増します** - クライアントはリバースプロキシのIPしか見ないので、裏でサーバーをスケールしたり、設定を変えやすくなります。\\n* **SSL termination** - 入力されるリクエストを解読し、サーバーのレスポンスを暗号化することでサーバーがこのコストのかかりうる処理をしなくて済むようになります。\\n    * [X.509 証明書](https://en.wikipedia.org/wiki/X.509) を各サーバーにインストールする必要がなくなります。\\n* **圧縮** - サーバーレスポンスを圧縮できます\\n* **キャッシング** - キャッシュされたリクエストに対して、レスポンスを返します\\n* **静的コンテンツ** - 静的コンテンツを直接送信することができます。\\n    * HTML/CSS/JS\\n    * 写真\\n    * 動画\\n    * などなど\\n\\n### ロードバランサー vs リバースプロキシ\\n\\n* 複数のサーバーがある時にはロードバランサーをデプロイすると役に立つでしょう。 しばしば、ロードバランサーは同じ機能を果たすサーバー群へのトラフィックを捌きます。\\n* リバースプロキシでは、上記に述べたような利点を、単一のウェブサーバーやアプリケーションレイヤーに対しても示すことができます。\\n* NGINX や HAProxy などの技術はlayer 7 リバースプロキシとロードバランサーの両方をサポートします。\\n\\n### 欠点: リバースプロキシ\\n\\n* リバースプロキシを導入するとシステムの複雑性が増します。\\n* 単一のリバースプロキシは単一障害点になりえます。一方で、複数のリバースプロキシを導入すると(例: [フェイルオーバー](https://en.wikipedia.org/wiki/Failover)) 複雑性はより増します。\\n\\n### その他の参考資料、ページ\\n\\n* [リバースプロキシ vs ロードバランサー](https://www.nginx.com/resources/glossary/reverse-proxy-vs-load-balancer/)\\n* [NGINX アーキテクチャ](https://www.nginx.com/blog/inside-nginx-how-we-designed-for-performance-scale/)\\n* [HAProxy アーキテクチャ ガイド](http://www.haproxy.org/download/1.2/doc/architecture.txt)\\n* [Wikipedia](https://en.wikipedia.org/wiki/Reverse_proxy)\\n\\n## アプリケーション層\\n\\n<p align=\"center\">\\n  <img src=\"images/yB5SYwm.png\">\\n  <br/>\\n  <i><a href=http://lethain.com/introduction-to-architecting-systems-for-scale/#platform_layer>Source: Intro to architecting systems for scale</a></i>\\n</p>\\n\\nウェブレイヤーをアプリケーション層 (プラットフォーム層とも言われる) と分離することでそれぞれの層を独立にスケール、設定することができるようになります。新しいAPIをアプリケーション層に追加する際に、不必要にウェブサーバーを追加する必要がなくなります。\\n\\n **単一責任の原則** では、小さい自律的なサービスが協調して動くように提唱しています。小さいサービスの小さいチームが急成長のためにより積極的な計画を立てられるようにするためです。\\n\\nアプリケーション層は[非同期処理](#非同期処理)もサポートします。\\n\\n### マイクロサービス\\n\\n独立してデプロイできる、小規模なモジュール様式である[マイクロサービス](https://en.wikipedia.org/wiki/Microservices)もこの議論に関係してくる技術でしょう。それぞれのサービスは独自のプロセスを処理し、明確で軽量なメカニズムで通信して、その目的とする機能を実現します。<sup><a href=https://smartbear.com/learn/api-design/what-are-microservices>1</a></sup>\\n\\n例えばPinterestでは以下のようなマイクロサービスに分かれています。ユーザープロフィール、フォロワー、フィード、検索、写真アップロードなどです。\\n\\n### サービスディスカバリー\\n\\n[Consul](https://www.consul.io/docs/index.html)、 [Etcd](https://coreos.com/etcd/docs/latest)、 [Zookeeper](http://www.slideshare.net/sauravhaloi/introduction-to-apache-zookeeper) などのシステムでは、登録されているサービスの名前、アドレス、ポートの情報を監視することで、サービス同士が互いを見つけやすくしています。サービスの完全性の確認には [Health checks](https://www.consul.io/intro/getting-started/checks.html) が便利で、これには [HTTP](#hypertext-transfer-protocol-http) エンドポイントがよく使われます。 Consul と Etcd のいずれも組み込みの [key-value store](#キーバリューストア) を持っており、設定データや共有データなどのデータを保存しておくことに使われます。\\n\\n### 欠点: アプリケーション層\\n\\n* アーキテクチャ、運用、そしてプロセスを考慮すると、緩く結び付けられたアプリケーション層を追加するには、モノリシックなシステムとは異なるアプローチが必要です。\\n* マイクロサービスはデプロイと運用の点から見ると複雑性が増すことになります。\\n\\n### その他の参考資料、ページ\\n\\n* [スケールするシステムアーキテクチャを設計するためのイントロ](http://lethain.com/introduction-to-architecting-systems-for-scale)\\n* [システム設計インタビューを紐解く](http://www.puncsky.com/blog/2016-02-13-crack-the-system-design-interview)\\n* [サービス指向アーキテクチャ](https://en.wikipedia.org/wiki/Service-oriented_architecture)\\n* [Zookeeperのイントロダクション](http://www.slideshare.net/sauravhaloi/introduction-to-apache-zookeeper)\\n* [マイクロサービスを作るために知っておきたいこと](https://cloudncode.wordpress.com/2016/07/22/msa-getting-started/)\\n\\n## データベース\\n\\n<p align=\"center\">\\n  <img src=\"images/Xkm5CXz.png\">\\n  <br/>\\n  <i><a href=https://www.youtube.com/watch?v=w95murBkYmU>Source: Scaling up to your first 10 million users</a></i>\\n</p>\\n\\n### リレーショナルデータベースマネジメントシステム (RDBMS)\\n\\nSQLなどのリレーショナルデータベースはテーブルに整理されたデータの集合である。\\n\\n**ACID** はリレーショナルデータベースにおける[トランザクション](https://en.wikipedia.org/wiki/Database_transaction)のプロパティの集合である\\n\\n* **不可分性** - それぞれのトランザクションはあるかないかのいずれかである\\n* **一貫性** - どんなトランザクションもデータベースをある確かな状態から次の状態に遷移させる。\\n* **独立性** - 同時にトランザクションを処理することは、連続的にトランザクションを処理するのと同じ結果をもたらす。\\n* **永続性** - トランザクションが処理されたら、そのように保存される\\n\\nリレーショナルデータベースをスケールさせるためにはたくさんの技術がある: **マスター・スレーブ レプリケーション**、 **マスター・マスター レプリケーション**、 **federation**、 **シャーディング**、 **非正規化**、 そして **SQL チューニング**\\n\\n#### マスタースレーブ レプリケーション\\n\\nマスターデータベースが読み取りと書き込みを処理し、書き込みを一つ以上のスレーブデータベースに複製します。スレーブデータベースは読み取りのみを処理します。スレーブデータベースは木構造のように追加のスレーブにデータを複製することもできます。マスターデータベースがオフラインになった場合には、いずれかのスレーブがマスターに昇格するか、新しいマスターデータベースが追加されるまでは読み取り専用モードで稼働します。\\n\\n<p align=\"center\">\\n  <img src=\"images/C9ioGtn.png\">\\n  <br/>\\n  <i><a href=http://www.slideshare.net/jboner/scalability-availability-stability-patterns/>Source: Scalability, availability, stability, patterns</a></i>\\n</p>\\n\\n##### 欠点: マスタースレーブ レプリケーション\\n\\n* スレーブをマスターに昇格させるには追加のロジックが必要になる。\\n* マスタースレーブ レプリケーション、マスターマスター レプリケーションの **両方** の欠点は[欠点: レプリケーション](#欠点-マスタースレーブ-レプリケーション)を参照\\n\\n#### マスターマスター レプリケーション\\n\\nいずれのマスターも読み取り書き込みの両方に対応する。書き込みに関してはそれぞれ協調する。いずれかのマスターが落ちても、システム全体としては読み書き両方に対応したまま運用できる。\\n\\n<p align=\"center\">\\n  <img src=\"images/krAHLGg.png\">\\n  <br/>\\n  <i><a href=http://www.slideshare.net/jboner/scalability-availability-stability-patterns/>Source: Scalability, availability, stability, patterns</a></i>\\n</p>\\n\\n##### 欠点: マスターマスター レプリケーション\\n\\n* ロードバランサーを導入するか、アプリケーションロジックを変更することでどこに書き込むかを指定しなければならない。\\n* 大体のマスターマスターシステムは、一貫性が緩い（ACID原理を守っていない）もしくは、同期する時間がかかるために書き込みのレイテンシーが増加してしまっている。\\n* 書き込みノードが追加され、レイテンシーが増加するにつれ書き込みの衝突の可能性が増える。\\n* マスタースレーブ レプリケーション、マスターマスター レプリケーションの **両方** の欠点は[欠点: レプリケーション](#欠点-マスタースレーブ-レプリケーション) を参照\\n\\n##### 欠点: レプリケーション\\n\\n* 新しいデータ書き込みを複製する前にマスターが落ちた場合にはそのデータが失われてしまう可能性がある。\\n* 書き込みは読み取りレプリカにおいてリプレイされる。書き込みが多い場合、複製ノードが書き込みの処理のみで行き詰まって、読み取りの処理を満足に行えない可能性がある。\\n* 読み取りスレーブノードの数が多ければ多いほど、複製しなければならない数も増え、複製時間が伸びてしまいます。\\n* システムによっては、マスターへの書き込みはマルチスレッドで並列処理できる一方、スレーブへの複製は単一スレッドで連続的に処理しなければならない場合があります。\\n* レプリケーションでは追加のハードウェアが必要になり、複雑性も増します。\\n\\n##### その他の参考資料、ページ: レプリケーション\\n\\n* [スケーラビリティ、 可用性、 スタビリティ パターン](http://www.slideshare.net/jboner/scalability-availability-stability-patterns/)\\n* [マルチマスター レプリケーション](https://en.wikipedia.org/wiki/Multi-master_replication)\\n\\n#### Federation\\n\\n<p align=\"center\">\\n  <img src=\"images/U3qV33e.png\">\\n  <br/>\\n  <i><a href=https://www.youtube.com/watch?v=w95murBkYmU>Source: Scaling up to your first 10 million users</a></i>\\n</p>\\n\\nフェデレーション (もしくは機能分割化とも言う) はデータベースを機能ごとに分割する。例えば、モノリシックな単一データベースの代わりに、データベースを **フォーラム**、 **ユーザー**、 **プロダクト** のように三つにすることで、データベース一つあたりの書き込み・読み取りのトラフィックが減り、その結果レプリケーションのラグも短くなります。データベースが小さくなることで、メモリーに収まるデータが増えます。キャッシュの局所性が高まるため、キャッシュヒット率も上がります。単一の中央マスターで書き込みを直列化したりしないため、並列で書き込みを処理することができ、スループットの向上が期待できます。\\n\\n##### 欠点: federation\\n\\n* 大規模な処理やテーブルを要するスキーマの場合、フェデレーションは効果的とは言えないでしょう。\\n* どのデータベースに読み書きをするのかを指定するアプリケーションロジックを更新しなければなりません。\\n* [server link](http://stackoverflow.com/questions/5145637/querying-data-by-joining-two-tables-in-two-database-on-different-servers)で二つのデータベースからのデータを連結するのはより複雑になるでしょう。\\n* フェデレーションでは追加のハードウェアが必要になり、複雑性も増します。\\n\\n##### その他の参考資料、ページ: federation\\n\\n* [Scaling up to your first 10 million users](https://www.youtube.com/watch?v=w95murBkYmU)\\n\\n#### シャーディング\\n\\n<p align=\"center\">\\n  <img src=\"images/wU8x5Id.png\">\\n  <br/>\\n  <i><a href=http://www.slideshare.net/jboner/scalability-availability-stability-patterns/>Source: Scalability, availability, stability, patterns</a></i>\\n</p>\\n\\nシャーディングでは異なるデータベースにそれぞれがデータのサブセット断片のみを持つようにデータを分割します。ユーザーデータベースを例にとると、ユーザー数が増えるにつれてクラスターにはより多くの断片が加えられることになります。\\n\\n[federation](#federation)の利点に似ていて、シャーディングでは読み書きのトラフィックを減らし、レプリケーションを減らし、キャッシュヒットを増やすことができます。インデックスサイズも減らすことができます。一般的にはインデックスサイズを減らすと、パフォーマンスが向上しクエリ速度が速くなります。なにがしかのデータを複製する機能がなければデータロスにつながりますが、もし、一つのシャードが落ちても、他のシャードが動いていることになります。フェデレーションと同じく、単一の中央マスターが書き込みの処理をしなくても、並列で書き込みを処理することができ、スループットの向上が期待できます。\\n\\nユーザーテーブルをシャードする一般的な方法は、ユーザーのラストネームイニシャルでシャードするか、ユーザーの地理的配置でシャードするなどです。\\n\\n##### 欠点: シャーディング\\n\\n* シャードに対応するようにアプリケーションロジックを変更しなければなりません。結果としてSQLクエリが複雑になります。\\n* シャードではデータ配分がいびつになってしまう可能性があります。例えば、標準ユーザーの集合を持つシャードがある場合、そのシャードが他のシャードよりも重い負荷を負うことになります。\\n    * リバランシングをすると複雑性がより増します。[consistent hashing](http://www.paperplanes.de/2011/12/9/the-magic-of-consistent-hashing.html) に基づいたシャーディングでは、通信データを削減することもできます。\\n* 複数のシャードからのデータを連結するのはより複雑です。\\n* シャーディングでは追加のハードウェアが必要になり、複雑性も増します。\\n\\n##### その他の参考資料、ページ: シャーディング\\n\\n* [シャードの登場](http://highscalability.com/blog/2009/8/6/an-unorthodox-approach-to-database-design-the-coming-of-the.html)\\n* [シャードデータベースアーキテクチャ](https://en.wikipedia.org/wiki/Shard_(database_architecture))\\n* [Consistent hashing](http://www.paperplanes.de/2011/12/9/the-magic-of-consistent-hashing.html)\\n\\n#### 非正規化\\n\\n非正規化では、書き込みのパフォーマンスをいくらか犠牲にして読み込みのパフォーマンスを向上させようとします。計算的に重いテーブルの結合などをせずに、複数のテーブルに冗長なデータのコピーが書き込まれるのを許容します。いくつかのRDBMS例えば、[PostgreSQL](https://en.wikipedia.org/wiki/PostgreSQL) やOracleはこの冗長な情報を取り扱い、一貫性を保つための[materialized views](https://en.wikipedia.org/wiki/Materialized_view) という機能をサポートしています。\\n\\n[フェデレーション](#federation) や [シャーディング](#シャーディング)などのテクニックによってそれぞれのデータセンターに分配されたデータを合一させることはとても複雑な作業です。非正規化によってそのような複雑な処理をしなくて済むようになります。\\n\\n多くのシステムで、100対1あるいは1000対1くらいになるくらい読み取りの方が、書き込みのトラフィックよりも多いことでしょう。読み込みを行うために、複雑なデータベースのジョイン処理が含まれるものは計算的に高価につきますし、ディスクの処理時間で膨大な時間を費消してしまうことになります。\\n\\n##### 欠点: 非正規化\\n\\n* データが複製される。\\n* 冗長なデータの複製が同期されるように制約が存在し、そのことでデータベース全体の設計が複雑化する。\\n* 非正規化されたデータベースは過大な書き込みを処理しなければならない場合、正規化されているそれよりもパフォーマンスにおいて劣る可能性がある。\\n\\n###### その他の参考資料、ページ: 非正規化\\n\\n* [Denormalization](https://en.wikipedia.org/wiki/Denormalization)\\n\\n#### SQLチューニング\\n\\nSQLチューニングは広範な知識を必要とする分野で多くの [本](https://www.amazon.com/s/ref=nb_sb_noss_2?url=search-alias%3Daps&field-keywords=sql+tuning) が書かれています。\\n\\nボトルネックを明らかにし、シミュレートする上で、 **ベンチマーク** を定め、 **プロファイル** することはとても重要です。\\n\\n* **ベンチマーク** - [ab](http://httpd.apache.org/docs/2.2/programs/ab.html)などのツールを用いて、高負荷の状況をシミュレーションしてみましょう。\\n* **プロファイル** - [slow query log](http://dev.mysql.com/doc/refman/5.7/en/slow-query-log.html) などのツールを用いて、パフォーマンス状況の確認をしましょう。\\n\\nベンチマークとプロファイルをとることで以下のような効率化の選択肢をとることになるでしょう。\\n\\n##### スキーマを絞る\\n\\n* MySQLはアクセス速度向上のため、ディスク上の連続したブロックへデータを格納しています。\\n* 長さの決まったフィールドに対しては `VARCHAR` よりも `CHAR` を使うようにしましょう。\\n    * `CHAR` の方が効率的に速くランダムにデータにアクセスできます。 一方、 `VARCHAR` では次のデータに移る前にデータの末尾を検知しなければならないために速度が犠牲になります。\\n* ブログの投稿など、大きなテキストには TEXT を使いましょう。 TEXT ではブーリアン型の検索も可能です。 TEXT フィールドには、テキストブロックが配置されている、ディスク上の場所へのポインターが保存されます。\\n* 2の32乗や40億以下を超えない程度の大きな数には INT を使いましょう。\\n* 通貨に関しては小数点表示上のエラーを避けるために `DECIMAL` を使いましょう。\\n* 大きな `BLOBS` を保存するのは避けましょう。どこからそのオブジェクトを取ってくることができるかの情報を保存しましょう。\\n* `VARCHAR(255)` は8ビットで数えられる最大の文字数です。一部のDBMSでは、1バイトの利用効率を最大化するためにこの文字数がよく使われます。\\n* [検索性能向上のため](http://stackoverflow.com/questions/1017239/how-do-null-values-affect-performance-in-a-database-search) 、可能であれば `NOT NULL` 制約を設定しましょう。\\n\\n##### インデックスを効果的に用いる\\n\\n* クエリ(`SELECT`、 `GROUP BY`、 `ORDER BY`、 `JOIN`) の対象となる列にインデックスを使うことで速度を向上できるかもしれません。\\n* インデックスは通常、平衡探索木である[B木](https://en.wikipedia.org/wiki/B-tree)の形で表されます。B木によりデータは常にソートされた状態になります。また検索、順次アクセス、挿入、削除を対数時間で行えます。\\n* インデックスを配置することはデータをメモリーに残すことにつながりより容量を必要とします。\\n* インデックスの更新も必要になるため書き込みも遅くなります。\\n* 大量のデータをロードする際には、インデックスを切ってからデータをロードして再びインデックスをビルドした方が速いことがあります。\\n\\n##### 高負荷なジョインを避ける\\n\\n* パフォーマンス上必要なところには[非正規化](#非正規化)を適用する\\n\\n##### テーブルのパーティション\\n\\n* テーブルを分割し、ホットスポットを独立したテーブルに分離してメモリーに乗せられるようにする。\\n\\n##### クエリキャッシュを調整する\\n\\n* 場合によっては[クエリキャッシュ](http://dev.mysql.com/doc/refman/5.7/en/query-cache) が[パフォーマンス問題](https://www.percona.com/blog/2014/01/28/10-mysql-performance-tuning-settings-after-installation/) を引き起こす可能性がある\\n\\n##### その他の参考資料、ページ: SQLチューニング\\n\\n* [MySQLクエリを最適化するためのTips](http://20bits.com/article/10-tips-for-optimizing-mysql-queries-that-dont-suck)\\n* [VARCHAR(255)をやたらよく見かけるのはなんで？](http://stackoverflow.com/questions/1217466/is-there-a-good-reason-i-see-varchar255-used-so-often-as-opposed-to-another-l)\\n* [null値はどのようにパフォーマンスに影響するのか？](http://stackoverflow.com/questions/1017239/how-do-null-values-affect-performance-in-a-database-search)\\n* [Slow query log](http://dev.mysql.com/doc/refman/5.7/en/slow-query-log.html)\\n\\n### NoSQL\\n\\nNoSQL は **key-value store**、 **document-store**、 **wide column store**、 もしくは **graph database**によって表現されるデータアイテムの集合です。データは一般的に正規化されておらず、アプリケーション側でジョインが行われます。大部分のNoSQLは真のACIDトランザクションを持たず、 [結果整合性](#結果整合性) 的な振る舞いの方を好みます。\\n\\n**BASE** はしばしばNoSQLデータベースのプロパティを説明するために用いられます。[CAP Theorem](#cap-理論) と対照的に、BASEは一貫性よりも可用性を優先します。\\n\\n* **Basically available** - システムは可用性を保証します。\\n* **Soft state** - システムの状態は入力がなくても時間経過とともに変化する可能性があります。\\n* **結果整合性** - システム全体は時間経過とともにその間に入力がないという前提のもと、一貫性が達成されます。\\n\\n[SQLか？NoSQLか？](#sqlかnosqlか) を選択するのに加えて、どのタイプのNoSQLがどの使用例に最も適するかを理解するのはとても有益です。このセクションでは **キーバリューストア**、 **ドキュメントストア**、 **ワイドカラムストア**、 と **グラフデータベース** について触れていきます。\\n\\n#### キーバリューストア\\n\\n> 概要: ハッシュテーブル\\n\\nキーバリューストアでは一般的にO(1)の読み書きができ、それらはメモリないしSSDで裏付けられています。データストアはキーを [辞書的順序](https://en.wikipedia.org/wiki/Lexicographical_order) で保持することでキーの効率的な取得を可能にしています。キーバリューストアではメタデータを値とともに保持することが可能です。\\n\\nキーバリューストアはハイパフォーマンスな挙動が可能で、単純なデータモデルやインメモリーキャッシュレイヤーなどのデータが急速に変わる場合などに使われます。単純な処理のみに機能が制限されているので、追加の処理機能が必要な場合にはその複雑性はアプリケーション層に載せることになります。\\n\\nキーバリューストアはもっと複雑なドキュメントストアや、グラフデータベースなどの基本です。\\n\\n##### その他の参考資料、ページ: キーバリューストア\\n\\n* [キーバリューデータベース](https://en.wikipedia.org/wiki/Key-value_database)\\n* [キーバリューストアの欠点](http://stackoverflow.com/questions/4056093/what-are-the-disadvantages-of-using-a-key-value-table-over-nullable-columns-or)\\n* [Redisアーキテクチャ](http://qnimate.com/overview-of-redis-architecture/)\\n* [メムキャッシュアーキテクチャ](https://adayinthelifeof.nl/2011/02/06/memcache-internals/)\\n\\n#### ドキュメントストア\\n\\n> 概要: ドキュメントがバリューとして保存されたキーバリューストア\\n\\nドキュメントストアはオブジェクトに関する全ての情報を持つドキュメント(XML、 JSON、 binaryなど)を中心に据えたシステムです。ドキュメントストアでは、ドキュメント自身の内部構造に基づいた、APIもしくはクエリ言語を提供します。 *メモ：多くのキーバリューストアでは、値のメタデータを扱う機能を含んでいますが、そのことによって二つドキュメントストアとの境界線が曖昧になってしまっています。*\\n\\n以上のことを実現するために、ドキュメントはコレクション、タグ、メタデータやディレクトリなどとして整理されています。ドキュメント同士はまとめてグループにできるものの、それぞれで全く異なるフィールドを持つ可能性があります。\\n\\n[MongoDB](https://www.mongodb.com/mongodb-architecture) や [CouchDB](https://blog.couchdb.org/2016/08/01/couchdb-2-0-architecture/) などのドキュメントストアも、複雑なクエリを処理するためのSQLのような言語を提供しています。[DynamoDB](http://www.read.seas.harvard.edu/~kohler/class/cs239-w08/decandia07dynamo.pdf) はキーバリューとドキュメントの両方をサポートしています。\\n\\nドキュメントストアは高い柔軟性を担保するので、頻繁に変化するデータを扱う時に用いられます。\\n\\n##### その他の参考資料、ページ:  ドキュメントストア\\n\\n* [ドキュメント指向 データベース](https://en.wikipedia.org/wiki/Document-oriented_database)\\n* [MongoDB アーキテクチャ](https://www.mongodb.com/mongodb-architecture)\\n* [CouchDB アーキテクチャ](https://blog.couchdb.org/2016/08/01/couchdb-2-0-architecture/)\\n* [Elasticsearch アーキテクチャ](https://www.elastic.co/blog/found-elasticsearch-from-the-bottom-up)\\n\\n#### ワイドカラムストア\\n\\n<p align=\"center\">\\n  <img src=\"images/n16iOGk.png\">\\n  <br/>\\n  <i><a href=http://blog.grio.com/2015/11/sql-nosql-a-brief-history.html>Source: SQL & NoSQL, a brief history</a></i>\\n</p>\\n\\n> 概要: ネストされたマップ `カラムファミリー<行キー、 カラム<ColKey、 Value、 Timestamp>>`\\n\\nワイドカラムストアのデータの基本単位はカラム（ネーム・バリューのペア）です。それぞれのカラムはカラムファミリーとして（SQLテーブルのように）グループ化することができます。スーパーカラムファミリーはカラムファミリーの集合です。それぞれのカラムには行キーでアクセスすることができます。同じ行キーを持つカラムは同じ行として認識されます。それぞれの値は、バージョン管理とコンフリクトが起きた時のために、タイムスタンプを含みます。\\n\\nGoogleは[Bigtable](http://www.read.seas.harvard.edu/~kohler/class/cs239-w08/chang06bigtable.pdf)を初のワイドカラムストアとして発表しました。それがオープンソースでHadoopなどでよく使われる[HBase](https://www.mapr.com/blog/in-depth-look-hbase-architecture) やFacebookによる[Cassandra](http://docs.datastax.com/en/archived/cassandra/2.0/cassandra/architecture/architectureIntro_c.html) などのプロジェクトに影響を与えました。BigTable、HBaseやCassandraなどのストアはキーを辞書形式で保持することで選択したキーレンジでのデータ取得を効率的にします。\\n\\nワイドカラムストアは高い可用性とスケーラビリティを担保します。これらはとても大規模なデータセットを扱うことによく使われます。\\n\\n##### その他の参考資料、ページ:  ワイドカラムストア\\n\\n* [SQL & NoSQL簡単に歴史をさらう](http://blog.grio.com/2015/11/sql-nosql-a-brief-history.html)\\n* [Bigtable アーキテクチャ](http://www.read.seas.harvard.edu/~kohler/class/cs239-w08/chang06bigtable.pdf)\\n* [HBase アーキテクチャ](https://www.mapr.com/blog/in-depth-look-hbase-architecture)\\n* [Cassandra アーキテクチャ](http://docs.datastax.com/en/archived/cassandra/2.0/cassandra/architecture/architectureIntro_c.html)\\n\\n#### グラフデータベース\\n\\n<p align=\"center\">\\n  <img src=\"images/fNcl65g.png\">\\n  <br/>\\n  <i><a href=https://en.wikipedia.org/wiki/File:GraphDatabase_PropertyGraph.png>Source: Graph database</a></i>\\n</p>\\n\\n> 概要: グラフ\\n\\nグラフデータベースでは、それぞれのノードがレコードで、それぞれのアークは二つのノードを繋ぐ関係性として定義されます。グラフデータベースは多数の外部キーや多対多などの複雑な関係性を表すのに最適です。\\n\\nグラフデータベースはSNSなどのサービスの複雑な関係性モデルなどについて高いパフォーマンスを発揮します。比較的新しく、まだ一般的には用いられていないので、開発ツールやリソースを探すのが他の方法に比べて難しいかもしれません。多くのグラフは[REST APIs](#representational-state-transfer-rest)を通じてのみアクセスできます。\\n\\n##### その他の参考資料、ページ:  グラフ\\n\\n* [Graphデータベース](https://en.wikipedia.org/wiki/Graph_database)\\n* [Neo4j](https://neo4j.com/)\\n* [FlockDB](https://blog.twitter.com/2010/introducing-flockdb)\\n\\n#### その他の参考資料、ページ:  NoSQL\\n\\n* [基本用語の説明](http://stackoverflow.com/questions/3342497/explanation-of-base-terminology)\\n* [NoSQLデータベースについて調査と選択ガイド](https://medium.com/baqend-blog/nosql-databases-a-survey-and-decision-guidance-ea7823a822d#.wskogqenq)\\n* [スケーラビリティ](http://www.lecloud.net/post/7994751381/scalability-for-dummies-part-2-database)\\n* [NoSQLのイントロダクション](https://www.youtube.com/watch?v=qI_g07C_Q5I)\\n* [NoSQLパターン](http://horicky.blogspot.com/2009/11/nosql-patterns.html)\\n\\n### SQLか？NoSQLか？\\n\\n<p align=\"center\">\\n  <img src=\"images/wXGqG5f.png\">\\n  <br/>\\n  <i><a href=https://www.infoq.com/articles/Transition-RDBMS-NoSQL/>Source: Transitioning from RDBMS to NoSQL</a></i>\\n</p>\\n\\n**SQL** を選ぶ理由:\\n\\n* 構造化されたデータ\\n* 厳格なスキーマ\\n* リレーショナルデータ\\n* 複雑なジョインをする必要性\\n* トランザクション\\n* スケールする際のパターンが明確なとき\\n* 開発者の数、コミュニティ、コード等がより充実している\\n* インデックスによるデータ探索はとても速い\\n\\n**NoSQL** を選ぶ理由:\\n\\n* 準構造化されたデータ\\n* ダイナミックないし、フレキシブルなスキーマ\\n* ノンリレーショナルなデータ\\n* 複雑なジョインをする必要がない\\n* データの多くのTB (もしくは PB) を保存する\\n* 集中的、大規模なデータ負荷に耐えられる\\n* IOPSについては極めて高いスループットを示す\\n\\nNoSQLに適するサンプルデータ:\\n\\n* 急激なクリックストリームやログデータの収集\\n* リーダーボードやスコアリングデータ\\n* ショッピングカートなどの一時的情報\\n* 頻繁にアクセスされる (\\'ホットな\\') テーブル\\n* メタデータやルックアップテーブル\\n\\n##### その他の参考資料、ページ:  \\u3000SQLもしくはNoSQL\\n\\n* [最初の1000万ユーザーにスケールアップするために](https://www.youtube.com/watch?v=w95murBkYmU)\\n* [SQLとNoSQLの違い](https://www.sitepoint.com/sql-vs-nosql-differences/)\\n\\n## キャッシュ\\n\\n<p align=\"center\">\\n  <img src=\"images/Q6z24La.png\">\\n  <br/>\\n  <i><a href=http://horicky.blogspot.com/2010/10/scalable-system-design-patterns.html>Source: Scalable system design patterns</a></i>\\n</p>\\n\\nキャッシュはページの読み込み時間を削減し、サーバーやデータベースへの負荷を低減することができます。このモデルでは、実際の処理を保存するために、ディスパッチャーがまず以前にリクエストが送信されたかどうかを確認し、直前の結果を受け取ります。\\n\\nデータベースはそのパーティションに渡って統合された読み取り書き込みの分配を要求しますが、人気アイテムはその分配を歪めてシステム全体のボトルネックになってしまうことがあります。データベースの前にキャッシュを差し込むことでこのように、均一でない負荷やトラフィックの急激な増加を吸収することができます。\\n\\n### クライアントキャッシング\\n\\nキャッシュはOSやブラウザーなどのクライアントサイド、[サーバーサイド](#リバースプロキシwebサーバー) もしくは独立のキャッシュレイヤーに設置することができます。\\n\\n### CDNキャッシング\\n\\n[CDN](#コンテンツデリバリーネットワークcontent-delivery-network) もキャッシュの一つとして考えることができます。\\n\\n### Webサーバーキャッシング\\n\\n[リバースプロキシ](#リバースプロキシwebサーバー) や [Varnish](https://www.varnish-cache.org/) などのキャッシュは静的そして動的なコンテンツを直接配信することができます。 webサーバーもリクエストをキャッシュしてアプリケーションサーバーに接続することなしにレスポンスを返すことができます。\\n\\n### データベースキャッシング\\n\\nデータベースは普通、一般的な使用状況に適するようなキャッシングの設定を初期状態で持っています。この設定を特定の仕様に合わせて調整することでパフォーマンスを向上させることができます。\\n\\n### アプリケーションキャッシング\\n\\nメムキャッシュなどのIn-memoryキャッシュやRedisはアプリケーションとデータストレージの間のキーバリューストアです。データはRAMで保持されるため、データがディスクで保存される一般的なデータベースよりもだいぶ速いです。RAM容量はディスクよりも限られているので、[least recently used (LRU)](https://en.wikipedia.org/wiki/Cache_algorithms#Least_Recently_Used)などの[cache invalidation](https://en.wikipedia.org/wiki/Cache_algorithms) アルゴリズムが \\'コールド\\' なエントリを弾き、\\'ホット\\' なデータをRAMに保存します。\\n\\nRedisはさらに以下のような機能を備えています:\\n\\n* パージステンス設定\\n* ソート済みセット、リストなどの組み込みデータ構造\\n\\nキャッシュには様々なレベルのものがありますが、いずれも大きく二つのカテゴリーのいずれかに分類することができます: **データベースクエリ** と **オブジェクト** です:\\n\\n* 行レベル\\n* クエリレベル\\n* Fully-formed serializable objects\\n* Fully-rendered HTML\\n\\n一般的に、ファイルベースキャッシングはクローンを作り出してオートスケーリングを難しくしてしまうので避けるべきです。\\n\\n### データベースクエリレベルでのキャッシング\\n\\nデータベースをクエリする際には必ずクエリをキーとしてハッシュして結果をキャッシュに保存しましょう。この手法はキャッシュ期限切れ問題に悩むことになります:\\n\\n* 複雑なクエリによりキャッシュされた結果を削除することが困難\\n* テーブルセルなどのデータ断片が変化した時に、その変化したセルを含むかもしれない全てのキャッシュされたクエリを削除する必要がある。\\n\\n### オブジェクトレベルでのキャッシング\\n\\nデータをアプリケーションコードでそうするように、オブジェクトとして捉えてみましょう。アプリケーションに、データベースからのデータセットをクラスインスタンスやデータ構造として組み立てさせます。:\\n\\n* そのデータが変更されたら、オブジェクトをキャッシュから削除すること\\n* 非同期処理を許容します: ワーカーがキャッシュされたオブジェクトの中で最新のものを集めてきます\\n\\n何をキャッシュするか:\\n\\n* ユーザーのセッション\\n* 完全にレンダーされたウェブページ\\n* アクテビティストリーム\\n* ユーザーグラフデータ\\n\\n### いつキャッシュを更新するか\\n\\nキャッシュに保存できる容量は限られているため、自分のケースではどのキャッシュ手法が一番いいかは検討する必要があります。\\n\\n#### キャッシュアサイド\\n\\n<p align=\"center\">\\n  <img src=\"images/ONjORqk.png\">\\n  <br/>\\n  <i><a href=http://www.slideshare.net/tmatyashovsky/from-cache-to-in-memory-data-grid-introduction-to-hazelcast>Source: From cache to in-memory data grid</a></i>\\n</p>\\n\\nアプリケーションはストレージへの読み書きの処理をします。キャッシュはストレージとは直接やりとりをしません。アプリケーションは以下のことをします:\\n\\n* キャッシュの中のエントリを参照しますが、結果としてキャッシュミスになります\\n* データベースからエントリを取得します\\n* エントリをキャッシュに追加します\\n* エントリを返します\\n\\n```python\\ndef get_user(self, user_id):\\n    user = cache.get(\"user.{0}\", user_id)\\n    if user is None:\\n        user = db.query(\"SELECT * FROM users WHERE user_id = {0}\", user_id)\\n        if user is not None:\\n            key = \"user.{0}\".format(user_id)\\n            cache.set(key, json.dumps(user))\\n    return user\\n```\\n\\n[Memcached](https://memcached.org/) は通常このように使われる。\\n\\nその後のキャッシュデータ読み込みは速いです。キャッシュアサイドはレージーローディングであるとも言われます。リクエストされたデータのみがキャッシュされ、リクエストされていないデータでキャッシュが溢れるのを防止します。\\n\\n##### 欠点: キャッシュアサイド\\n\\n* 各キャッシュミスは三つのトリップを呼び出すことになり、体感できるほどの遅延が起きてしまいます。\\n* データベースのデータが更新されるとキャッシュデータは古いものになってしまいます。time-to-live (TTL)を設定することでキャッシュエントリの更新を強制的に行う、もしくはライトスルーを採用することでこの問題は緩和できます。\\n* ノードが落ちると、新規の空のノードで代替されることでレイテンシーが増加することになります。\\n\\n#### ライトスルー\\n\\n<p align=\"center\">\\n  <img src=\"images/0vBc0hN.png\">\\n  <br/>\\n  <i><a href=http://www.slideshare.net/jboner/scalability-availability-stability-patterns/>Source: Scalability, availability, stability, patterns</a></i>\\n</p>\\n\\nアプリケーションはキャッシュをメインのデータストアとして使い、そこにデータの読み書きを行います。一方、キャッシュはデータベースへの読み書きを担当します。\\n\\n* アプリケーションはキャッシュにあるエントリを追加・更新します\\n* キャッシュは同期的にデータストアに書き込みを行います\\n* エントリを返します\\n\\nアプリケーションコード:\\n\\n```\\nset_user(12345, {\"foo\":\"bar\"})\\n```\\n\\nキャッシュコード:\\n\\n```python\\ndef set_user(user_id, values):\\n    user = db.query(\"UPDATE Users WHERE id = {0}\", user_id, values)\\n    cache.set(user_id, user)\\n```\\n\\nライトスルーは書き込み処理のせいで全体としては遅いオペレーションですが、書き込まれたばかりのデータに関する読み込みは速いです。ユーザー側は一般的にデータ更新時の方が読み込み時よりもレイテンシーに許容的です。キャッシュ内のデータは最新版で保たれます。\\n\\n##### 欠点: ライトスルー\\n\\n* ノードが落ちたこと、もしくはスケーリングによって新しいノードが作成された時に、新しいノードはデータベース内のエントリーが更新されるまではエントリーをキャッシュしません。キャッシュアサイドとライトスルーを併用することでこの問題を緩和できます。\\n* 書き込まれたデータの大部分は一度も読み込まれることはありません。このデータはTTLによって圧縮することができます。\\n\\n#### ライトビハインド (ライトバック)\\n\\n<p align=\"center\">\\n  <img src=\"images/rgSrvjG.png\">\\n  <br/>\\n  <i><a href=http://www.slideshare.net/jboner/scalability-availability-stability-patterns/>Source: Scalability, availability, stability, patterns</a></i>\\n</p>\\n\\nライトビハインドではアプリケーションは以下のことをします:\\n\\n* キャッシュのエントリーを追加・更新します\\n* データストアへの書き込みを非同期的に行うことで、書き込みパフォーマンスを向上させます。\\n\\n##### 欠点: ライトビハインド\\n\\n* キャッシュがデータストア内のコンテンツにヒットする前にキャッシュが落ちるとデータ欠損が起きる可能性があります。\\n* キャッシュアサイドやライトスルーよりも実装が複雑になります。\\n\\n#### リフレッシュアヘッド\\n\\n<p align=\"center\">\\n  <img src=\"images/kxtjqgE.png\">\\n  <br/>\\n  <i><a href=http://www.slideshare.net/tmatyashovsky/from-cache-to-in-memory-data-grid-introduction-to-hazelcast>Source: From cache to in-memory data grid</a></i>\\n</p>\\n\\n期限切れよりも前に、直近でアクセスされた全てのキャッシュエントリを自動的に更新するように設定することができます。\\n\\nもしどのアイテムが将来必要になるのかを正確に予測することができるのならば、リードスルーよりもレイテンシーを削減することができます。\\n\\n##### 欠点: リフレッシュアヘッド\\n\\n* どのアイテムが必要になるかの予測が正確でない場合にはリフレッシュアヘッドがない方がレイテンシーは良いという結果になってしまいます。\\n\\n### 欠点: キャッシュ\\n\\n* [cache invalidation](https://en.wikipedia.org/wiki/Cache_algorithms)などを用いて、データベースなどの真のデータとキャッシュの間の一貫性を保つ必要があります。\\n* Redisやmemcachedを追加することでアプリケーション構成を変更する必要があります。\\n* Cache invalidationも難しいですがそれに加えて、いつキャッシュを更新するかという複雑な問題にも悩まされることになります。\\n\\n### その他の参考資料、ページ\\n\\n* [From cache to in-memory data grid](http://www.slideshare.net/tmatyashovsky/from-cache-to-in-memory-data-grid-introduction-to-hazelcast)\\n* [スケーラブルなシステムデザインパターン](http://horicky.blogspot.com/2010/10/scalable-system-design-patterns.html)\\n* [スケールできるシステムを設計するためのイントロダクション](http://lethain.com/introduction-to-architecting-systems-for-scale/)\\n* [スケーラビリティ、可用性、安定性、パターン](http://www.slideshare.net/jboner/scalability-availability-stability-patterns/)\\n* [スケーラビリティ](http://www.lecloud.net/post/9246290032/scalability-for-dummies-part-3-cache)\\n* [AWS ElastiCacheのストラテジー](http://docs.aws.amazon.com/AmazonElastiCache/latest/UserGuide/Strategies.html)\\n* [Wikipedia](https://en.wikipedia.org/wiki/Cache_(computing))\\n\\n## 非同期処理\\n\\n<p align=\"center\">\\n  <img src=\"images/54GYsSx.png\">\\n  <br/>\\n  <i><a href=http://lethain.com/introduction-to-architecting-systems-for-scale/#platform_layer>Source: Intro to architecting systems for scale</a></i>\\n</p>\\n\\n非同期のワークフローはもし、連続的に行われるとリクエスト時間を圧迫してしまうような重い処理を別で処理する手法です。また、定期的にデータを集合させるなどの時間がかかるような処理を前もって処理しておくことにも役立ちます。\\n\\n### メッセージキュー\\n\\nメッセージキューはメッセージを受け取り、保存し、配信します。もし、処理がインラインで行うには遅すぎる場合、以下のようなワークフローでメッセージキューを用いるといいでしょう:\\n\\n* アプリケーションはジョブをキューに配信し、ユーザーにジョブステータスを伝えます。\\n* ワーカーがジョブキューから受け取って、処理を行い、終了したらそのシグナルを返します。\\n\\nユーザーの処理が止まることはなく、ジョブはバックグラウンドで処理されます。この間に、クライアントはオプションとして、タスクが完了したかのように見せるために小規模の処理を行います。例えば、ツイートを投稿するときに、ツイートはすぐにあなたのタイムラインに反映されたように見えますが、そのツイートが実際に全てのフォロワーに配信されるまでにはもう少し時間がかかっているでしょう。\\n\\n**Redis** はシンプルなメッセージ仲介としてはいいですが、メッセージが失われてしまう可能性があります。\\n\\n**RabbitMQ** はよく使われていますが、\\'AMQP\\'プロトコルに対応して、自前のノードを立てる必要があります。\\n\\n**Amazon SQS** という選択肢もありますが、レイテンシーが高く、メッセージが重複して配信されてしまう可能性があります。\\n\\n### タスクキュー\\n\\nタスクキューはタスクとその関連するデータを受け取り、処理した上でその結果を返します。スケジュール管理をできるほか、バックグラウンドでとても重いジョブをこなすこともできます。\\n\\n**Celery** はスケジューリングとpythonのサポートがあります。\\n\\n### バックプレッシャー\\n\\nもし、キューが拡大しすぎると、メモリーよりもキューの方が大きくなりキャッシュミスが起こり、ディスク読み出しにつながり、パフォーマンスが低下することにつながります。[バックプレッシャー](http://mechanical-sympathy.blogspot.com/2012/05/apply-back-pressure-when-overloaded.html)はキューサイズを制限することで回避することができ、高いスループットを確保しキューにすでにあるジョブについてのレスポンス時間を短縮できます。キューがいっぱいになると、クライアントはサーバービジーもしくはHTTP 503をレスポンスとして受け取りまた後で時間をおいてアクセスするようにメッセージを受け取ります。クライアントは[exponential backoff](https://en.wikipedia.org/wiki/Exponential_backoff)などによって後ほど再度時間を置いてリクエストすることができます。\\n\\n### 欠点: 非同期処理\\n\\n* キューを用いることで遅延が起こり、複雑さも増すため、あまり重くない計算処理やリアルタイムワークフローにおいては同期処理の方がいいでしょう。\\n\\n### その他の参考資料、ページ\\n\\n* [It\\'s all a numbers game](https://www.youtube.com/watch?v=1KRYH75wgy4)\\n* [オーバーロードした時にバックプレッシャーを適用する](http://mechanical-sympathy.blogspot.com/2012/05/apply-back-pressure-when-overloaded.html)\\n* [Little\\'s law](https://en.wikipedia.org/wiki/Little%27s_law)\\n* [メッセージキューとタスクキューの違いとは？](https://www.quora.com/What-is-the-difference-between-a-message-queue-and-a-task-queue-Why-would-a-task-queue-require-a-message-broker-like-RabbitMQ-Redis-Celery-or-IronMQ-to-function)\\n\\n## 通信\\n\\n<p align=\"center\">\\n  <img src=\"images/5KeocQs.jpg\">\\n  <br/>\\n  <i><a href=http://www.escotal.com/osilayer.html>Source: OSI 7 layer model</a></i>\\n</p>\\n\\n### Hypertext transfer protocol (HTTP)\\n\\nHTTP はクライアントとサーバー間でのデータをエンコードして転送するための手法です。リクエスト・レスポンスに関わるプロトコルです。クライアントがリクエストをサーバーに投げ、サーバーがリクエストに関係するコンテンツと完了ステータス情報をレスポンスとして返します。HTTPは自己完結するので、間にロードバランサー、キャッシュ、エンクリプション、圧縮などのどんな中間ルーターが入っても動くようにできています。\\n\\n基本的なHTTPリクエストはHTTP動詞(メソッド)とリソース(エンドポイント)で成り立っています。以下がよくあるHTTP動詞です。:\\n\\n| 動詞 | 詳細 | 冪等性* | セーフ | キャッシュできるか |\\n|---|---|---|---|---|\\n| GET | リソースを読み取る | Yes | Yes | Yes |\\n| POST | リソースを作成するもしくはデータを処理するトリガー | No | No | Yes レスポンスが新しい情報を含む場合 |\\n| PUT | リソースを作成もしくは入れ替える | Yes | No | No |\\n| PATCH | リソースを部分的に更新する | No | No | Yes レスポンスが新しい情報を含む場合 |\\n| DELETE | リソースを削除する | Yes | No | No |\\n\\n*何度呼んでも同じ結果が返ってくること*\\n\\nHTTPは**TCP** や **UDP** などの低級プロトコルに依存しているアプリケーションレイヤーのプロトコルである。\\n\\n#### その他の参考資料、ページ: HTTP\\n\\n* [HTTPってなに?](https://www.nginx.com/resources/glossary/http/)\\n* [HTTP と TCPの違い](https://www.quora.com/What-is-the-difference-between-HTTP-protocol-and-TCP-protocol)\\n* [PUT と PATCHの違い](https://laracasts.com/discuss/channels/general-discussion/whats-the-differences-between-put-and-patch?page=1)\\n\\n### 伝送制御プロトコル (TCP)\\n\\n<p align=\"center\">\\n  <img src=\"images/JdAsdvG.jpg\">\\n  <br/>\\n  <i><a href=http://www.wildbunny.co.uk/blog/2012/10/09/how-to-make-a-multi-player-game-part-1/>Source: How to make a multiplayer game</a></i>\\n</p>\\n\\nTCPは[IP network](https://en.wikipedia.org/wiki/Internet_Protocol)の上で成り立つ接続プロトコルです。接続は[handshake](https://en.wikipedia.org/wiki/Handshaking)によって開始、解除されます。全ての送信されたパケットは欠損なしで送信先に送信された順番で到達するように以下の方法で保証されています:\\n\\n* シーケンス番号と[checksum fields](https://en.wikipedia.org/wiki/Transmission_Control_Protocol#Checksum_computation)が全てのパケットに用意されている\\n* [Acknowledgement](https://en.wikipedia.org/wiki/Acknowledgement_(data_networks))パケットと自動再送信\\n\\nもし送信者が正しいレスポンスを受け取らなかったとき、パケットを再送信します。複数のタイムアウトがあったとき、接続は解除されます。TCP は[フロー制御](https://en.wikipedia.org/wiki/Flow_control_(data)) と [輻輳制御](https://en.wikipedia.org/wiki/Network_congestion#Congestion_control)も実装しています。これらの機能によって速度は低下し、一般的にUDPよりも非効率な転送手段になっています。\\n\\nハイスループットを実現するために、ウェブサーバーはかなり大きな数のTCP接続を開いておくことがあり、そのことでメモリー使用が圧迫されます。ウェブサーバスレッドと例えば[memcached](#memcached) サーバーの間で多数のコネクションを保っておくことは高くつくかもしれません。可能なところではUDPに切り替えるだけでなく[コネクションプーリング](https://en.wikipedia.org/wiki/Connection_pool)なども役立つかもしれません。\\n\\nTCPは高い依存性を要し、時間制約が厳しくないものに適しているでしょう。ウェブサーバー、データベース情報、SMTP、FTPやSSHなどの例に適用されます。\\n\\n以下の時にUDPよりもTCPを使うといいでしょう:\\n\\n* 全てのデータが欠損することなしに届いてほしい\\n* ネットワークスループットの最適な自動推測をしてオペレーションしたい\\n\\n### ユーザデータグラムプロトコル (UDP)\\n\\n<p align=\"center\">\\n  <img src=\"images/yzDrJtA.jpg\">\\n  <br/>\\n  <i><a href=http://www.wildbunny.co.uk/blog/2012/10/09/how-to-make-a-multi-player-game-part-1/>Source: How to make a multiplayer game</a></i>\\n</p>\\n\\nUDPはコネクションレスです。データグラム（パケットのようなもの）はデータグラムレベルでの保証しかされません。データグラムは順不同で受け取り先に到着したりそもそも着かなかったりします。UDPは輻輳制御をサポートしません。TCPにおいてはサポートされているこれらの保証がないため、UDPは一般的に、TCPよりも効率的です。\\n\\nUDPはサブネット上のすべての機器にデータグラムを送信することができます。これは[DHCP](https://en.wikipedia.org/wiki/Dynamic_Host_Configuration_Protocol) において役に立ちます。というのも、クライアントはまだIPアドレスを取得していないので、IPアドレスを必要とするTCPによるストリームができないからです。\\n\\nUDPは信頼性の面では劣りますが、VoIP、ビデオチャット、ストリーミングや同時通信マルチプレイヤーゲームなどのリアルタイム性が重視される時にはとても効果的です。\\n\\nTCPよりもUDPを使うのは:\\n\\n* レイテンシーを最低限に抑えたい時\\n* データ欠損よりも、データ遅延を重視するとき\\n* エラー修正を自前で実装したいとき\\n\\n#### その他の参考資料、ページ: TCP と UDP\\n\\n* [ゲームプログラミングのためのネットワーク](http://gafferongames.com/networking-for-game-programmers/udp-vs-tcp/)\\n* [TCP と UDP プロトコルの主な違い](http://www.cyberciti.biz/faq/key-differences-between-tcp-and-udp-protocols/)\\n* [TCP と UDPの違い](http://stackoverflow.com/questions/5970383/difference-between-tcp-and-udp)\\n* [Transmission control protocol](https://en.wikipedia.org/wiki/Transmission_Control_Protocol)\\n* [User datagram protocol](https://en.wikipedia.org/wiki/User_Datagram_Protocol)\\n* [Facebookのメムキャッシュスケーリング](http://www.cs.bu.edu/~jappavoo/jappavoo.github.com/451/papers/memcache-fb.pdf)\\n\\n### 遠隔手続呼出 (RPC)\\n\\n<p align=\"center\">\\n  <img src=\"images/iF4Mkb5.png\">\\n  <br/>\\n  <i><a href=http://www.puncsky.com/blog/2016-02-13-crack-the-system-design-interview>Source: Crack the system design interview</a></i>\\n</p>\\n\\nRPCではクライアントがリモートサーバーなどの異なるアドレス空間でプロシージャーが処理されるようにします。プロシージャーはローカルでのコールのように、クライアントからサーバーにどのように通信するかという詳細を省いた状態でコードが書かれます。リモートのコールは普通、ローカルのコールよりも遅く、信頼性に欠けるため、RPCコールをローカルコールと区別させておくことが好ましいでしょう。人気のRPCフレームワークは以下です。[Protobuf](https://developers.google.com/protocol-buffers/)、 [Thrift](https://thrift.apache.org/)、[Avro](https://avro.apache.org/docs/current/)\\n\\nRPC は リクエストレスポンスプロトコル:\\n\\n* **クライアントプログラム** - クライアントスタブプロシージャーを呼び出します。パラメータはローカルでのプロシージャーコールのようにスタックへとプッシュされていきます。\\n* **クライアントスタブプロシージャー** - プロシージャIDとアーギュメントをパックしてリクエストメッセージにします。\\n* **クライアント通信モジュール** - OSがクライアントからサーバーへとメッセージを送ります。\\n* **サーバー通信モジュール** - OSが受け取ったパケットをサーバースタブプロシージャーに受け渡します。\\n* **サーバースタブプロシージャー** -  結果を展開し、プロシージャーIDにマッチするサーバープロシージャーを呼び出し、結果を返します。\\n* サーバーレスポンスは上記のステップを逆順で繰り返します。\\n\\nSample RPC calls:\\n\\n```\\nGET /someoperation?data=anId\\n\\nPOST /anotheroperation\\n{\\n  \"data\":\"anId\";\\n  \"anotherdata\": \"another value\"\\n}\\n```\\n\\nRPCは振る舞いを公開することに焦点を当てています。RPCは内部通信パフォーマンスを理由として使われることが多いです。というのも、使用する状況に合わせてネイティブコールを自作することができるからです。\\n\\nネイティブライブラリー (aka SDK) を呼ぶのは以下の時:\\n\\n* ターゲットのプラットフォームを知っている時\\n* ロジックがどのようにアクセスされるのかを管理したいとき\\n* ライブラリー外でエラーがどのようにコントロールされるかを管理したい時\\n* パフォーマンスとエンドユーザーエクスペリエンスが最優先の時\\n\\n**REST** プロトコルに従うHTTP APIはパブリックAPIにおいてよく用いられます。\\n\\n#### 欠点: RPC\\n\\n* RPCクライアントとはサービス実装により厳密に左右されることになります。\\n* 新しいオペレーション、使用例があるたびに新しくAPIが定義されなければなりません。\\n* RPCをデバッグするのは難しい可能性があります。\\n* 既存のテクノロジーをそのまま使ってサービスを構築することはできないかもしれません。例えば、[Squid](http://www.squid-cache.org/)などのサーバーに[RPCコールが正しくキャッシュ](http://etherealbits.com/2012/12/debunking-the-myths-of-rpc-rest/) されるように追加で骨を折る必要があるかもしれません。\\n\\n### Representational state transfer (REST)\\n\\nRESTは、クライアントがサーバーによってマネージされるリソースに対して処理を行うクライアント・サーバーモデルを支持するアーキテキチャスタイルです。サーバーは操作できるもしくは新しいリソースレプレゼンテーションを受け取ることができるようなリソースやアクションのレプレゼンテーションを提供します。すべての通信はステートレスでキャッシュ可能でなければなりません。\\n\\nRESTful なインターフェースには次の四つの特徴があります:\\n\\n* **特徴的なリソース (URI in HTTP)** - どのオペレーションであっても同じURIを使う。\\n* **HTTP動詞によって変わる (Verbs in HTTP)** - 動詞、ヘッダー、ボディを使う\\n* **自己説明的なエラーメッセージ (status response in HTTP)** - ステータスコードを使い、新しく作ったりしないこと。\\n* **[HATEOAS](http://restcookbook.com/Basics/hateoas/) (HTML interface for HTTP)** - 自分のwebサービスがブラウザで完全にアクセスできること。\\n\\nサンプル REST コール:\\n\\n```\\nGET /someresources/anId\\n\\nPUT /someresources/anId\\n{\"anotherdata\": \"another value\"}\\n```\\n\\nRESTはデータを公開することに焦点を当てています。クライアントとサーバーのカップリングを最小限にするもので、パブリックAPIなどによく用いられます。RESTはURI、 [representation through headers](https://github.com/for-GET/know-your-http-well/blob/master/headers.md)、そして、GET、POST、PUT、 DELETE、PATCHなどのHTTP動詞等のよりジェネリックで統一されたメソッドを用います。ステートレスであるのでRESTは水平スケーリングやパーティショニングに最適です。\\n\\n#### 欠点: REST\\n\\n* RESTはデータ公開に焦点を当てているので、リソースが自然に整理されていなかったり、シンプルなヒエラルキーで表せられない時にはよい選択肢とは言えないかもしれません。例えば、とあるイベントのセットにマッチするすべての更新情報を返すと言った処理は簡単にはパスで表現することができません。RESTでは、URIパス、クエリパラメータ、そして場合によってはリクエストボディなどによって実装されることが多いでしょう。\\n* RESTは少数の動詞に依存しています(GET、POST、PUT、DELETE、そして PATCH) が時には使いたい事例に合わないことがあります。例えば、期限の切れたドキュメントをアーカイブに移したい場合などはこれらの動詞の中には綺麗にはフィットしません。\\n* ネストされたヒエラルキーの中にあるリソースをとってくるのはシングルビューを描画するのにクライアントとサーバー間で数回やりとりしなければなりません。例として、ブログエントリーのコンテンツとそれに対するコメントを表示する場合などです。様々なネットワーク環境で動作する可能性が考えられるモバイルアプリケーションにおいてはこのような複数のやり取りは好ましくありません。\\n* 時が経つにつれて、APIレスポンスにより多くのフィールドが与えられて、古いクライアントはすでにいらないものも含めてすべてのデータフィールドを受け取ることになります。そのことで、ペイロードが大きくなりすぎて、レイテンシーも拡大することになります。\\n\\n### RPCとREST比較\\n\\n| Operation | RPC | REST |\\n|---|---|---|\\n| サインアップ\\t| **POST** /signup | **POST** /persons |\\n| リザイン\\t| **POST** /resign<br/>{<br/>\"personid\": \"1234\"<br/>} | **DELETE** /persons/1234 |\\n| Person読み込み | **GET** /readPerson?personid=1234 | **GET** /persons/1234 |\\n| Personのアイテムリスト読み込み | **GET** /readUsersItemsList?personid=1234 | **GET** /persons/1234/items |\\n| Personのアイテムへのアイテム追加 | **POST** /addItemToUsersItemsList<br/>{<br/>\"personid\": \"1234\";<br/>\"itemid\": \"456\"<br/>} | **POST** /persons/1234/items<br/>{<br/>\"itemid\": \"456\"<br/>} |\\n| アイテム更新\\t| **POST** /modifyItem<br/>{<br/>\"itemid\": \"456\";<br/>\"key\": \"value\"<br/>} | **PUT** /items/456<br/>{<br/>\"key\": \"value\"<br/>} |\\n| アイテム削除 | **POST** /removeItem<br/>{<br/>\"itemid\": \"456\"<br/>} | **DELETE** /items/456 |\\n\\n<p align=\"center\">\\n  <i><a href=https://apihandyman.io/do-you-really-know-why-you-prefer-rest-over-rpc/>Source: Do you really know why you prefer REST over RPC</a></i>\\n</p>\\n\\n#### その他の参考資料、ページ: REST と RPC\\n\\n* [Do you really know why you prefer REST over RPC](https://apihandyman.io/do-you-really-know-why-you-prefer-rest-over-rpc/)\\n* [When are RPC-ish approaches more appropriate than REST?](http://programmers.stackexchange.com/a/181186)\\n* [REST vs JSON-RPC](http://stackoverflow.com/questions/15056878/rest-vs-json-rpc)\\n* [Debunking the myths of RPC and REST](http://etherealbits.com/2012/12/debunking-the-myths-of-rpc-rest/)\\n* [What are the drawbacks of using REST](https://www.quora.com/What-are-the-drawbacks-of-using-RESTful-APIs)\\n* [Crack the system design interview](http://www.puncsky.com/blog/2016-02-13-crack-the-system-design-interview)\\n* [Thrift](https://code.facebook.com/posts/1468950976659943/)\\n* [Why REST for internal use and not RPC](http://arstechnica.com/civis/viewtopic.php?t=1190508)\\n\\n## セキュリティ\\n\\nこのセクションは更新が必要です。[contributing](#contributing)してください！\\n\\nセキュリティは幅広いトピックです。十分な経験、セキュリティ分野のバックグラウンドがなくても、セキュリティの知識を要する職に応募するのでない限り、基本以上のことを知る必要はないでしょう。\\n\\n* 情報伝達、保存における暗号化\\n* [XSS](https://en.wikipedia.org/wiki/Cross-site_scripting) や [SQL injection](https://en.wikipedia.org/wiki/SQL_injection)を防ぐために、全てのユーザー入力もしくはユーザーに露出される入力パラメーターをサニタイズする\\n* SQL injectionを防ぐためにパラメータ化されたクエリを用いる。\\n* [least privilege](https://en.wikipedia.org/wiki/Principle_of_least_privilege)の原理を用いる\\n\\n### その他の参考資料、ページ:\\n\\n* [開発者のためのセキュリティガイド](https://github.com/FallibleInc/security-guide-for-developers)\\n* [OWASP top ten](https://www.owasp.org/index.php/OWASP_Top_Ten_Cheat_Sheet)\\n\\n## 補遺\\n\\n暗算で、推計値を求める必要があることも時にはあります。例えば、ディスクから100枚イメージ分のサムネイルを作る時間を求めたり、その時にどれだけディスクメモリーが消費されるかなどの値です。**2の乗数表** と **全てのプログラマーが知るべきレイテンシー値** は良い参考になるでしょう。\\n\\n### 2の乗数表\\n\\n```\\n乗数           厳密な値         約        Bytes\\n---------------------------------------------------------------\\n7                             128\\n8                             256\\n10                           1024   1 thousand           1 KB\\n16                         65,536                       64 KB\\n20                      1,048,576   1 million            1 MB\\n30                  1,073,741,824   1 billion            1 GB\\n32                  4,294,967,296                        4 GB\\n40              1,099,511,627,776   1 trillion           1 TB\\n```\\n\\n#### その他の参考資料、ページ:\\n\\n* [2の乗数表](https://en.wikipedia.org/wiki/Power_of_two)\\n\\n### 全てのプログラマーが知るべきレイテンシー値\\n\\n```\\nLatency Comparison Numbers\\n--------------------------\\nL1 cache reference                           0.5 ns\\nBranch mispredict                            5   ns\\nL2 cache reference                           7   ns                      14x L1 cache\\nMutex lock/unlock                           25   ns\\nMain memory reference                      100   ns                      20x L2 cache, 200x L1 cache\\nCompress 1K bytes with Zippy            10,000   ns       10 us\\nSend 1 KB bytes over 1 Gbps network     10,000   ns       10 us\\nRead 4 KB randomly from SSD*           150,000   ns      150 us          ~1GB/sec SSD\\nRead 1 MB sequentially from memory     250,000   ns      250 us\\nRound trip within same datacenter      500,000   ns      500 us\\nRead 1 MB sequentially from SSD*     1,000,000   ns    1,000 us    1 ms  ~1GB/sec SSD, 4X memory\\nDisk seek                           10,000,000   ns   10,000 us   10 ms  20x datacenter roundtrip\\nRead 1 MB sequentially from 1 Gbps  10,000,000   ns   10,000 us   10 ms  40x memory, 10X SSD\\nRead 1 MB sequentially from disk    30,000,000   ns   30,000 us   30 ms 120x memory, 30X SSD\\nSend packet CA->Netherlands->CA    150,000,000   ns  150,000 us  150 ms\\n\\nNotes\\n-----\\n1 ns = 10^-9 seconds\\n1 us = 10^-6 seconds = 1,000 ns\\n1 ms = 10^-3 seconds = 1,000 us = 1,000,000 ns\\n```\\n\\n上記表に基づいた役に立つ数値:\\n\\n* ディスクからの連続読み取り速度 30 MB/s\\n* 1 Gbps Ethernetからの連続読み取り速度\\u3000100 MB/s\\n* SSDからの連続読み取り速度 1 GB/s\\n* main memoryからの連続読み取り速度 4 GB/s\\n* 1秒で地球6-7周できる\\n* 1秒でデータセンターと2000周やりとりできる\\n\\n#### レイテンシーの視覚的表\\n\\n![](https://camo.githubusercontent.com/77f72259e1eb58596b564d1ad823af1853bc60a3/687474703a2f2f692e696d6775722e636f6d2f6b307431652e706e67)\\n\\n#### その他の参考資料、ページ:\\n\\n* [全てのプログラマーが知るべきレイテンシー値 - 1](https://gist.github.com/jboner/2841832)\\n* [全てのプログラマーが知るべきレイテンシー値 - 2](https://gist.github.com/hellerbarde/2843375)\\n* [Designs, lessons, and advice from building large distributed systems](http://www.cs.cornell.edu/projects/ladis2009/talks/dean-keynote-ladis2009.pdf)\\n* [Software Engineering Advice from Building Large-Scale Distributed Systems](https://static.googleusercontent.com/media/research.google.com/en//people/jeff/stanford-295-talk.pdf)\\n\\n### 他のシステム設計面接例題\\n\\n> 頻出のシステム設計面接課題とその解答へのリンク\\n\\n| 質問 | 解答 |\\n|---|---|\\n| Dropboxのようなファイル同期サービスを設計する | [youtube.com](https://www.youtube.com/watch?v=PE4gwstWhmc) |\\n| Googleのような検索エンジンの設計 | [queue.acm.org](http://queue.acm.org/detail.cfm?id=988407)<br/>[stackexchange.com](http://programmers.stackexchange.com/questions/38324/interview-question-how-would-you-implement-google-search)<br/>[ardendertat.com](http://www.ardendertat.com/2012/01/11/implementing-search-engines/)<br/>[stanford.edu](http://infolab.stanford.edu/~backrub/google.html) |\\n| Googleのようなスケーラブルなwebクローラーの設計 | [quora.com](https://www.quora.com/How-can-I-build-a-web-crawler-from-scratch) |\\n| Google docsの設計 | [code.google.com](https://code.google.com/p/google-mobwrite/)<br/>[neil.fraser.name](https://neil.fraser.name/writing/sync/) |\\n| Redisのようなキーバリューストアの設計 | [slideshare.net](http://www.slideshare.net/dvirsky/introduction-to-redis) |\\n| Memcachedのようなキャッシュシステムの設計 | [slideshare.net](http://www.slideshare.net/oemebamo/introduction-to-memcached) |\\n| Amazonのようなレコメンデーションシステムの設計 | [hulu.com](http://tech.hulu.com/blog/2011/09/19/recommendation-system.html)<br/>[ijcai13.org](http://ijcai13.org/files/tutorial_slides/td3.pdf) |\\n| BitlyのようなURL短縮サービスの設計 | [n00tc0d3r.blogspot.com](http://n00tc0d3r.blogspot.com/) |\\n| WhatsAppのようなチャットアプリの設計 | [highscalability.com](http://highscalability.com/blog/2014/2/26/the-whatsapp-architecture-facebook-bought-for-19-billion.html)\\n| Instagramのような写真共有サービスの設計 | [highscalability.com](http://highscalability.com/flickr-architecture)<br/>[highscalability.com](http://highscalability.com/blog/2011/12/6/instagram-architecture-14-million-users-terabytes-of-photos.html) |\\n| Facebookニュースフィードの設計 | [quora.com](http://www.quora.com/What-are-best-practices-for-building-something-like-a-News-Feed)<br/>[quora.com](http://www.quora.com/Activity-Streams/What-are-the-scaling-issues-to-keep-in-mind-while-developing-a-social-network-feed)<br/>[slideshare.net](http://www.slideshare.net/danmckinley/etsy-activity-feeds-architecture) |\\n| Facebookタイムラインの設計 | [facebook.com](https://www.facebook.com/note.php?note_id=10150468255628920)<br/>[highscalability.com](http://highscalability.com/blog/2012/1/23/facebook-timeline-brought-to-you-by-the-power-of-denormaliza.html) |\\n| Facebookチャットの設計 | [erlang-factory.com](http://www.erlang-factory.com/upload/presentations/31/EugeneLetuchy-ErlangatFacebook.pdf)<br/>[facebook.com](https://www.facebook.com/note.php?note_id=14218138919&id=9445547199&index=0) |\\n| Facebookのようなgraph検索の設計 | [facebook.com](https://www.facebook.com/notes/facebook-engineering/under-the-hood-building-out-the-infrastructure-for-graph-search/10151347573598920)<br/>[facebook.com](https://www.facebook.com/notes/facebook-engineering/under-the-hood-indexing-and-ranking-in-graph-search/10151361720763920)<br/>[facebook.com](https://www.facebook.com/notes/facebook-engineering/under-the-hood-the-natural-language-interface-of-graph-search/10151432733048920) |\\n| CloudFlareのようなCDNの設計 | [cmu.edu](http://repository.cmu.edu/cgi/viewcontent.cgi?article=2112&context=compsci) |\\n| Twitterのトレンド機能の設計 | [michael-noll.com](http://www.michael-noll.com/blog/2013/01/18/implementing-real-time-trending-topics-in-storm/)<br/>[snikolov .wordpress.com](http://snikolov.wordpress.com/2012/11/14/early-detection-of-twitter-trends/) |\\n| ランダムID発行システムの設計 | [blog.twitter.com](https://blog.twitter.com/2010/announcing-snowflake)<br/>[github.com](https://github.com/twitter/snowflake/) |\\n| 一定のインターバル時間での上位k件を返す | [ucsb.edu](https://icmi.cs.ucsb.edu/research/tech_reports/reports/2005-23.pdf)<br/>[wpi.edu](http://davis.wpi.edu/xmdv/docs/EDBT11-diyang.pdf) |\\n| 複数のデータセンターからデータを配信するサービスの設計 | [highscalability.com](http://highscalability.com/blog/2009/8/24/how-google-serves-data-from-multiple-datacenters.html) |\\n| オンラインの複数プレイヤーカードゲームの設計 | [indieflashblog.com](https://web.archive.org/web/20180929181117/http://www.indieflashblog.com/how-to-create-an-asynchronous-multiplayer-game.html)<br/>[buildnewgames.com](http://buildnewgames.com/real-time-multiplayer/) |\\n| ガーベッジコレクションシステムの設計 | [stuffwithstuff.com](http://journal.stuffwithstuff.com/2013/12/08/babys-first-garbage-collector/)<br/>[washington.edu](http://courses.cs.washington.edu/courses/csep521/07wi/prj/rick.pdf) |\\n| システム設計例題を追加する | [Contribute](#contributing) |\\n\\n### 実世界のアーキテクチャ\\n\\n> 世の中のシステムがどのように設計されているかについての記事\\n\\n<p align=\"center\">\\n  <img src=\"images/TcUo2fw.png\">\\n  <br/>\\n  <i><a href=https://www.infoq.com/presentations/Twitter-Timeline-Scalability>Source: Twitter timelines at scale</a></i>\\n</p>\\n\\n**以下の記事の重箱の隅をつつくような細かい詳細にこだわらないこと。むしろ**\\n\\n* 共通の原理、技術、パターンを探ること\\n* それぞれのコンポーネントでどんな問題が解決され、コンポーネントはどこでうまく使えもしくは使えないかを知ること\\n* 学んだことを復習すること\\n\\n|種類 | システム | 参考ページ |\\n|---|---|---|\\n| データ処理 | **MapReduce** - Googleの分散データ処理システム | [research.google.com](http://static.googleusercontent.com/media/research.google.com/zh-CN/us/archive/mapreduce-osdi04.pdf) |\\n| データ処理 | **Spark** - Databricksの分散データ処理システム | [slideshare.net](http://www.slideshare.net/AGrishchenko/apache-spark-architecture) |\\n| データ処理 | **Storm** - Twitterの分散データ処理システム | [slideshare.net](http://www.slideshare.net/previa/storm-16094009) |\\n| | | |\\n| データストア | **Bigtable** - Googleのカラム指向分散データベース | [harvard.edu](http://www.read.seas.harvard.edu/~kohler/class/cs239-w08/chang06bigtable.pdf) |\\n| データストア | **HBase** - Bigtableのオープンソース実装 | [slideshare.net](http://www.slideshare.net/alexbaranau/intro-to-hbase) |\\n| データストア | **Cassandra** - Facebookのカラム指向分散データベース | [slideshare.net](http://www.slideshare.net/planetcassandra/cassandra-introduction-features-30103666)\\n| データストア | **DynamoDB** - Amazonのドキュメント指向分散データベース | [harvard.edu](http://www.read.seas.harvard.edu/~kohler/class/cs239-w08/decandia07dynamo.pdf) |\\n| データストア | **MongoDB** - ドキュメント指向分散データベース | [slideshare.net](http://www.slideshare.net/mdirolf/introduction-to-mongodb) |\\n| データストア | **Spanner** - Googleのグローバル分散データベース | [research.google.com](http://research.google.com/archive/spanner-osdi2012.pdf) |\\n| データストア | **Memcached** - 分散メモリーキャッシングシステム | [slideshare.net](http://www.slideshare.net/oemebamo/introduction-to-memcached) |\\n| データストア | **Redis** - 永続性とバリュータイプを兼ね備えた分散メモリーキャッシングシステム | [slideshare.net](http://www.slideshare.net/dvirsky/introduction-to-redis) |\\n| | | |\\n| ファイルシステム | **Google File System (GFS)** - 分散ファイルシステム | [research.google.com](http://static.googleusercontent.com/media/research.google.com/zh-CN/us/archive/gfs-sosp2003.pdf) |\\n| ファイルシステム | **Hadoop File System (HDFS)** - GFSのオープンソース実装 | [apache.org](https://hadoop.apache.org/docs/r1.2.1/hdfs_design.html) |\\n| | | |\\n| Misc | **Chubby** - 疎結合の分散システムをロックするGoogleのサービス | [research.google.com](http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en/us/archive/chubby-osdi06.pdf) |\\n| Misc | **Dapper** - 分散システムを追跡するインフラ | [research.google.com](http://static.googleusercontent.com/media/research.google.com/en//pubs/archive/36356.pdf)\\n| Misc | **Kafka** - LinkedInによるPub/subメッセージキュー | [slideshare.net](http://www.slideshare.net/mumrah/kafka-talk-tri-hug) |\\n| Misc | **Zookeeper** - 同期を可能にする中央集権インフラとサービス | [slideshare.net](http://www.slideshare.net/sauravhaloi/introduction-to-apache-zookeeper) |\\n| | アーキテクチャを追加する | [Contribute](#contributing) |\\n\\n### 各企業のアーキテクチャ\\n\\n| 企業 | 参考ページ |\\n|---|---|\\n| Amazon | [Amazon architecture](http://highscalability.com/amazon-architecture) |\\n| Cinchcast | [Producing 1,500 hours of audio every day](http://highscalability.com/blog/2012/7/16/cinchcast-architecture-producing-1500-hours-of-audio-every-d.html) |\\n| DataSift | [Realtime datamining At 120,000 tweets per second](http://highscalability.com/blog/2011/11/29/datasift-architecture-realtime-datamining-at-120000-tweets-p.html) |\\n| DropBox | [How we\\'ve scaled Dropbox](https://www.youtube.com/watch?v=PE4gwstWhmc) |\\n| ESPN | [Operating At 100,000 duh nuh nuhs per second](http://highscalability.com/blog/2013/11/4/espns-architecture-at-scale-operating-at-100000-duh-nuh-nuhs.html) |\\n| Google | [Google architecture](http://highscalability.com/google-architecture) |\\n| Instagram | [14 million users, terabytes of photos](http://highscalability.com/blog/2011/12/6/instagram-architecture-14-million-users-terabytes-of-photos.html)<br/>[What powers Instagram](http://instagram-engineering.tumblr.com/post/13649370142/what-powers-instagram-hundreds-of-instances) |\\n| Justin.tv | [Justin.Tv\\'s live video broadcasting architecture](http://highscalability.com/blog/2010/3/16/justintvs-live-video-broadcasting-architecture.html) |\\n| Facebook | [Scaling memcached at Facebook](https://cs.uwaterloo.ca/~brecht/courses/854-Emerging-2014/readings/key-value/fb-memcached-nsdi-2013.pdf)<br/>[TAO: Facebook’s distributed data store for the social graph](https://cs.uwaterloo.ca/~brecht/courses/854-Emerging-2014/readings/data-store/tao-facebook-distributed-datastore-atc-2013.pdf)<br/>[Facebook’s photo storage](https://www.usenix.org/legacy/event/osdi10/tech/full_papers/Beaver.pdf) |\\n| Flickr | [Flickr architecture](http://highscalability.com/flickr-architecture) |\\n| Mailbox | [From 0 to one million users in 6 weeks](http://highscalability.com/blog/2013/6/18/scaling-mailbox-from-0-to-one-million-users-in-6-weeks-and-1.html) |\\n| Pinterest | [From 0 To 10s of billions of page views a month](http://highscalability.com/blog/2013/4/15/scaling-pinterest-from-0-to-10s-of-billions-of-page-views-a.html)<br/>[18 million visitors, 10x growth, 12 employees](http://highscalability.com/blog/2012/5/21/pinterest-architecture-update-18-million-visitors-10x-growth.html) |\\n| Playfish | [50 million monthly users and growing](http://highscalability.com/blog/2010/9/21/playfishs-social-gaming-architecture-50-million-monthly-user.html) |\\n| PlentyOfFish | [PlentyOfFish architecture](http://highscalability.com/plentyoffish-architecture) |\\n| Salesforce | [How they handle 1.3 billion transactions a day](http://highscalability.com/blog/2013/9/23/salesforce-architecture-how-they-handle-13-billion-transacti.html) |\\n| Stack Overflow | [Stack Overflow architecture](http://highscalability.com/blog/2009/8/5/stack-overflow-architecture.html) |\\n| TripAdvisor | [40M visitors, 200M dynamic page views, 30TB data](http://highscalability.com/blog/2011/6/27/tripadvisor-architecture-40m-visitors-200m-dynamic-page-view.html) |\\n| Tumblr | [15 billion page views a month](http://highscalability.com/blog/2012/2/13/tumblr-architecture-15-billion-page-views-a-month-and-harder.html) |\\n| Twitter | [Making Twitter 10000 percent faster](http://highscalability.com/scaling-twitter-making-twitter-10000-percent-faster)<br/>[Storing 250 million tweets a day using MySQL](http://highscalability.com/blog/2011/12/19/how-twitter-stores-250-million-tweets-a-day-using-mysql.html)<br/>[150M active users, 300K QPS, a 22 MB/S firehose](http://highscalability.com/blog/2013/7/8/the-architecture-twitter-uses-to-deal-with-150m-active-users.html)<br/>[Timelines at scale](https://www.infoq.com/presentations/Twitter-Timeline-Scalability)<br/>[Big and small data at Twitter](https://www.youtube.com/watch?v=5cKTP36HVgI)<br/>[Operations at Twitter: scaling beyond 100 million users](https://www.youtube.com/watch?v=z8LU0Cj6BOU) |\\n| Uber | [How Uber scales their real-time market platform](http://highscalability.com/blog/2015/9/14/how-uber-scales-their-real-time-market-platform.html) |\\n| WhatsApp | [The WhatsApp architecture Facebook bought for $19 billion](http://highscalability.com/blog/2014/2/26/the-whatsapp-architecture-facebook-bought-for-19-billion.html) |\\n| YouTube | [YouTube scalability](https://www.youtube.com/watch?v=w5WVu624fY8)<br/>[YouTube architecture](http://highscalability.com/youtube-architecture) |\\n\\n### 企業のエンジニアブログ\\n\\n> 面接を受ける企業のアーキテクチャ\\n>\\n> 投げられる質問は同じ分野から来ることもあるでしょう\\n\\n* [Airbnb Engineering](http://nerds.airbnb.com/)\\n* [Atlassian Developers](https://developer.atlassian.com/blog/)\\n* [Autodesk Engineering](http://cloudengineering.autodesk.com/blog/)\\n* [AWS Blog](https://aws.amazon.com/blogs/aws/)\\n* [Bitly Engineering Blog](http://word.bitly.com/)\\n* [Box Blogs](https://www.box.com/blog/engineering/)\\n* [Cloudera Developer Blog](http://blog.cloudera.com/blog/)\\n* [Dropbox Tech Blog](https://tech.dropbox.com/)\\n* [Engineering at Quora](http://engineering.quora.com/)\\n* [Ebay Tech Blog](http://www.ebaytechblog.com/)\\n* [Evernote Tech Blog](https://blog.evernote.com/tech/)\\n* [Etsy Code as Craft](http://codeascraft.com/)\\n* [Facebook Engineering](https://www.facebook.com/Engineering)\\n* [Flickr Code](http://code.flickr.net/)\\n* [Foursquare Engineering Blog](http://engineering.foursquare.com/)\\n* [GitHub Engineering Blog](https://github.blog/category/engineering)\\n* [Google Research Blog](http://googleresearch.blogspot.com/)\\n* [Groupon Engineering Blog](https://engineering.groupon.com/)\\n* [Heroku Engineering Blog](https://engineering.heroku.com/)\\n* [Hubspot Engineering Blog](http://product.hubspot.com/blog/topic/engineering)\\n* [High Scalability](http://highscalability.com/)\\n* [Instagram Engineering](http://instagram-engineering.tumblr.com/)\\n* [Intel Software Blog](https://software.intel.com/en-us/blogs/)\\n* [Jane Street Tech Blog](https://blogs.janestreet.com/category/ocaml/)\\n* [LinkedIn Engineering](http://engineering.linkedin.com/blog)\\n* [Microsoft Engineering](https://engineering.microsoft.com/)\\n* [Microsoft Python Engineering](https://blogs.msdn.microsoft.com/pythonengineering/)\\n* [Netflix Tech Blog](http://techblog.netflix.com/)\\n* [Paypal Developer Blog](https://devblog.paypal.com/category/engineering/)\\n* [Pinterest Engineering Blog](http://engineering.pinterest.com/)\\n* [Quora Engineering](https://engineering.quora.com/)\\n* [Reddit Blog](http://www.redditblog.com/)\\n* [Salesforce Engineering Blog](https://developer.salesforce.com/blogs/engineering/)\\n* [Slack Engineering Blog](https://slack.engineering/)\\n* [Spotify Labs](https://labs.spotify.com/)\\n* [Twilio Engineering Blog](http://www.twilio.com/engineering)\\n* [Twitter Engineering](https://engineering.twitter.com/)\\n* [Uber Engineering Blog](http://eng.uber.com/)\\n* [Yahoo Engineering Blog](http://yahooeng.tumblr.com/)\\n* [Yelp Engineering Blog](http://engineeringblog.yelp.com/)\\n* [Zynga Engineering Blog](https://www.zynga.com/blogs/engineering)\\n\\n#### その他の参考資料、ページ:\\n\\n* [kilimchoi/engineering-blogs](https://github.com/kilimchoi/engineering-blogs)\\n\\nここにあるリストは比較的小規模なものにとどめ、[kilimchoi/engineering-blogs](https://github.com/kilimchoi/engineering-blogs)により詳細に記すことで重複しないようにしておくことにする。エンジニアブログへのリンクを追加する場合はここではなく、engineering-blogsレボジトリに追加することを検討してください。\\n\\n## 進行中の作業\\n\\nセクションの追加や、進行中の作業を手伝っていただける場合は[こちら](#contributing)!\\n\\n* MapReduceによる分散コンピューティング\\n* Consistent hashing\\n* Scatter gather\\n* [Contribute](#contributing)\\n\\n## クレジット\\n\\nクレジット及び、参照ページは適時このリポジトリ内に記載してあります\\n\\nSpecial thanks to:\\n\\n* [Hired in tech](http://www.hiredintech.com/system-design/the-system-design-process/)\\n* [Cracking the coding interview](https://www.amazon.com/dp/0984782850/)\\n* [High scalability](http://highscalability.com/)\\n* [checkcheckzz/system-design-interview](https://github.com/checkcheckzz/system-design-interview)\\n* [shashank88/system_design](https://github.com/shashank88/system_design)\\n* [mmcgrana/services-engineering](https://github.com/mmcgrana/services-engineering)\\n* [System design cheat sheet](https://gist.github.com/vasanthk/485d1c25737e8e72759f)\\n* [A distributed systems reading list](http://dancres.github.io/Pages/)\\n* [Cracking the system design interview](http://www.puncsky.com/blog/2016-02-13-crack-the-system-design-interview)\\n\\n## Contact info\\n\\nFeel free to contact me to discuss any issues, questions, or comments.\\n\\nMy contact info can be found on my [GitHub page](https://github.com/donnemartin).\\n\\n## License\\n\\n*I am providing code and resources in this repository to you under an open source license.  Because this is my personal repository, the license you receive to my code and resources is from me and not my employer (Facebook).*\\n\\n    Copyright 2017 Donne Martin\\n\\n    Creative Commons Attribution 4.0 International License (CC BY 4.0)\\n\\n    http://creativecommons.org/licenses/by/4.0/\\n'},\n",
       " {'repo': './Z4nzu/hackingtool',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '### All in One Hacking tool For Hackers🥇\\n![](https://img.shields.io/github/license/Z4nzu/hackingtool)\\n![](https://img.shields.io/github/issues/Z4nzu/hackingtool)\\n![](https://img.shields.io/github/issues-closed/Z4nzu/hackingtool)\\n![](https://img.shields.io/badge/Python-3-blue)\\n![](https://img.shields.io/github/forks/Z4nzu/hackingtool)\\n![](https://img.shields.io/github/stars/Z4nzu/hackingtool)\\n![](https://img.shields.io/github/last-commit/Z4nzu/hackingtool)\\n[![HitCount](http://hits.dwyl.com/Z4nzu/hackingtool.svg)](http://hits.dwyl.com/Z4nzu/hackingtool)\\n![](https://img.shields.io/badge/platform-Linux%20%7C%20KaliLinux%20%7C%20ParrotOs-blue)\\n\\n#### Install Kali Linux in WIndows10 Without VirtualBox [YOUTUBE](https://youtu.be/BsFhpIDcd9I) or use Docker\\n\\n## Update Available V1.2.0 🚀 \\n- [✔] Installation Bug Fixed\\n- [x] Added New Tools \\n    - [x] Reverse Engineering\\n    - [x] RAT Tools\\n    - [x] Web Crawling \\n    - [x] Payload Injector\\n- [x] Multitor Tools update\\n- [X] Added Tool in wifijamming\\n- [X] Added Tool in steganography\\n\\n\\n\\n# Hackingtool Menu 🧰\\n- [Anonymously Hiding Tools](#anonymously-hiding-tools)\\n- [Information gathering tools](#information-gathering-tools)\\n- [Wordlist Generator](#wordlist-generator)\\n- [Wireless attack tools](#wireless-attack-tools)\\n- [SQL Injection Tools](#sql-injection-tools)\\n- [Phishing attack tools](#phishing-attack-tools)\\n- [Web Attack tools](#web-attack-tools)\\n- [Post exploitation tools](#post-exploitation-tools)\\n- [Forensic tools](#forensic-tools)\\n- [Payload creation tools](#payload-creation-tools)\\n- [Exploit framework](#exploit-framework)\\n- [Reverse engineering tools](#reverse-engineering-tools)\\n- [DDOS Attack Tools](#ddos-attack-tools)\\n- [Remote Administrator Tools (RAT)](#remote-administrator-tools--rat-)\\n- [XSS Attack Tools](#xss-attack-tools)\\n- [Steganograhy tools](#steganograhy-tools)\\n- [Other tools](#other-tools)\\n    - [SocialMedia Bruteforce](#socialmedia-bruteforce)\\n    - [Android Hacking tools](#android-hacking-tools)\\n    - [IDN Homograph Attack](#idn-homograph-attack)\\n    - [Email Verify tools](#email-verify-tools)\\n    - [Hash cracking tools](#hash-cracking-tools)\\n    - [Wifi Deauthenticate](#wifi-deauthenticate)\\n    - [SocialMedia Finder](#socialmedia-finder)\\n    - [Payload Injector](#payload-injector)\\n    - [Web crawling](#web-crawling)\\n    - [Mix tools](#mix-tools)\\n\\n\\n### Anonymously Hiding Tools\\n- [Anonmously Surf](https://github.com/Und3rf10w/kali-anonsurf)\\n- [Multitor](https://github.com/trimstray/multitor)\\n### Information gathering tools\\n- [Network Map (nmap)](https://github.com/nmap/nmap)\\n- [Dracnmap](https://github.com/Screetsec/Dracnmap)\\n- Port scanning\\n- Host to IP \\n- [Xerosploit](https://github.com/LionSec/xerosploit)\\n- [RED HAWK (All In One Scanning)](https://github.com/Tuhinshubhra/RED_HAWK)\\n- [ReconSpider(For All Scanning)](https://github.com/bhavsec/reconspider)\\n- IsItDown (Check Website Down/Up)\\n- [Infoga - Email OSINT](https://github.com/m4ll0k/Infoga)\\n- [ReconDog](https://github.com/s0md3v/ReconDog)\\n- [Striker](https://github.com/s0md3v/Striker)\\n- [SecretFinder (like API & etc)](https://github.com/m4ll0k/SecretFinder)\\n- [Find Info Using Shodan](https://github.com/m4ll0k/Shodanfy.py)\\n- [Port Scanner - rang3r (Python 2.7)](https://github.com/floriankunushevci/rang3r)\\n- [Port Scanner - Ranger Reloaded (Python 3+)](https://github.com/joeyagreco/ranger-reloaded)\\n- [Breacher](https://github.com/s0md3v/Breacher)\\n### Wordlist Generator\\n- [Cupp](https://github.com/Mebus/cupp.git)\\n- [WordlistCreator](https://github.com/Z4nzu/wlcreator)\\n- [Goblin WordGenerator](https://github.com/UndeadSec/GoblinWordGenerator.git)\\n- [Password list (1.4 Billion Clear Text Password)](https://github.com/Viralmaniar/SMWYG-Show-Me-What-You-Got)\\n### Wireless attack tools\\n- [WiFi-Pumpkin](https://github.com/P0cL4bs/wifipumpkin3)\\n- [pixiewps](https://github.com/wiire/pixiewps)\\n- [Bluetooth Honeypot GUI Framework](https://github.com/andrewmichaelsmith/bluepot)\\n- [Fluxion](https://github.com/thehackingsage/Fluxion)\\n- [Wifiphisher](https://github.com/wifiphisher/wifiphisher)\\n- [Wifite](https://github.com/derv82/wifite2)\\n- [EvilTwin](https://github.com/Z4nzu/fakeap)\\n- [Fastssh](https://github.com/Z4nzu/fastssh)\\n- Howmanypeople\\n### SQL Injection Tools\\n- [Sqlmap tool](https://github.com/sqlmapproject/sqlmap)\\n- [NoSqlMap](https://github.com/codingo/NoSQLMap)\\n- [Damn Small SQLi Scanner](https://github.com/stamparm/DSSS)\\n- [Explo](https://github.com/dtag-dev-sec/explo)\\n- [Blisqy - Exploit Time-based blind-SQL injection](https://github.com/JohnTroony/Blisqy)\\n- [Leviathan - Wide Range Mass Audit Toolkit](https://github.com/leviathan-framework/leviathan)\\n- [SQLScan](https://github.com/Cvar1984/sqlscan)\\n### Phishing attack tools\\n- [Setoolkit](https://github.com/trustedsec/social-engineer-toolkit)\\n- [SocialFish](https://github.com/UndeadSec/SocialFish)\\n- [HiddenEye](https://github.com/DarkSecDevelopers/HiddenEye)\\n- [Evilginx2](https://github.com/kgretzky/evilginx2)\\n- [I-See_You(Get Location using phishing attack)](https://github.com/Viralmaniar/I-See-You)\\n- [SayCheese (Grab target\\'s Webcam Shots)](https://github.com/hangetzzu/saycheese)\\n- [QR Code Jacking](https://github.com/cryptedwolf/ohmyqr)\\n- [ShellPhish](https://github.com/An0nUD4Y/shellphish)\\n- [BlackPhish](https://github.com/iinc0gnit0/BlackPhish)\\n### Web Attack tools\\n- [Web2Attack](https://github.com/santatic/web2attack)\\n- Skipfish\\n- [SubDomain Finder](https://github.com/aboul3la/Sublist3r)\\n- [CheckURL](https://github.com/UndeadSec/checkURL)\\n- [Blazy(Also Find ClickJacking)](https://github.com/UltimateHackers/Blazy)\\n- [Sub-Domain TakeOver](https://github.com/m4ll0k/takeover)\\n- [Dirb](https://gitlab.com/kalilinux/packages/dirb)\\n### Post exploitation tools\\n- [Vegile - Ghost In The Shell](https://github.com/Screetsec/Vegile)\\n- [Chrome Keylogger](https://github.com/UndeadSec/HeraKeylogger)\\n### Forensic tools\\n- Autopsy\\n- Wireshark\\n- [Bulk extractor](https://github.com/simsong/bulk_extractor)\\n- [Disk Clone and ISO Image Acquire](https://guymager.sourceforge.io/)\\n- [Toolsley](https://www.toolsley.com/)\\n### Payload creation tools\\n- [The FatRat](https://github.com/Screetsec/TheFatRat)\\n- [Brutal](https://github.com/Screetsec/Brutal)\\n- [Stitch](https://nathanlopez.github.io/Stitch)\\n- [MSFvenom Payload Creator](https://github.com/g0tmi1k/msfpc)\\n- [Venom Shellcode Generator](https://github.com/r00t-3xp10it/venom)\\n- [Spycam](https://github.com/indexnotfound404/spycam)\\n- [Mob-Droid](https://github.com/kinghacker0/Mob-Droid)\\n- [Enigma](https://github.com/UndeadSec/Enigma)\\n### Exploit framework\\n- [RouterSploit](https://github.com/threat9/routersploit)\\n- [WebSploit](https://github.com/The404Hacking/websploit )\\n- [Commix](https://github.com/commixproject/commix)\\n- [Web2Attack](https://github.com/santatic/web2attack)\\n### Reverse engineering tools\\n- [Androguard](https://github.com/androguard/androguard )\\n- [Apk2Gold](https://github.com/lxdvs/apk2gold )\\n- [JadX](https://github.com/skylot/jadx)\\n### DDOS Attack Tools\\n- SlowLoris\\n- [Asyncrone | Multifunction SYN Flood DDoS Weapon](https://github.com/fatihsnsy/aSYNcrone)\\n- [UFOnet](https://github.com/epsylon/ufonet)\\n- [GoldenEye](https://github.com/jseidl/GoldenEye)\\n### Remote Administrator Tools (RAT)\\n- [Stitch](https://github.com/nathanlopez/Stitch)\\n- [Pyshell](https://github.com/knassar702/pyshell)\\n### XSS Attack Tools\\n- [DalFox(Finder of XSS)](https://github.com/hahwul/dalfox)\\n- [XSS Payload Generator](https://github.com/capture0x/XSS-LOADER.git)\\n- [Extended XSS Searcher and Finder](https://github.com/Damian89/extended-xss-search)\\n- [XSS-Freak](https://github.com/PR0PH3CY33/XSS-Freak)\\n- [XSpear](https://github.com/hahwul/XSpear)\\n- [XSSCon](https://github.com/menkrep1337/XSSCon)\\n- [XanXSS](https://github.com/Ekultek/XanXSS)\\n- [Advanced XSS Detection Suite](https://github.com/UltimateHackers/XSStrike)\\n- [RVuln](https://github.com/iinc0gnit0/RVuln)\\n- [Cyclops](https://github.com/v8blink/Chromium-based-XSS-Taint-Tracking) \\n### Steganograhy tools\\n- SteganoHide\\n- StegnoCracker\\n- [StegoCracker](https://github.com/W1LDN16H7/StegoCracker)\\n- [Whitespace](https://github.com/beardog108/snow10)\\n### Other tools\\n#### SocialMedia Bruteforce\\n- [Instagram Attack](https://github.com/chinoogawa/instaBrute)\\n- [AllinOne SocialMedia Attack](https://github.com/Matrix07ksa/Brute_Force)\\n- [Facebook Attack](https://github.com/Matrix07ksa/Brute_Force)\\n- [Application Checker](https://github.com/jakuta-tech/underhanded)\\n#### Android Hacking tools\\n- [Keydroid](https://github.com/F4dl0/keydroid)\\n- [MySMS](https://github.com/papusingh2sms/mysms)\\n- [Lockphish (Grab target LOCK PIN)](https://github.com/JasonJerry/lockphish)\\n- [DroidCam (Capture Image)](https://github.com/kinghacker0/WishFish)\\n- [EvilApp (Hijack Session)](https://github.com/crypticterminal/EvilApp)\\n- [HatCloud(Bypass CloudFlare for IP)](https://github.com/HatBashBR/HatCloud)\\n#### IDN Homograph Attack\\n- [EvilURL](https://github.com/UndeadSec/EvilURL)\\n#### Email Verify tools\\n- [Knockmail](https://github.com/4w4k3/KnockMail)\\n#### Hash cracking tools\\n- [Hash Buster](https://github.com/s0md3v/Hash-Buster)\\n#### Wifi Deauthenticate\\n- [WifiJammer-NG](https://github.com/MisterBianco/wifijammer-ng)\\n- [KawaiiDeauther](https://github.com/aryanrtm/KawaiiDeauther)\\n#### SocialMedia Finder\\n- [Find SocialMedia By Facial Recognation System](https://github.com/Greenwolf/social_mapper)\\n- [Find SocialMedia By UserName](https://github.com/xHak9x/finduser)\\n- [Sherlock](https://github.com/sherlock-project/sherlock)\\n- [SocialScan | Username or Email](https://github.com/iojw/socialscan)\\n#### Payload Injector\\n- [Debinject](https://github.com/UndeadSec/Debinject)\\n- [Pixload](https://github.com/chinarulezzz/pixload)\\n#### Web crawling\\n- [Gospider](https://github.com/jaeles-project/gospider)\\n#### Mix tools\\n- Terminal Multiplexer\\n\\n\\n![](https://github.com/Z4nzu/hackingtool/blob/master/images/A00.png)\\n![](https://github.com/Z4nzu/hackingtool/blob/master/images/A0.png)\\n![](https://github.com/Z4nzu/hackingtool/blob/master/images/A1.png)\\n![](https://github.com/Z4nzu/hackingtool/blob/master/images/A2.png)\\n![](https://github.com/Z4nzu/hackingtool/blob/master/images/A4.png)\\n\\n## Installation For Linux <img src=\"https://konpa.github.io/devicon/devicon.git/icons/linux/linux-original.svg\" alt=\"linux\" width=\"25\" height=\"25\"/></p><p align=\"center\">\\n\\n#### This Tool Must Run As ROOT !!!\\n\\n    git clone https://github.com/Z4nzu/hackingtool.git\\n    \\n    chmod -R 755 hackingtool  \\n    \\n    cd hackingtool\\n    \\n    sudo bash install.sh\\n    \\n    sudo hackingtool\\n\\n After Following All Steps Just Type In Terminal **root@kaliLinux:~** **hackingtool**\\n\\n## Use image with Docker\\n\\n### Run in one click\\n`docker run -it vgpastor/hackingtool`\\n\\n### Build locally\\n`docker-compose build`\\n\\n`docker-compose run hackingtool`\\n\\n- If need open other ports you can edit the docker-compose.yml file\\n- Volumes are mounted in the container to persist data and can share files between the host and the container\\n\\n\\n#### Thanks to original Author of the tools used in hackingtool\\n\\n<img src =\"https://img.shields.io/badge/Important-notice-red\" />\\n<h4>Please Don\\'t Use for illegal Activity</h4>\\n\\n### To do \\n- [ ] Release Tool \\n- [ ] Add Tools for CTF\\n- [ ] Want to do automatic \\n\\n## Social Media :mailbox_with_no_mail:\\n[![Twitter](https://img.shields.io/twitter/url?color=%231DA1F2&label=follow&logo=twitter&logoColor=%231DA1F2&style=flat-square&url=https%3A%2F%2Fwww.reddit.com%2Fuser%2FFatChicken277)](https://twitter.com/_Zinzu07)\\n[![GitHub](https://img.shields.io/badge/-GitHub-181717?style=flat-square&logo=github&link=https://github.com/Z4nzu/)](https://github.com/Z4nzu/)\\n##### Your Favourite Tool is not in hackingtool or Suggestions Please [CLICK HERE](https://forms.gle/b235JoCKyUq5iM3t8)\\n![Z4nzu\\'s github stats](https://github-readme-stats.vercel.app/api?username=Z4nzu&show_icons=true&title_color=fff&icon_color=79ff97&text_color=9f9f9f&bg_color=151515)\\n\\n#### Don\\'t Forgot to share with Your Friends \\n### The new Update get will soon stay updated\\n#### Thank you..!!\\n'},\n",
       " {'repo': './ChanseyIsTheBest/NX-60FPS-RES-GFX-Cheats',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '# NX-60FPS-RES-GFX-Cheats\\n\\n![fpsdemo1](https://user-images.githubusercontent.com/119771197/205493192-ad157f47-7060-4fa7-9af6-12b9027361a8.gif)\\n\\n## About\\n\\nA complete database of 60FPS cheats, resolution cheats and general graphics cheats for Nintendo Switch games.\\n\\nThese cheats are designed for usage on real Nintendo Switch hardware running custom firmware and may not work on emulators.\\n\\nDownload these cheats directly on your Switch using [**AIO-Switch-Updater**](https://github.com/HamletDuFromage/aio-switch-updater)\\n\\nYou can search for games that have cheats by using the automatically updated [**Games List**](https://github.com/ChanseyIsTheBest/NX-60FPS-RES-GFX-Cheats/blob/main/GAMES.md) \\n* selecting the game **NAME** will take you to a file that contains a credit for the cheat author(s) as well as any information they\\'ve included about the cheat\\n* selecting a **TITLE ID** will take you to the game\\'s TitleID directory which contains the credit/info file and the cheats directory for the game\\n* selecting a **BUILD ID** or **VERSION** will take you directly to the corresponding cheat file\\n* available cheat types per game and cheat availability for the latest game version are indicated by icons according to the key\\n\\nTo mass download the entire cheat repository on a computer click \"Code\" -> \"Download ZIP\", extract the zip and copy the TitleID folders to your SD card under `sdmc:/atmosphere/contents` for usage\\n\\n## Recommended Switch tools\\n\\nThe latest versions of the following custom firmware and homebrew tools are recommended to get the most out of these cheats. Without overclocking most of the cheats will not work to their proper potential and in many cases handheld profiles should be forced to allow for more performance headroom.\\n\\n#### Custom firmware:\\n\\n[**Atmosphère**](https://github.com/Atmosphere-NX/Atmosphere)\\n\\n#### Cheat downloads:\\n\\n[**AIO-Switch-Updater**](https://github.com/HamletDuFromage/aio-switch-updater) to download cheats from this database directly on your Switch either as a batch download for all detected installed titles or as individual cheats for detected installed titles\\n\\n#### Overclocking:\\n\\n**You are responsible for your own usage of overclocking software as it may reduce the life of the hardware!**\\n\\n[**sys-clk**](https://github.com/retronx-team/sys-clk) homebrew overclocking/underclocking system module and frontend\\n\\n#### Overlays:\\n\\nThe [**Tesla sysmodule**](https://github.com/WerWolv/nx-ovlloader) and [**Overlay menu**](https://github.com/WerWolv/Tesla-Menu)\\n\\n#### FPS, clock speed, temperature monitoring and more:\\n\\nThe [**SaltyNX sysmodule**](https://github.com/masagrator/SaltyNX) along with [**Status Monitor Overlay**](https://github.com/masagrator/Status-Monitor-Overlay), the [**NX-FPS plugin**](https://github.com/masagrator/NX-FPS) and [**FPSLocker Overlay**](https://github.com/masagrator/FPSLocker)\\n\\n#### Cheat software:\\n\\n[**EdiZon-SE homebrew**](https://github.com/tomvita/EdiZon-SE) and this special version of [**EdiZon Overlay**](https://github.com/proferabg/EdiZon-Overlay)\\n\\n#### Other:\\n\\n[**ReverseNX-Tool homebrew**](https://github.com/masagrator/ReverseNX-Tool) for forcing docked mode in handheld or vice versa as well as the [**ReverseNX-RT overlay and plugin**](https://github.com/masagrator/ReverseNX-RT) for performing the same functions in-game on the fly\\n\\n## Contributing cheats\\n\\nThe best ways to contribute your cheats are either in the [discord server](#community-and-contact) or by submitting a pull request that uses the following directory structure:\\n\\n`titles/TitleID/Game Name.txt`\\n\\n`titles/TitleID/cheats/BuildID.txt`\\n\\nEnsure cheats are tested. Include your credit and any additional notes you wish to include in the `Game Name.txt`\\n\\n`[Cheat tags]` should only be used for cheat names, do not include any comments within the cheat file such as the title comment automatically generated by Breeze.\\n\\nSee any other cheats in this repository for examples.\\n\\n## Guides and tutorials\\n\\nPlease see the following resources for some guidance and tutorials on creating cheats and patches yourself:\\n\\n[**Hazerou\\'s** video tutorials](https://www.youtube.com/playlist?list=PL7F3HUhpLGiS7TzPM9V1hIh42GbcidN0n)\\n\\n[**Hazerou\\'s** text guide](https://gbatemp.net/threads/how-to-search-the-fps-codes-using-edizon-se.586786/)\\n\\n[**ChanseyIsTheBest\\'s** video tutorial](https://www.youtube.com/watch?v=h_XuSugIAsk)\\n\\n[**ChanseyIsTheBest\\'s** text guide on how to make 60FPS patches](https://gbatemp.net/threads/how-to-make-60fps-ips-patch-for-nintendo-switch-game-ghidra-tutorial.625675/)\\n\\n[**ChanseyIsTheBest\\'s** text guide on how to convert ips/pchtxt into cheats](https://gbatemp.net/threads/how-to-convert-ips-or-pchtxt-into-cheat.626182/)\\n\\nHow to use ips+pchtxt2cheat.py\\nThis tool converts ips/pchtxt files into cheat files, however not all converted cheats will work on actual hardware due to how assets are loaded.\\n1. Download and save ips+pchtxt2cheat.py\\n2. Drag and drop the ips or pchtxt file\\n3. A cheat will be generated with the .txt extension with the correct BID\\n\\n## Issues\\n\\nThe issues section is only for reporting issues with the cheat database, for example: missing credits, incorrect TitleIDs or BuildIDs.\\n\\nDo not open issues requesting cheats, asking for assistance with using cheats etc. Join our [discord server](#community-and-contact) or seek help on GBAtemp for that.\\n\\n## Community and contact\\n\\nCome and join us on discord for cheat requests, cheat updates, assistance, to contribute cheats or just to hang out!\\n\\n**Invite link:** https://discord.gg/VndKxFg7EE\\n\\n## Credits\\n\\nThis project was born [from the work of **Hazerou**](https://gbatemp.net/threads/60-fps-cheats-for-nswitch.592464/) and all of the great cheat makers who contributed and continue to contribute cheats.\\n\\nSpecial thanks to [**AnimatedSwine37**](https://twitter.com/AnimatedSwine37) for creating the automatically updated [**Games List**](https://github.com/ChanseyIsTheBest/NX-60FPS-RES-GFX-Cheats/blob/main/GAMES.md) and the [**ips2cheat** python tool](https://github.com/ChanseyIsTheBest/NX-60FPS-RES-GFX-Cheats/blob/main/ips2cheat.py) based on code by [**3096** and their ipswitch project](https://github.com/3096/ipswitch)\\n\\nSpecial thanks to [**HamletDuFromage**](https://github.com/HamletDuFromage) for adding this database to [**AIO-Switch-Updater**](https://github.com/HamletDuFromage/aio-switch-updater)\\n\\nAll cheat authors are credited inside the `TitleID/Game Name.txt` files! If you believe a credit is missing, please open an issue.\\n'},\n",
       " {'repo': './BlinkDL/RWKV-LM',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '# The RWKV Language Model (and my LM tricks)\\n\\n## RWKV: Parallelizable RNN with Transformer-level LLM Performance (pronounced as \"RwaKuv\", from 4 major params: R W K V)\\n\\nRWKV is an RNN with Transformer-level LLM performance, which can also be directly trained like a GPT transformer (parallelizable). And it\\'s 100% attention-free. You only need the hidden state at position t to compute the state at position t+1. You can use the \"GPT\" mode to quickly compute the hidden state for the \"RNN\" mode.\\n\\nSo it\\'s combining the best of RNN and transformer - **great performance, fast inference, saves VRAM, fast training, \"infinite\" ctx_len, and free sentence embedding** (using the final hidden state).\\n\\n**Raven 14B** (finetuned on Alpaca+ShareGPT+...) Demo: https://huggingface.co/spaces/BlinkDL/ChatRWKV-gradio\\n\\n**Raven 7B** (finetuned on Alpaca+ShareGPT+...) Demo: https://huggingface.co/spaces/BlinkDL/Raven-RWKV-7B\\n\\n**ChatRWKV:** with \"stream\" and \"split\" strategies and INT8. **3G VRAM is enough to run RWKV 14B :)** https://github.com/BlinkDL/ChatRWKV\\n\\n**Download RWKV-4 0.1/0.4/1.5/3/7/14B weights**: https://huggingface.co/BlinkDL\\n\\n**RWKV pip package**: https://pypi.org/project/rwkv/\\n\\n```python\\nos.environ[\"RWKV_JIT_ON\"] = \\'1\\'\\nos.environ[\"RWKV_CUDA_ON\"] = \\'0\\' # if \\'1\\' then use CUDA kernel for seq mode (much faster)\\nfrom rwkv.model import RWKV                         # pip install rwkv\\nmodel = RWKV(model=\\'/fsx/BlinkDL/HF-MODEL/rwkv-4-pile-1b5/RWKV-4-Pile-1B5-20220903-8040\\', strategy=\\'cuda fp16\\')\\n\\nout, state = model.forward([187, 510, 1563, 310, 247], None)   # use 20B_tokenizer.json\\nprint(out.detach().cpu().numpy())                   # get logits\\nout, state = model.forward([187, 510], None)\\nout, state = model.forward([1563], state)           # RNN has state (use deepcopy if you want to clone it)\\nout, state = model.forward([310, 247], state)\\nprint(out.detach().cpu().numpy())                   # same result as above\\n```\\n**Cool Community RWKV Projects (check them!)**:\\n\\nhttps://github.com/saharNooby/rwkv.cpp INT4 INT8 FP16 FP32 inference for CPU using [ggml](https://github.com/ggerganov/ggml)\\n\\nhttps://github.com/harrisonvanderbyl/rwkv-cpp-cuda pure CUDA RWKV (no need for python & pytorch)\\n\\nhttps://github.com/Blealtan/RWKV-LM-LoRA LoRA fine-tuning\\n\\nMore RWKV projects: https://github.com/search?o=desc&q=rwkv&s=updated&type=Repositories\\n\\n## Join Our Discord: https://discord.gg/bDSBUMeFpc (lots of developers)\\n\\n**Twitter**: https://twitter.com/BlinkDL_AI\\n\\n**RWKV in 150 lines** (model, inference, text generation): https://github.com/BlinkDL/ChatRWKV/blob/main/RWKV_in_150_lines.py\\n\\n**RWKV introduction, and in 100 lines of numpy**: https://johanwind.github.io/2023/03/23/rwkv_overview.html https://johanwind.github.io/2023/03/23/rwkv_details.html\\n\\nA cool paper (Spiking Neural Network) using RWKV: https://github.com/ridgerchu/SpikeGPT\\n\\nYou are welcome to join the RWKV discord https://discord.gg/bDSBUMeFpc to build upon it. We have plenty of potential compute (A100 40Gs) now (thanks to Stability and EleutherAI), so if you have interesting ideas I can run them.\\n\\n![RWKV-eval2](RWKV-eval2.png)\\n\\nRWKV [loss vs token position] for 10000 ctx4k+ documents in Pile. RWKV 1B5-4k is mostly flat after ctx1500, but 3B-4k and 7B-4k and 14B-4k have some slopes, and they are getting better. This debunks the old view that RNNs cannot model long ctxlens. We can predict that RWKV 100B will be great, and RWKV 1T is probably all you need :)\\n\\n![RWKV-ctxlen](RWKV-ctxlen.png)\\n\\nChatRWKV with RWKV 14B ctx8192:\\n\\n![RWKV-chat](RWKV-chat.png)\\n\\nI believe RNN is a better candidate for fundamental models, because: (1) It\\'s more friendly for ASICs (no kv cache). (2) It\\'s more friendly for RL. (3) When we write, our brain is more similar to RNN. (4) The universe is like an RNN too (because of locality). Transformers are non-local models.\\n\\nRWKV-3 1.5B on A40 (tf32) = always 0.015 sec/token, tested using simple pytorch code (no CUDA), GPU utilization 45%, VRAM 7823M\\n\\nGPT2-XL 1.3B on A40 (tf32) = 0.032 sec/token (for ctxlen 1000), tested using HF, GPU utilization 45% too (interesting), VRAM 9655M\\n\\nTraining speed: (new training code) RWKV-4 14B BF16 ctxlen4096 = 114K tokens/s on 8x8 A100 80G (ZERO2+CP). (old training code) RWKV-4 1.5B BF16 ctxlen1024 = 106K tokens/s on 8xA100 40G.\\n\\nI am doing image experiments too (For example: https://huggingface.co/BlinkDL/clip-guided-binary-autoencoder) and RWKV will be able to do txt2img diffusion :) My idea: 256x256 rgb image -> 32x32x13bit latents -> apply RWKV to compute transition probability for each of the 32x32 grid -> pretend the grids are independent and \"diffuse\" using these probabilities.\\n\\nSmooth training - no loss spikes! (lr & bsz change around 15G tokens)\\n![RWKV-loss](RWKV-loss.png)\\n\\n![RWKV-eval](RWKV-eval.png)\\n\\nAll of the trained models will be open-source. Inference is very fast (only matrix-vector multiplications, no matrix-matrix multiplications) even on CPUs, so you can even run a LLM on your phone.\\n\\nHow it works: RWKV gathers information to a number of channels, which are also decaying with different speeds as you move to the next token. It\\'s very simple once you understand it.\\n\\n**RWKV is parallelizable because the time-decay of each channel is data-independent (and trainable)**. For example, in usual RNN you can adjust the time-decay of a channel from say 0.8 to 0.5 (these are called \"gates\"), while in RWKV you simply move the information from a W-0.8-channel to a W-0.5-channel to achieve the same effect. Moreover, you can fine-tune RWKV into a non-parallelizable RNN (then you can use outputs of later layers of the previous token) if you want extra performance.\\n\\n![RWKV-formula](RWKV-formula.png)\\n\\nHere are some of my TODOs. Let\\'s work together :)\\n\\n* HuggingFace integration (check https://github.com/huggingface/transformers/issues/17230\\n), and optimized CPU & iOS & Android & WASM & WebGL inference. RWKV is a RNN and very friendly for edge devices. Let\\'s make it possible to run a LLM on your phone. \\n\\n* Test it on bidirectional & MLM tasks, and image & audio & video tokens. I think RWKV can support Encoder-Decoder via this: for each decoder token, use a learned mixture of [decoder previous hidden state] & [encoder final hidden state]. Hence all decoder tokens will have access to the encoder output.\\n\\n* Now training RWKV-4a with one single tiny extra attention (just a few extra lines comparing with RWKV-4) to further improve some difficult zeroshot tasks (such as LAMBADA) for smaller models. See https://github.com/BlinkDL/RWKV-LM/commit/a268cd2e40351ee31c30c5f8a5d1266d35b41829\\n\\nUser feedback:\\n> *I\\'ve so far toyed around the character-based model on our relatively small pre-training dataset (around 10GB of text), and the results are extremely good - similar ppl to models taking much, much longer to train.*\\n\\n> *dear god rwkv is fast. i switched to another tab after starting training it from scratch & when i returned it was emitting plausible english & maori words, i left to go microwave some coffee & when i came back it was producing fully grammatically correct sentences.*\\n\\nTweet from Sepp Hochreiter (thank you!): https://twitter.com/HochreiterSepp/status/1524270961314484227\\n\\nYou can find me (BlinkDL) in the EleutherAI Discord too: https://www.eleuther.ai/get-involved/\\n\\n![RWKV-demo](RWKV-demo.png)\\n\\n## Quick start\\n\\n**IMPORTANT: Use deepspeed==0.7.0 pytorch-lightning==1.9.2 torch 1.13.1+cu117**\\n\\nUse https://github.com/BlinkDL/RWKV-LM/tree/main/RWKV-v4neo (latest code, compatible with v4).\\n\\nHere is a great prompt for testing Q&A of LLMs. Works for any model: (found by minimizing ChatGPT ppls for RWKV 1.5B)\\n```python\\nprompt = f\\'\\\\nQ & A\\\\n\\\\nQuestion:\\\\n{qq}\\\\n\\\\nDetailed Expert Answer:\\\\n\\' # let the model generate after this\\n```\\n\\n### Inference\\n\\n**Run RWKV-4 Pile models:** Download models from https://huggingface.co/BlinkDL. Set TOKEN_MODE = \\'pile\\' in run.py and run it. It\\'s fast even on CPU (the default mode).\\n\\n**Colab for RWKV-4 Pile 1.5B**: https://colab.research.google.com/drive/1F7tZoPZaWJf1fsCmZ5tjw6sYHiFOYVWM\\n\\nRun RWKV-4 Pile models in your browser (and onnx version): see this issue https://github.com/BlinkDL/RWKV-LM/issues/7\\n\\nRWKV-4 Web Demo: https://josephrocca.github.io/rwkv-v4-web/demo/ (note: only greedy sampling for now)\\n\\nFor the old RWKV-2: see the release here for a 27M params model on enwik8 with 0.72 BPC(dev). Run run.py in https://github.com/BlinkDL/RWKV-LM/tree/main/RWKV-v2-RNN. You can even run it in your browser: https://github.com/BlinkDL/AI-Writer/tree/main/docs/eng https://blinkdl.github.io/AI-Writer/eng/ (this is using tf.js WASM single-thread mode).\\n\\n### Training / Fine-tuning\\n\\npip install deepspeed==0.7.0 // pip install pytorch-lightning==1.9.2 // torch 1.13.1+cu117\\n\\n**Training RWKV-4 from scratch:** run train.py, which by default is using the enwik8 dataset (unzip https://data.deepai.org/enwik8.zip).\\n\\nYou will be training the \"GPT\" version because it\\'s paralleziable and faster to train. RWKV-4 can extrapolate, so training with ctxLen 1024 can work for ctxLen of 2500+. You can fine-tune the model with longer ctxLen and it can quickly adapt to longer ctxLens.\\n\\n**Fine-tuning RWKV-4 Pile models:** use \\'prepare-data.py\\' in https://github.com/BlinkDL/RWKV-v2-RNN-Pile/tree/main/RWKV-v3 to tokenize .txt into train.npy data. Then use https://github.com/BlinkDL/RWKV-LM/blob/main/RWKV-v4neo/train.py to train it.\\n\\nRead the inference code in src/model.py and try using the final hidden state（.xx .aa .bb) as a faithful sentence embedding for other tasks. Probably you should begin with .xx and .aa/.bb (.aa divided by .bb).\\n\\nColab for fine-tuning RWKV-4 Pile models: https://colab.research.google.com/github/resloved/RWKV-notebooks/blob/master/RWKV_v4_RNN_Pile_Fine_Tuning.ipynb\\n\\n**Large corpus:** Use https://github.com/EleutherAI/gpt-neox to convert .jsonl into .bin and .idx\\n```\\npython tools/preprocess_data.py --input ./my_data.jsonl --output-prefix ./data/my_data --vocab ./20B_tokenizer.json --dataset-impl mmap --tokenizer-type HFTokenizer --append-eod\\n```\\n\\n**UPDATE:** now you can use https://github.com/Abel2076/json2binidx_tool (much easier to install)\\n\\nThe jsonl format sample (one line for each document):\\n```\\n{\"text\": \"This is the first document.\"}\\n{\"text\": \"Hello\\\\nWorld\"}\\n{\"text\": \"1+1=2\\\\n1+2=3\\\\n2+2=4\"}\\n```\\ngenerated by code like this:\\n```\\nss = json.dumps({\"text\": text}, ensure_ascii=False)\\nout.write(ss + \"\\\\n\")\\n```\\n\\n### How to use RWKV hidden state as text embedding\\n\\nConsider RWKV 14B. The state has 200 vectors, that is, 5 vectors for each block: fp16 (xx), fp32 (aa), fp32 (bb), fp32 (pp), fp16 (xx).\\n\\nDo not avg pool because different vectors (xx aa bb pp xx) in the state have very different meanings and ranges. You can probably remove pp.\\n\\nI suggest firstly collect the mean+stdev statistics of each channel of each vector, and normalize all of them (note: the normalization should be data-indepedent and collected from various texts). Then train a linear classifer.\\n\\n## Towards RWKV-5 (just to record some new ideas)\\n\\n### Some ideas\\n\\n1. Now time decay is like 0.999^T (0.999 is learnable). Change it to something like (0.999^T + 0.1) where 0.1 is learnable too. The 0.1 part will be kept forever. Or, A^T + B^T + C = fast-decay + slow-decay + constant. Can even use different formulas (for example, K^2 instead of e^K for a decay component, or, without normalization).\\n\\n2. Use complex-valued decay (so, rotation instead of decay) in some channels.\\n\\n3. Inject some trainable and extrapolatable positional encoding?\\n\\n4. Aside from 2d rotation, we can try other Lie groups such as 3d rotation ( SO(3) ). Non-abelian RWKV lol.\\n\\n5. RWKV might be great on analog devices (search for Analog Matrix-vector multiplication & Photonic Matrix-vector multiplication). The RNN mode is very hardware-friendly (processing-in-memory). Can be a SNN too (https://github.com/ridgerchu/SpikeGPT). I wonder if it can be optimized for quantum computation.\\n\\n6. Trainable initial hidden state (xx aa bb pp xx).\\n\\n7. Layerwise (or even row/column-wise, elementwise) LR, and test Lion optimizer.\\n\\n### Vision Tasks\\n\\n1. I find it\\'s good to add a 2d pos encoding:\\n```\\nself.pos_emb_x = nn.Parameter(torch.zeros((1,args.my_pos_emb,args.n_embd)))\\nself.pos_emb_y = nn.Parameter(torch.zeros((args.my_pos_emb,1,args.n_embd)))\\n...\\nx = x + pos_emb_x + pos_emb_y\\n```\\n\\n2. In a BPE langauge model, it\\'s the best to use [tokenShift of 1 token] (you can mix more tokens in a char-level English model). However you can try [tokenShift of N (or N-1) (or N+1) tokens] if the image size is N x N, because that will be like mixing [the token above the current positon (or the token above the to-be-predicted positon)] with [current token]. You can use try different tokenShift styles for \"ATT\" & \"FFN\", or mixing different tokenShift styles - such as mixing [token A] with [token A-1] and [token A-(N-1)] etc.\\n\\n### Misc\\n\\n#### Idea: Bytes-aware Embedding\\n\\nThe idea is to make sure each token in vocab understand its length and raw UTF-8 bytes.\\n\\nLet a = max(len(token)) for all token in vocab. Define AA : float[a][d_emb]\\n\\nLet b = max(len_in_utf8_bytes(token)) for all token in vocab. Define BB : float[b][256][d_emb]\\n\\nFor each token X in vocab, let [x0, x1, ..., xn] be its raw UTF-8 bytes. We will add some extra values to its embedding EMB(X):\\n\\nEMB(X) += AA[len(X)] + BB[0][x0] + BB[1][x1] + ... + BB[n][xn] (note: AA BB are learnable weights)\\n\\n* We can do this for the final Linear(d_emb, n_vocab) projection too.\\n* We can use some small networks to generate AA and BB, for some extra regularization (for example, BB[m][xi] and BB[n][xi] should be related).\\n\\n#### Old Idea\\n\\nI have an idea to improve tokenization. We can hardcode some channels to have meanings. Example:\\n\\nChannel 0 = \"space\"\\n\\nChannel 1 = \"capitalize first letter\"\\n\\nChannel 2 = \"capitalize all letters\"\\n\\nTherefore:\\n\\nEmbedding of \"abc\":  [0, 0, 0, x0, x1, x2 , ..]\\n\\nEmbedding of \" abc\":  [1, 0, 0, x0, x1, x2, ..]\\n\\nEmbedding of \" Abc\":  [1, 1, 0, x0, x1, x2, ..]\\n\\nEmbedding of \"ABC\": [0, 0, 1, x0, x1, x2, ...]\\n\\n......\\n\\nso they will share most of the embedding. And we can rapidly compute the output probability of all variations of \"abc\".\\n\\nNote: the above method is assuming that p(\" xyz\") / p(\"xyz\") is the same for any \"xyz\", which can be wrong.\\n\\nBetter: define emb_space emb_capitalize_first emb_capitalize_all to be a function of emb.\\n\\nMaybe the Best: let \\'abc\\' \\' abc\\' etc. to share the last 90% of their embeddings.\\n\\nAt this moment, all our tokenizers spend too many items to represent all variations of \\'abc\\' \\' abc\\' \\' Abc\\' etc. Moreover the model cannot discover that these are actually similar if some of these variations are rare in the dataset. The method here can improve this. I plan to test this in a new version of RWKV.\\n\\n#### Idea: Better Initial States\\n\\nExample (single-round Q & A):\\n\\n1. Generate the final state of all wiki documents.\\n\\n2. For any user Q, find the best wiki document, and use its final state as the initial state.\\n\\n3. Train a model to directly generate the optimal initial state for any user Q.\\n\\nHowever this can be a bit more tricky for multi-round Q & A :)\\n\\n## How it works\\n\\nRWKV is inspired by Apple\\'s AFT (https://arxiv.org/abs/2105.14103).\\n\\nMoreover it\\'s using a number of my tricks, such as:\\n\\n* SmallInitEmb: https://github.com/BlinkDL/SmallInitEmb (applicable to all transformers) which helps the embedding quality, and stabilizes Post-LN (which is what I am using).\\n\\n* Token-shift: https://github.com/BlinkDL/RWKV-LM#token-shift-time-shift-mixing (applicable to all transformers), especially helpful for char-level models.\\n\\n* Head-QK: https://github.com/BlinkDL/RWKV-LM#the-head-qk-trick-learning-to-copy-and-avoid-tokens (applicable to all transformers). Note: it\\'s helpful, but I disabled it in the Pile model to keep it 100% RNN.\\n\\n* Extra R-gate in the FFN (applicable to all transformers). I am also using reluSquared from Primer.\\n\\n* Better initilization: I init most of the matrices to ZERO (see RWKV_Init in https://github.com/BlinkDL/RWKV-LM/blob/main/RWKV-v2-RNN/src/model.py).\\n\\n* You can transfer some parameters from a small model to a large model (note: I sort & smooth them too), for faster and better convergence (see https://www.reddit.com/r/MachineLearning/comments/umq908/r_rwkvv2rnn_a_parallelizable_rnn_with/).\\n\\n* My CUDA kernel: https://github.com/BlinkDL/RWKV-CUDA to speedup training.\\n\\n## The pseudocode (execution from top to bottom):\\n\\n![RWKV-v2-RNN](RWKV-v2-RNN.png)\\n\\nThe a b c d factors work together to build a time-decay curve: [X, 1, W, W^2, W^3, ...].\\n\\nWrite out the formulas for \"token at pos 2\" and \"token at pos 3\" and you will get the idea:\\n* a and b: EMAs of kv and k.\\n* c and d: these are a and b combined with \"self-attention\".\\n\\nkv / k is the memory mechanism. The token with high k can be remembered for a long duration, if W is close to 1 in the channel.\\n\\nThe R-gate is important for performance. k = info strength of this token (to be passed to future tokens). r = whether to apply the info to this token.\\n\\n## RWKV-3 improvements\\n\\nUse different trainable TimeMix factors for R / K / V in SA and FF layers. Example:\\n```python\\nxx = self.time_shift(x)\\nxk = x * self.time_mix_k + xx * (1 - self.time_mix_k)\\nxv = x * self.time_mix_v + xx * (1 - self.time_mix_v)\\nxr = x * self.time_mix_r + xx * (1 - self.time_mix_r)\\n```\\n\\nUse preLN instead of postLN (more stable & faster convergence):\\n```python\\nif self.layer_id == 0:\\n\\tx = self.ln0(x)\\nx = x + self.att(self.ln1(x))\\nx = x + self.ffn(self.ln2(x))\\n```\\n\\n## Explaining the code for RWKV-3 GPT mode\\n\\n### The GPT mode - overview\\n\\nThe building blocks of RWKV-3 GPT mode are similar to that of a usual preLN GPT.\\n\\nThe only difference is an extra LN after embedding. Note you can absorb this LN into the embedding after finishing the training.\\n```python\\nx = self.emb(idx)  # input: idx = token indices\\nx = self.ln_emb(x) # extra LN after embedding\\nx = x + self.att_0(self.ln_att_0(x)) # preLN\\nx = x + self.ffn_0(self.ln_ffn_0(x))\\n...\\nx = x + self.att_n(self.ln_att_n(x))\\nx = x + self.ffn_n(self.ln_ffn_n(x))\\nx = self.ln_head(x) # final LN before projection\\nx = self.head(x)    # output: x = logits\\n```\\nIt is important to initialize emb to tiny values, such as nn.init.uniform_(a=-1e-4, b=1e-4), to utilize my trick https://github.com/BlinkDL/SmallInitEmb.\\n\\nFor the 1.5B RWKV-3, I use Adam (no wd, no dropout) optimizer on 8 * A100 40G.\\n\\nbatchSz = 32 * 896, ctxLen = 896. I am using tf32 so the batchSz is a bit small. \\n\\nFor the first 15B tokens, LR is fixed at 3e-4, and beta=(0.9, 0.99).\\n\\nThen I set beta=(0.9, 0.999), and do an exponential decay of LR, reaching 1e-5 at 332B tokens.\\n\\n### The GPT mode - ATT block\\n\\nThe RWKV-3 does not have any attention in the usual sense, but we will call this block ATT anyway.\\n```python\\nB, T, C = x.size() # x = (Batch,Time,Channel)\\n\\n# Mix x with the previous timestep to produce xk, xv, xr\\nxx = self.time_shift(x) # self.time_shift = nn.ZeroPad2d((0,0,1,-1))\\nxk = x * self.time_mix_k + xx * (1 - self.time_mix_k)\\nxv = x * self.time_mix_v + xx * (1 - self.time_mix_v)\\nxr = x * self.time_mix_r + xx * (1 - self.time_mix_r)\\n\\n# Use xk, xv, xr to produce k, v, r\\nk = self.key(xk).transpose(-1, -2)\\nv = self.value(xv).transpose(-1, -2)\\nr = self.receptance(xr)\\nk = torch.clamp(k, max=60) # clamp k to avoid overflow\\nk = torch.exp(k)\\nkv = k * v\\n\\n# Compute the W-curve = [e^(-n * e^time_decay), e^(-(n-1) * e^time_decay), ..., 1, e^(time_first)]\\nself.time_w = torch.cat([torch.exp(self.time_decay) * self.time_curve.to(x.device), self.time_first], dim=-1)\\nw = torch.exp(self.time_w)\\n\\n# Use W to mix kv and k respectively. Add K_EPS to wk to avoid divide-by-zero\\nif RUN_DEVICE == \\'cuda\\':\\n    wkv = TimeX.apply(w, kv, B,C,T, 0)\\n    wk = TimeX.apply(w, k, B,C,T, K_EPS)\\nelse:\\n    w = w[:,-T:].unsqueeze(1)\\n    wkv = F.conv1d(nn.ZeroPad2d((T-1, 0, 0, 0))(kv), w, groups=C)\\n    wk = F.conv1d(nn.ZeroPad2d((T-1, 0, 0, 0))(k), w, groups=C) + K_EPS\\n\\n# The RWKV formula\\nrwkv = torch.sigmoid(r) * (wkv / wk).transpose(-1, -2)\\nrwkv = self.output(rwkv) # final output projection\\n```\\n\\nThe self.key, self.receptance, self.output matrices are all initialized to zero.\\n\\nThe time_mix, time_decay, time_first vectors are transferred from a smaller trained model (note: I sort & smooth them too).\\n\\n### The GPT mode - FFN block\\n\\nThe FFN block has three tricks comparing with the usual GPT:\\n\\n1. My time_mix trick.\\n\\n2. The sqReLU from the Primer paper.\\n\\n3. An extra receptance-gate (similar to the receptance-gate in ATT block).\\n```python\\n# Mix x with the previous timestep to produce xk, xr\\nxx = self.time_shift(x)\\nxk = x * self.time_mix_k + xx * (1 - self.time_mix_k)\\nxr = x * self.time_mix_r + xx * (1 - self.time_mix_r)\\n\\n# The usual FFN operation\\nk = self.key(xk)\\nk = torch.square(torch.relu(k)) # from the Primer paper\\nkv = self.value(k)\\n\\n# Apply an extra receptance-gate to kv\\nrkv = torch.sigmoid(self.receptance(xr)) * kv\\nreturn rkv\\n```\\nThe self.value, self.receptance matrices are all initialized to zero.\\n\\n## RWKV-4 improvements\\n\\n![RWKV-v3-plan](RWKV-v3-plan.png)\\n\\n## From GPT to RWKV (the formulas)\\n\\nLet F[t] be the system state at t.\\n\\nLet x[t] be the new external input at t.\\n\\nIn GPT, predicting F[t+1] requires considering F[0], F[1], .. F[t]. So it takes O(T^2) to generate a length T sequence.\\n\\nThe **simplified formula** for GPT:\\n\\n![F[\\\\mathrm{t}+1]=\\\\frac{\\\\sum_{\\\\mathrm{i}=0}^{\\\\mathrm{t}} \\\\exp (\\\\mathbf{Q}x[\\\\mathrm{t}] * \\\\mathbf{K}F[\\\\mathrm{i}]) \\\\cdot(\\\\mathbf{V}F[\\\\mathrm{i}])}{\\\\sum_{\\\\mathrm{i}=0}^{\\\\mathrm{t}} \\\\exp (\\\\mathbf{Q}x[\\\\mathrm{t}] * \\\\mathbf{K}F[\\\\mathrm{i}])}](https://render.githubusercontent.com/render/math?math=%5Ccolor%7Bblack%7D%5Cdisplaystyle+F%5B%5Cmathrm%7Bt%7D%2B1%5D%3D%5Cfrac%7B%5Csum_%7B%5Cmathrm%7Bi%7D%3D0%7D%5E%7B%5Cmathrm%7Bt%7D%7D+%5Cexp+%28%5Cmathbf%7BQ%7Dx%5B%5Cmathrm%7Bt%7D%5D+%2A+%5Cmathbf%7BK%7DF%5B%5Cmathrm%7Bi%7D%5D%29+%5Ccdot%28%5Cmathbf%7BV%7DF%5B%5Cmathrm%7Bi%7D%5D%29%7D%7B%5Csum_%7B%5Cmathrm%7Bi%7D%3D0%7D%5E%7B%5Cmathrm%7Bt%7D%7D+%5Cexp+%28%5Cmathbf%7BQ%7Dx%5B%5Cmathrm%7Bt%7D%5D+%2A+%5Cmathbf%7BK%7DF%5B%5Cmathrm%7Bi%7D%5D%29%7D)\\n\\nIt\\'s very capable in theory, however that **does not mean we can fully utilize its capability with usual optimizers**. I suspect the loss landscape is too difficult for our current methods.\\n\\nCompare with the **simplified formula** for RWKV (the parallel mode, looks similar to Apple\\'s AFT):\\n\\n![F[\\\\mathrm{t}+1]=\\\\sigma(\\\\mathbf{R}x[\\\\mathrm{t}]) \\\\cdot \\\\frac{\\\\sum_{\\\\mathrm{i}=0}^{\\\\mathrm{t}} \\\\exp (\\\\mathbf{W} \\\\cdot(\\\\mathrm{t}-\\\\mathrm{i})) \\\\cdot \\\\exp (\\\\mathbf{K}F[\\\\mathrm{i}]) \\\\cdot(\\\\mathbf{V}F[\\\\mathrm{i}])}{\\\\sum_{\\\\mathrm{i}=0}^{\\\\mathrm{t}} \\\\exp (\\\\mathbf{W} \\\\cdot(\\\\mathrm{t}-\\\\mathrm{i})) \\\\cdot \\\\exp (\\\\mathbf{K }F[\\\\mathrm{i}])}](https://render.githubusercontent.com/render/math?math=%5Ccolor%7Bblack%7D%5Cdisplaystyle+F%5B%5Cmathrm%7Bt%7D%2B1%5D%3D%5Csigma%28%5Cmathbf%7BR%7Dx%5B%5Cmathrm%7Bt%7D%5D%29+%5Ccdot+%5Cfrac%7B%5Csum_%7B%5Cmathrm%7Bi%7D%3D0%7D%5E%7B%5Cmathrm%7Bt%7D%7D+%5Cexp+%28%5Cmathbf%7BW%7D+%5Ccdot%28%5Cmathrm%7Bt%7D-%5Cmathrm%7Bi%7D%29%29+%5Ccdot+%5Cexp+%28%5Cmathbf%7BK%7DF%5B%5Cmathrm%7Bi%7D%5D%29+%5Ccdot%28%5Cmathbf%7BV%7DF%5B%5Cmathrm%7Bi%7D%5D%29%7D%7B%5Csum_%7B%5Cmathrm%7Bi%7D%3D0%7D%5E%7B%5Cmathrm%7Bt%7D%7D+%5Cexp+%28%5Cmathbf%7BW%7D+%5Ccdot%28%5Cmathrm%7Bt%7D-%5Cmathrm%7Bi%7D%29%29+%5Ccdot+%5Cexp+%28%5Cmathbf%7BK+%7DF%5B%5Cmathrm%7Bi%7D%5D%29%7D)\\n\\nThe R, K, V are trainable matrices, and W is a trainable vector (time-decay factor for each channel).\\n\\nIn GPT, the contribution of F[i] to F[t+1] is weighted by ![ \\\\exp (\\\\mathbf{Q}x[\\\\mathrm{t}] * \\\\mathbf{K}F[\\\\mathrm{i}]) ](https://render.githubusercontent.com/render/math?math=%5Ccolor%7Bblack%7D%5Cdisplaystyle++%5Cexp+%28%5Cmathbf%7BQ%7Dx%5B%5Cmathrm%7Bt%7D%5D+%2A+%5Cmathbf%7BK%7DF%5B%5Cmathrm%7Bi%7D%5D%29+).\\n\\nIn RWKV-2, the contribution of F[i] to F[t+1] is weighted by ![\\\\sigma(\\\\mathbf{R}x[\\\\mathrm{t}]) \\\\cdot \\\\exp (\\\\mathbf{W} \\\\cdot(\\\\mathrm{t}-\\\\mathrm{i})) \\\\cdot \\\\exp (\\\\mathbf{K}F[\\\\mathrm{i}]) ](https://render.githubusercontent.com/render/math?math=%5Ccolor%7Bblack%7D%5Cdisplaystyle+%5Csigma%28%5Cmathbf%7BR%7Dx%5B%5Cmathrm%7Bt%7D%5D%29+%5Ccdot+%5Cexp+%28%5Cmathbf%7BW%7D+%5Ccdot%28%5Cmathrm%7Bt%7D-%5Cmathrm%7Bi%7D%29%29+%5Ccdot+%5Cexp+%28%5Cmathbf%7BK%7DF%5B%5Cmathrm%7Bi%7D%5D%29+).\\n* The ![\\\\sigma](https://render.githubusercontent.com/render/math?math=%5Ccolor%7Bblack%7D%5Cdisplaystyle+%5Csigma) is a non-linearity and we can use sigmoid. \\n* Note ![\\\\sigma(\\\\mathbf{R}x[\\\\mathrm{t}])](https://render.githubusercontent.com/render/math?math=%5Ccolor%7Bblack%7D%5Cdisplaystyle+%5Csigma%28%5Cmathbf%7BR%7Dx%5B%5Cmathrm%7Bt%7D%5D%29) is not in the denominator, and I call R the \"receptance\".\\n* The ![\\\\exp (\\\\mathbf{W} \\\\cdot(\\\\mathrm{t}-\\\\mathrm{i}))](https://render.githubusercontent.com/render/math?math=%5Ccolor%7Bblack%7D%5Cdisplaystyle+%5Cexp+%28%5Cmathbf%7BW%7D+%5Ccdot%28%5Cmathrm%7Bt%7D-%5Cmathrm%7Bi%7D%29%29) is the time-decay factor. I proposed the same idea (scaling the attention by distance) in Aug 2020 and called it the \"time-weighting\" (check the commit history of https://github.com/BlinkDL/minGPT-tuned).\\n\\nHere comes the punchline: we can rewrite it into a RNN (recursive formula). Note:\\n\\n![F[1]=\\\\sigma(\\\\mathbf{R }x[0]) \\\\cdot \\\\frac{ \\\\exp (\\\\mathbf{K }F[0]) \\\\cdot(\\\\mathbf{V }F[0])}{\\\\exp (\\\\mathbf{K }F[0])}](https://render.githubusercontent.com/render/math?math=%5Ccolor%7Bblack%7D%5Cdisplaystyle+F%5B1%5D%3D%5Csigma%28%5Cmathbf%7BR+%7Dx%5B0%5D%29+%5Ccdot+%5Cfrac%7B+%5Cexp+%28%5Cmathbf%7BK+%7DF%5B0%5D%29+%5Ccdot%28%5Cmathbf%7BV+%7DF%5B0%5D%29%7D%7B%5Cexp+%28%5Cmathbf%7BK+%7DF%5B0%5D%29%7D)\\n\\n![F[2]=\\\\sigma(\\\\mathbf{R }x[1]) \\\\cdot \\\\frac{ \\\\exp (\\\\mathbf{K }F[1]) \\\\cdot(\\\\mathbf{V }F[1])+\\\\exp (\\\\mathbf{W} ) \\\\cdot \\\\exp (\\\\mathbf{K }F[0]) \\\\cdot(\\\\mathbf{V }F[0])}{ \\\\exp (\\\\mathbf{K }F[1])+\\\\exp (\\\\mathbf{W} ) \\\\cdot \\\\exp (\\\\mathbf{K }F[0])}](https://render.githubusercontent.com/render/math?math=%5Ccolor%7Bblack%7D%5Cdisplaystyle+F%5B2%5D%3D%5Csigma%28%5Cmathbf%7BR+%7Dx%5B1%5D%29+%5Ccdot+%5Cfrac%7B+%5Cexp+%28%5Cmathbf%7BK+%7DF%5B1%5D%29+%5Ccdot%28%5Cmathbf%7BV+%7DF%5B1%5D%29%2B%5Cexp+%28%5Cmathbf%7BW%7D+%29+%5Ccdot+%5Cexp+%28%5Cmathbf%7BK+%7DF%5B0%5D%29+%5Ccdot%28%5Cmathbf%7BV+%7DF%5B0%5D%29%7D%7B+%5Cexp+%28%5Cmathbf%7BK+%7DF%5B1%5D%29%2B%5Cexp+%28%5Cmathbf%7BW%7D+%29+%5Ccdot+%5Cexp+%28%5Cmathbf%7BK+%7DF%5B0%5D%29%7D)\\n\\nTherefore it\\'s straightforward to verify:\\n\\n![F[t+1]=\\\\sigma(\\\\mathbf{R }x[t]) \\\\cdot \\\\frac{\\\\exp (\\\\mathbf{K}F[\\\\mathrm{t}]) \\\\cdot(\\\\mathbf{V}F[\\\\mathrm{t}])+\\\\exp (\\\\mathbf{W}) \\\\cdot A[\\\\mathrm{t}]}{ \\\\exp (\\\\mathbf{K}F[\\\\mathrm{t}])+\\\\exp (\\\\mathbf{W}) \\\\cdot B[\\\\mathrm{t}]}](https://render.githubusercontent.com/render/math?math=%5Ccolor%7Bblack%7D%5Cdisplaystyle+F%5Bt%2B1%5D%3D%5Csigma%28%5Cmathbf%7BR+%7Dx%5Bt%5D%29+%5Ccdot+%5Cfrac%7B%5Cexp+%28%5Cmathbf%7BK%7DF%5B%5Cmathrm%7Bt%7D%5D%29+%5Ccdot%28%5Cmathbf%7BV%7DF%5B%5Cmathrm%7Bt%7D%5D%29%2B%5Cexp+%28%5Cmathbf%7BW%7D%29+%5Ccdot+A%5B%5Cmathrm%7Bt%7D%5D%7D%7B+%5Cexp+%28%5Cmathbf%7BK%7DF%5B%5Cmathrm%7Bt%7D%5D%29%2B%5Cexp+%28%5Cmathbf%7BW%7D%29+%5Ccdot+B%5B%5Cmathrm%7Bt%7D%5D%7D)\\n\\nwhere A[t] and B[t] are the numerator and denominator of the previous step, respectively.\\n\\nI believe RWKV is performant because W is like repeatedly applying a diagonal matrix. Note (P^{-1} D P)^n = P^{-1} D^n P, so it is similar to repeatedly applying a general diagonalizable matrix.\\n\\nMoreover it\\'s possible to turn it into a continuous ODE (a bit similar to State Space Models). I will write about it later.\\n\\n## Star History\\n\\n[![Star History Chart](https://api.star-history.com/svg?repos=BlinkDL/RWKV-LM&type=Date)](https://star-history.com/#BlinkDL/RWKV-LM&Date)\\n\\n## Multimodal ideas\\n\\nI have an idea for [text --> 32x32 RGB image] using a LM (transformer, RWKV, etc.). Will test it soon.\\n\\nFirstly, LM loss (instead of L2 loss), so the image will not be blurry.\\n\\nSecondly, color quantization. For example, only allowing 8 levels for R/G/B. Then the image vocab size is 8x8x8 = 512 (for each pixel), instead of 2^24.\\nTherefore, a 32x32 RGB image = a len1024 sequence of vocab512 (image tokens), which is a typical input for usual LMs.\\n(Later we can use diffusion models to upsample and generate RGB888 images. We might be able to use a LM for this too.)\\n\\nThirdly, 2D positional embeddings that are easy for the model to understand.\\nFor example, add one-hot X & Y coords to the first 64(=32+32) channels. Say if the pixel is at x=8, y=20, then we will add 1 to channel 8 and channel 52 (=32+20).\\nMoreover probably we can add the float X & Y coords (normalized to 0~1 range) to another 2 channels. And other periodic pos. encoding might help too (will test). \\n\\nFinally, RandRound when doing the color quantization in the DataLoader.\\nFor example, if the float level is 4.578, then there is a 57.8% chance to use 5, and (1-57.8%) chance to use 4.\\nAnd we can allow both 4 and 5 in the prediction, but the loss will be higher if the prediction is 4.\\n\\nMulti-task training might help too. I will try this dataset format:\\n[TxtFirst] [Desc of Img (txt tokens)] [Img] [img tokens]\\nand sometimes\\n[ImgFirst] [img tokens] [Txt] [Desc of Img (txt tokens)]\\n... the order of the imgs should be randomized in the DataLoader, and [TxtFirst] [ImgFirst] [Img] [Txt] are special tokens\\nand do random sampling of the full dataset. So sometimes the model will see the img tokens first and then the corresponding txt tokens, which is a [img -> txt] task. And the model will see some partial imgs and partial txts. I think a char-level LM might help the model to write correct text on images.\\n\\n## How to sample a large dataset (for training)\\n\\nI am using a trick to sample the Pile deterministically yet randomly enough.\\n\\nLet\\'s say the pile has x chunks (a chunk = ctx_len tokens).\\n\\npick a prime number p just less than x, and make sure p = 2 (mod 3).\\n\\nUse (step * step * step) mod p to sample it. Add some bias to step for extra randomness.\\n\\n## The top-p-x sampling method (for inference)\\n\\nWe propose a new sampling method called top-p-x:\\n\\nit\\'s like top-p, and the only difference is you also keep all tokens whose prob > x.\\n\\nTry x = 0.01 first.\\n\\n## Better Learning Rate Schedule via Variantional Method of Loss Curve\\n\\nI propose a simple new method to find better LR schedules. The method is cost-efficient and practical for large LMs. The takeaway is we can model the loss curve dynamics (phenomenology) w.r.t. the LR, and a nice closed-form LR curve can be directly computed from it using variantional method. Moreover we can predict the final loss with reasonable accuracy.\\n\\nUPDATE: In \"Conclusion 1.\", use the best-fitting regime (ignore the initial steps where our approximations break down) to fit the parameters.\\n\\nTry this: fixed lr for 1 hr, then exponential decay to 0.2 * lr in 12 hrs, and choose the t=[1hr, 13hr] segment.\\n\\nIn the last three plots, black = predicted loss curve of the new LR schedule, blue = original (unoptimized) real loss curve, orange = new LR schedule.\\n\\n![better_lr_schedule](Research/better_lr_schedule.png)\\n\\n# RWKV v1\\n\\nWe propose the RWKV language model, with alternating time-mix and channel-mix layers:\\n\\n<img src=\\n\"https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+%5Cbegin%7Balign%2A%7D%0A%5Ctext%7BTime-mix+%3A%7D+%26%26+%5Ctext%7BTM%7D_%7Bt%2Cc%7D+%26%26%3D%26%26%5Ctext%7Bsigmoid%7D%28%5Ctext%7BR%7D_%7Bt%2Cc%7D%29+%26%26%5Ccdot%26%26+%26%26%5Ctextstyle%5Csum_%7Bu%7D+%26%26%5Ctextbf%7BW%7D_%7Bt%2Cu%2Cc%7D+%26%26%5Ccdot%26%26+%5Ctext%7Bsoftmax%7D_t%28%5Ctext%7BK%7D_%7Bu%2Cc%7D%29+%26%26%5Ccdot%26%26+%5Ctext%7BV%7D_%7Bu%2Cc%7D%5C%5C%0A%5Ctext%7BChannel-mix+%3A%7D+%26%26+%5Ctext%7BCM%7D_%7Bt%2Cc%7D+%26%26%3D%26%26%5Ctext%7Bsigmoid%7D%28%5Ctext%7BR%7D_%7Bt%2Cc%7D%29+%26%26%5Ccdot%26%26+%26%26%5Ctextstyle%5Csum_d+%26%26%5Ctextbf%7BW%7D_%7Bc%2Cd%7D+%26%26%5Ccdot%26%26+%5Ctext%7Bgelu%7D%28%5Ctext%7BK%7D_%7Bt%2Cd%7D%29+%26%26%5Ccdot%26%26+%5Ctext%7BV%7D_%7Bt%2Cd%7D%0A%5Cend%7Balign%2A%7D%0A\" \\nalt=\"\\\\begin{align*}\\n\\\\text{Time-mix :} && \\\\text{TM}_{t,c} &&=&&\\\\text{sigmoid}(\\\\text{R}_{t,c}) &&\\\\cdot&& &&\\\\textstyle\\\\sum_{u} &&\\\\textbf{W}_{t,u,c} &&\\\\cdot&& \\\\text{softmax}_t(\\\\text{K}_{u,c}) &&\\\\cdot&& \\\\text{V}_{u,c}\\\\\\\\\\n\\\\text{Channel-mix :} && \\\\text{CM}_{t,c} &&=&&\\\\text{sigmoid}(\\\\text{R}_{t,c}) &&\\\\cdot&& &&\\\\textstyle\\\\sum_d &&\\\\textbf{W}_{c,d} &&\\\\cdot&& \\\\text{gelu}(\\\\text{K}_{t,d}) &&\\\\cdot&& \\\\text{V}_{t,d}\\n\\\\end{align*}\\n\">\\n\\n* The R, K, V are generated by linear transforms of input, and W is parameter. The idea of RWKV is to decompose attention into R(target) * W(src, target) * K(src). So we can call R \"receptance\", and sigmoid means it\\'s in 0~1 range.\\n\\n* The Time-mix is similar to AFT (https://arxiv.org/abs/2105.14103). There are two differences.\\n\\n(1) We changed the normalization (denominator). For masked language models, we define:\\n\\n<img src=\\n\"https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+%5Ctext%7Bsoftmax%7D_t%28%5Ctext%7BK%7D_%7Bu%2Cc%7D%29+%3D+%5Cfrac%7B%5Cexp%28%5Ctext%7BK%7D_%7Bu%2Cc%7D%29%7D%7B%5Csum_%7Bv+%5Cleq+t%7D%5Cexp%28%5Ctext%7BK%7D_%7Bv%2Cc%7D%29%7D\" \\nalt=\"\\\\text{softmax}_t(\\\\text{K}_{u,c}) = \\\\frac{\\\\exp(\\\\text{K}_{u,c})}{\\\\sum_{v \\\\leq t}\\\\exp(\\\\text{K}_{v,c})}\">\\n\\n**(UPDATE: We are using the original AFT normalization in v2)**\\n \\nInitialize K and R matrices (and the output projection matrix) to ZERO for fast & stable convergence.\\n \\n(2) We decompose W_{t,u,c} and introduce multi-head W (here h is the corresponding head of c):\\n\\n<img src=\\n\"https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+W_%7Bt%2Cu%2Cc%7D%3Df_h%28t-u%29%5Ccdot+%5Calpha_h%28u%29+%5Ccdot+%5Cbeta_h%28t%29\" \\nalt=\"W_{t,u,c}=f_h(t-u)\\\\cdot \\\\alpha_h(u) \\\\cdot \\\\beta_h(t)\">\\n\\nMoreover we multiply the final output of Time-mix layer by γ(t). The reason for the α β γ factors, is because the context size is smaller when t is small, and this can be compensated using the α β γ factors.\\n\\n**(UPDATE: We remove α β γ factors in v2-RNN and restrict W to be of a simple form and hence able to rewrite it as RNN)**\\n\\n* The Channel-mix is similar to GeGLU (https://arxiv.org/abs/2002.05202) with an extra R factor. Initialize R and W matrices to ZERO for fast & stable convergence.\\n\\n* Finally, we add extra token-shift (time-shift mixing) as in (https://github.com/BlinkDL/minGPT-tuned).\\n\\n# Token-shift (time-shift mixing)\\n\\nThe token-shift explicitly uses (half the channels of this token) & (half the channels of prev token) to generate all vectors (QKV, RWKV, ...).\\n\\n```\\nself.time_shift = nn.ZeroPad2d((0,0,1,-1))\\n\\nx = torch.cat([self.time_shift(x[:, :, :C//2]), x[:, :, C//2:]], dim = -1)\\n```\\n\\nDividing channels by 2 and shift-1 works great for char-level English and char-level Chinese LM.\\n\\nHowever for BPE-level English LM, it\\'s only effective if your embedding is large enough (at least 1024 - so the usual small L12-D768 model is not enough).\\n\\nMy theory on the effectiveness of token-shift:\\n\\nWhen we train a GPT, the hidden representation of a token has to accomplish two different objects:\\n\\n1. Predict the next token. Sometimes this is easy (obvious next token).\\n\\n2. Collect all previous context info, so later tokens can use it. This is always hard.\\n\\nThe shifted channels can focus on (2), so we have good propagation of info. It\\'s like some kind of residual connection, or a small RNN inside the transformer.\\n\\nYou can use token-shift in usual QKV self-attention too. I looked at the weights, and found V really likes the shifted channels, less so for Q. Makes sense if you think about it. I also found you may want to use less mixing in higher layers.\\n\\np.s. There is a MHA_pro model in this repo with strong performance. Give it a try :)\\n\\n# The Head-QK Trick: learning to copy and avoid tokens\\n\\nIn usual transformer, a small model has difficulty copying tokens (such as person names) in the context. We add extra Q & K to the final output such that the model can directly copy (or avoid) tokens in the context. Afterwards the model will teach itself NER (named entity recognition) if you look at the learned weights.\\n```\\nq = self.head_q(x)[:,:T,:] # projecting to 256-d\\nk = self.head_k(x)[:,:T,:] # projecting to 256-d\\nc = (q @ k.transpose(-2, -1)) * (1.0 / 256)\\nc = c.masked_fill(self.copy_mask[:T,:T] == 0, 0)\\nc = c @ F.one_hot(idx, num_classes = self.config.vocab_size).float()       \\nx = self.head(x) + c\\n```\\nNote: when a token occurs multiple times in the context, it might be better to use max(prob) instead of sum(prob).\\n\\n# The top-a sampling method\\n\\nWe also propose a new sampling method called top-a (as in src/utils.py):\\n\\n(1) Find the max probability p_max after softmax.\\n\\n(2) Remove all entries whose probability is lower than 0.2 * pow(p_max, 2). So it\\'s adaptive, hence \"top-a\".\\n\\n(3) Feel free to tune the 0.2 and 2 factor. Tune 0.2 first.\\n\\nThe idea of top-a:\\n1. If max_prob=0.9, then remove all tokens with prob < 0.162 (so, removing all alternatives)\\n2. If max_prob=0.5, then remove all tokens with prob < 0.05  (so, allowing more choices)\\n3. If max_prob=0.1, then remove all tokens with prob < 0.002 (so, allowing lots of possibilities)\\n\\n```\\nprobs = F.softmax(logits, dim=-1)\\n\\nlimit = torch.pow(torch.max(probs), 2) * 0.02\\nlogits[probs < limit] = -float(\\'Inf\\')\\n```\\n\\n# Performance\\n\\nCharacter-level loss on simplebooks-92 dataset https://dldata-public.s3.us-east-2.amazonaws.com/simplebooks.zip\\n\\n![RWKV-vs-MHA](RWKV-vs-MHA.png)\\n\\nGray: usual MHA+Rotary+GeGLU - performance not as good. 17.2M params.\\n\\nRed: RWKV (\"linear\" attention) - VRAM friendly - quite faster when ctx window is long - good performance. 16.6M params.\\n\\nGreen: MHA+Rotary+GeGLU+Token_shift. 17.2M params.\\n\\nBlue: MHA_pro (MHA with various tweaks & RWKV-type-FFN) - slow - needs more VRAM - good performance. 16.6M params.\\n\\n```\\n@software{peng_bo_2021_5196578,\\n  author       = {PENG Bo},\\n  title        = {BlinkDL/RWKV-LM: 0.01},\\n  month        = aug,\\n  year         = 2021,\\n  publisher    = {Zenodo},\\n  version      = {0.01},\\n  doi          = {10.5281/zenodo.5196577},\\n  url          = {https://doi.org/10.5281/zenodo.5196577}\\n}\\n```\\n\\n# Initialization\\n\\nWe use careful initialization for RWKV to get fast convergence - orthogonal matrices with proper scaling, and special time_w curves. Check model.py for details.\\n\\nSome learned time_w examples:\\n\\n![RWKV-time-w](RWKV-time-w.png)\\n'},\n",
       " {'repo': './neonbjb/tortoise-tts',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '# TorToiSe\\n\\nTortoise is a text-to-speech program built with the following priorities:\\n\\n1. Strong multi-voice capabilities.\\n2. Highly realistic prosody and intonation.\\n\\nThis repo contains all the code needed to run Tortoise TTS in inference mode.\\n\\nA (*very*) rough draft of the Tortoise paper is now available in doc format. I would definitely appreciate any comments, suggestions or reviews:\\nhttps://docs.google.com/document/d/13O_eyY65i6AkNrN_LdPhpUjGhyTNKYHvDrIvHnHe1GA\\n\\n### Version history\\n\\n#### v2.4; 2022/5/17\\n- Removed CVVP model. Found that it does not, in fact, make an appreciable difference in the output.\\n- Add better debugging support; existing tools now spit out debug files which can be used to reproduce bad runs.\\n\\n#### v2.3; 2022/5/12\\n- New CLVP-large model for further improved decoding guidance.\\n- Improvements to read.py and do_tts.py (new options)\\n\\n#### v2.2; 2022/5/5\\n- Added several new voices from the training set.\\n- Automated redaction. Wrap the text you want to use to prompt the model but not be spoken in brackets.\\n- Bug fixes\\n\\n#### v2.1; 2022/5/2\\n- Added ability to produce totally random voices.\\n- Added ability to download voice conditioning latent via a script, and then use a user-provided conditioning latent.\\n- Added ability to use your own pretrained models.\\n- Refactored directory structures.\\n- Performance improvements & bug fixes.\\n\\n## What\\'s in a name?\\n\\nI\\'m naming my speech-related repos after Mojave desert flora and fauna. Tortoise is a bit tongue in cheek: this model\\nis insanely slow. It leverages both an autoregressive decoder **and** a diffusion decoder; both known for their low\\nsampling rates. On a K80, expect to generate a medium sized sentence every 2 minutes.\\n\\n## Demos\\n\\nSee [this page](http://nonint.com/static/tortoise_v2_examples.html) for a large list of example outputs.\\n\\nCool application of Tortoise+GPT-3 (not by me): https://twitter.com/lexman_ai\\n\\n## Usage guide\\n\\n### Colab\\n\\nThe original colab no longer works by a combination of Google\\'s tendency to forward-break things and Python\\'s package management system. I do not intend to keep fixing it so it has been removed. Apologies!\\n\\n### Local Installation\\n\\nIf you want to use this on your own computer, you must have an NVIDIA GPU.\\n\\nFirst, install pytorch using these instructions: [https://pytorch.org/get-started/locally/](https://pytorch.org/get-started/locally/).\\nOn Windows, I **highly** recommend using the Conda installation path. I have been told that if you do not do this, you\\nwill spend a lot of time chasing dependency problems.\\n\\nNext, install TorToiSe and it\\'s dependencies:\\n\\n```shell\\ngit clone https://github.com/neonbjb/tortoise-tts.git\\ncd tortoise-tts\\npython -m pip install -r ./requirements.txt\\npython setup.py install\\n```\\n\\nIf you are on windows, you will also need to install pysoundfile: `conda install -c conda-forge pysoundfile`\\n\\n### do_tts.py\\n\\nThis script allows you to speak a single phrase with one or more voices.\\n```shell\\npython tortoise/do_tts.py --text \"I\\'m going to speak this\" --voice random --preset fast\\n```\\n\\n### read.py\\n\\nThis script provides tools for reading large amounts of text.\\n\\n```shell\\npython tortoise/read.py --textfile <your text to be read> --voice random\\n```\\n\\nThis will break up the textfile into sentences, and then convert them to speech one at a time. It will output a series\\nof spoken clips as they are generated. Once all the clips are generated, it will combine them into a single file and\\noutput that as well.\\n\\nSometimes Tortoise screws up an output. You can re-generate any bad clips by re-running `read.py` with the --regenerate\\nargument.\\n\\n### API\\n\\nTortoise can be used programmatically, like so:\\n\\n```python\\nreference_clips = [utils.audio.load_audio(p, 22050) for p in clips_paths]\\ntts = api.TextToSpeech()\\npcm_audio = tts.tts_with_preset(\"your text here\", voice_samples=reference_clips, preset=\\'fast\\')\\n```\\n\\n## Voice customization guide\\n\\nTortoise was specifically trained to be a multi-speaker model. It accomplishes this by consulting reference clips.\\n\\nThese reference clips are recordings of a speaker that you provide to guide speech generation. These clips are used to determine many properties of the output, such as the pitch and tone of the voice, speaking speed, and even speaking defects like a lisp or stuttering. The reference clip is also used to determine non-voice related aspects of the audio output like volume, background noise, recording quality and reverb.\\n\\n### Random voice\\n\\nI\\'ve included a feature which randomly generates a voice. These voices don\\'t actually exist and will be random every time you run\\nit. The results are quite fascinating and I recommend you play around with it!\\n\\nYou can use the random voice by passing in \\'random\\' as the voice name. Tortoise will take care of the rest.\\n\\nFor the those in the ML space: this is created by projecting a random vector onto the voice conditioning latent space.\\n\\n### Provided voices\\n\\nThis repo comes with several pre-packaged voices. Voices prepended with \"train_\" came from the training set and perform\\nfar better than the others. If your goal is high quality speech, I recommend you pick one of them. If you want to see\\nwhat Tortoise can do for zero-shot mimicking, take a look at the others.\\n\\n### Adding a new voice\\n\\nTo add new voices to Tortoise, you will need to do the following:\\n\\n1. Gather audio clips of your speaker(s). Good sources are YouTube interviews (you can use youtube-dl to fetch the audio), audiobooks or podcasts. Guidelines for good clips are in the next section.\\n2. Cut your clips into ~10 second segments. You want at least 3 clips. More is better, but I only experimented with up to 5 in my testing.\\n3. Save the clips as a WAV file with floating point format and a 22,050 sample rate.\\n4. Create a subdirectory in voices/\\n5. Put your clips in that subdirectory.\\n6. Run tortoise utilities with --voice=<your_subdirectory_name>.\\n\\n### Picking good reference clips\\n\\nAs mentioned above, your reference clips have a profound impact on the output of Tortoise. Following are some tips for picking\\ngood clips:\\n\\n1. Avoid clips with background music, noise or reverb. These clips were removed from the training dataset. Tortoise is unlikely to do well with them.\\n2. Avoid speeches. These generally have distortion caused by the amplification system.\\n3. Avoid clips from phone calls.\\n4. Avoid clips that have excessive stuttering, stammering or words like \"uh\" or \"like\" in them.\\n5. Try to find clips that are spoken in such a way as you wish your output to sound like. For example, if you want to hear your target voice read an audiobook, try to find clips of them reading a book.\\n6. The text being spoken in the clips does not matter, but diverse text does seem to perform better.\\n\\n## Advanced Usage\\n\\n### Generation settings\\n\\nTortoise is primarily an autoregressive decoder model combined with a diffusion model. Both of these have a lot of knobs\\nthat can be turned that I\\'ve abstracted away for the sake of ease of use. I did this by generating thousands of clips using\\nvarious permutations of the settings and using a metric for voice realism and intelligibility to measure their effects. I\\'ve\\nset the defaults to the best overall settings I was able to find. For specific use-cases, it might be effective to play with\\nthese settings (and it\\'s very likely that I missed something!)\\n\\nThese settings are not available in the normal scripts packaged with Tortoise. They are available, however, in the API. See\\n```api.tts``` for a full list.\\n\\n### Prompt engineering\\n\\nSome people have discovered that it is possible to do prompt engineering with Tortoise! For example, you can evoke emotion\\nby including things like \"I am really sad,\" before your text. I\\'ve built an automated redaction system that you can use to\\ntake advantage of this. It works by attempting to redact any text in the prompt surrounded by brackets. For example, the\\nprompt \"\\\\[I am really sad,\\\\] Please feed me.\" will only speak the words \"Please feed me\" (with a sad tonality).\\n\\n### Playing with the voice latent\\n\\nTortoise ingests reference clips by feeding them through individually through a small submodel that produces a point latent,\\nthen taking the mean of all of the produced latents. The experimentation I have done has indicated that these point latents\\nare quite expressive, affecting everything from tone to speaking rate to speech abnormalities.\\n\\nThis lends itself to some neat tricks. For example, you can combine feed two different voices to tortoise and it will output\\nwhat it thinks the \"average\" of those two voices sounds like.\\n\\n#### Generating conditioning latents from voices\\n\\nUse the script `get_conditioning_latents.py` to extract conditioning latents for a voice you have installed. This script\\nwill dump the latents to a .pth pickle file. The file will contain a single tuple, (autoregressive_latent, diffusion_latent).\\n\\nAlternatively, use the api.TextToSpeech.get_conditioning_latents() to fetch the latents.\\n\\n#### Using raw conditioning latents to generate speech\\n\\nAfter you\\'ve played with them, you can use them to generate speech by creating a subdirectory in voices/ with a single\\n\".pth\" file containing the pickled conditioning latents as a tuple (autoregressive_latent, diffusion_latent).\\n\\n### Send me feedback!\\n\\nProbabilistic models like Tortoise are best thought of as an \"augmented search\" - in this case, through the space of possible\\nutterances of a specific string of text. The impact of community involvement in perusing these spaces (such as is being done with\\nGPT-3 or CLIP) has really surprised me. If you find something neat that you can do with Tortoise that isn\\'t documented here,\\nplease report it to me! I would be glad to publish it to this page.\\n\\n## Tortoise-detect\\n\\nOut of concerns that this model might be misused, I\\'ve built a classifier that tells the likelihood that an audio clip\\ncame from Tortoise.\\n\\nThis classifier can be run on any computer, usage is as follows:\\n\\n```commandline\\npython tortoise/is_this_from_tortoise.py --clip=<path_to_suspicious_audio_file>\\n```\\n\\nThis model has 100% accuracy on the contents of the results/ and voices/ folders in this repo. Still, treat this classifier\\nas a \"strong signal\". Classifiers can be fooled and it is likewise not impossible for this classifier to exhibit false\\npositives.\\n\\n## Model architecture\\n\\nTortoise TTS is inspired by OpenAI\\'s DALLE, applied to speech data and using a better decoder. It is made up of 5 separate\\nmodels that work together. I\\'ve assembled a write-up of the system architecture here:\\n[https://nonint.com/2022/04/25/tortoise-architectural-design-doc/](https://nonint.com/2022/04/25/tortoise-architectural-design-doc/)\\n\\n## Training\\n\\nThese models were trained on my \"homelab\" server with 8 RTX 3090s over the course of several months. They were trained on a dataset consisting of\\n~50k hours of speech data, most of which was transcribed by [ocotillo](http://www.github.com/neonbjb/ocotillo). Training was done on my own\\n[DLAS](https://github.com/neonbjb/DL-Art-School) trainer.\\n\\nI currently do not have plans to release the training configurations or methodology. See the next section..\\n\\n## Ethical Considerations\\n\\nTortoise v2 works considerably better than I had planned. When I began hearing some of the outputs of the last few versions, I began\\nwondering whether or not I had an ethically unsound project on my hands. The ways in which a voice-cloning text-to-speech system\\ncould be misused are many. It doesn\\'t take much creativity to think up how.\\n\\nAfter some thought, I have decided to go forward with releasing this. Following are the reasons for this choice:\\n\\n1. It is primarily good at reading books and speaking poetry. Other forms of speech do not work well.\\n2. It was trained on a dataset which does not have the voices of public figures. While it will attempt to mimic these voices if they are provided as references, it does not do so in such a way that most humans would be fooled.\\n3. The above points could likely be resolved by scaling up the model and the dataset. For this reason, I am currently withholding details on how I trained the model, pending community feedback.\\n4. I am releasing a separate classifier model which will tell you whether a given audio clip was generated by Tortoise or not. See `tortoise-detect` above.\\n5. If I, a tinkerer with a BS in computer science with a ~$15k computer can build this, then any motivated corporation or state can as well. I would prefer that it be in the open and everyone know the kinds of things ML can do.\\n\\n### Diversity\\n\\nThe diversity expressed by ML models is strongly tied to the datasets they were trained on.\\n\\nTortoise was trained primarily on a dataset consisting of audiobooks. I made no effort to\\nbalance diversity in this dataset. For this reason, Tortoise will be particularly poor at generating the voices of minorities\\nor of people who speak with strong accents.\\n\\n## Looking forward\\n\\nTortoise v2 is about as good as I think I can do in the TTS world with the resources I have access to. A phenomenon that happens when\\ntraining very large models is that as parameter count increases, the communication bandwidth needed to support distributed training\\nof the model increases multiplicatively. On enterprise-grade hardware, this is not an issue: GPUs are attached together with\\nexceptionally wide buses that can accommodate this bandwidth. I cannot afford enterprise hardware, though, so I am stuck.\\n\\nI want to mention here\\nthat I think Tortoise could be a **lot** better. The three major components of Tortoise are either vanilla Transformer Encoder stacks\\nor Decoder stacks. Both of these types of models have a rich experimental history with scaling in the NLP realm. I see no reason\\nto believe that the same is not true of TTS.\\n\\nThe largest model in Tortoise v2 is considerably smaller than GPT-2 large. It is 20x smaller that the original DALLE transformer.\\nImagine what a TTS model trained at or near GPT-3 or DALLE scale could achieve.\\n\\nIf you are an ethical organization with computational resources to spare interested in seeing what this model could do\\nif properly scaled out, please reach out to me! I would love to collaborate on this.\\n\\n## Acknowledgements\\n\\nThis project has garnered more praise than I expected. I am standing on the shoulders of giants, though, and I want to\\ncredit a few of the amazing folks in the community that have helped make this happen:\\n\\n- Hugging Face, who wrote the GPT model and the generate API used by Tortoise, and who hosts the model weights.\\n- [Ramesh et al](https://arxiv.org/pdf/2102.12092.pdf) who authored the DALLE paper, which is the inspiration behind Tortoise.\\n- [Nichol and Dhariwal](https://arxiv.org/pdf/2102.09672.pdf) who authored the (revision of) the code that drives the diffusion model.\\n- [Jang et al](https://arxiv.org/pdf/2106.07889.pdf) who developed and open-sourced univnet, the vocoder this repo uses.\\n- [Kim and Jung](https://github.com/mindslab-ai/univnet) who implemented univnet pytorch model.\\n- [lucidrains](https://github.com/lucidrains) who writes awesome open source pytorch models, many of which are used here.\\n- [Patrick von Platen](https://huggingface.co/patrickvonplaten) whose guides on setting up wav2vec were invaluable to building my dataset.\\n\\n## Notice\\n\\nTortoise was built entirely by me using my own hardware. My employer was not involved in any facet of Tortoise\\'s development.\\n\\nIf you use this repo or the ideas therein for your research, please cite it! A bibtex entree can be found in the right pane on GitHub.\\n'},\n",
       " {'repo': './neuml/txtai',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '<p align=\"center\">\\n    <img src=\"https://raw.githubusercontent.com/neuml/txtai/master/logo.png\"/>\\n</p>\\n\\n<h3 align=\"center\">\\n    <p>Semantic search and workflows powered by language models</p>\\n</h3>\\n\\n<p align=\"center\">\\n    <a href=\"https://github.com/neuml/txtai/releases\">\\n        <img src=\"https://img.shields.io/github/release/neuml/txtai.svg?style=flat&color=success\" alt=\"Version\"/>\\n    </a>\\n    <a href=\"https://github.com/neuml/txtai\">\\n        <img src=\"https://img.shields.io/github/last-commit/neuml/txtai.svg?style=flat&color=blue\" alt=\"GitHub last commit\"/>\\n    </a>\\n    <a href=\"https://github.com/neuml/txtai/issues\">\\n        <img src=\"https://img.shields.io/github/issues/neuml/txtai.svg?style=flat&color=success\" alt=\"GitHub issues\"/>\\n    </a>\\n    <a href=\"https://join.slack.com/t/txtai/shared_invite/zt-1cagya4yf-DQeuZbd~aMwH5pckBU4vPg\">\\n        <img src=\"https://img.shields.io/badge/slack-join-blue?style=flat&logo=slack&logocolor=white\" alt=\"Join Slack\"/>\\n    </a>\\n    <a href=\"https://github.com/neuml/txtai/actions?query=workflow%3Abuild\">\\n        <img src=\"https://github.com/neuml/txtai/workflows/build/badge.svg\" alt=\"Build Status\"/>\\n    </a>\\n    <a href=\"https://coveralls.io/github/neuml/txtai?branch=master\">\\n        <img src=\"https://img.shields.io/coverallsCoverage/github/neuml/txtai\" alt=\"Coverage Status\">\\n    </a>\\n</p>\\n\\n-------------------------------------------------------------------------------------------------------------------------------------------------------\\n\\ntxtai is an open-source platform for semantic search and workflows powered by language models.\\n\\n![demo](https://raw.githubusercontent.com/neuml/txtai/master/demo.gif)\\n\\nTraditional search systems use keywords to find data. Semantic search has an understanding of natural language and identifies results that have the same meaning, not necessarily the same keywords.\\n\\n![search](https://raw.githubusercontent.com/neuml/txtai/master/docs/images/search.png#gh-light-mode-only)\\n![search](https://raw.githubusercontent.com/neuml/txtai/master/docs/images/search-dark.png#gh-dark-mode-only)\\n\\ntxtai builds embeddings databases, which are a union of vector indexes and relational databases. This enables similarity search with SQL. Embeddings databases can stand on their own and/or serve as a powerful knowledge source for large language model (LLM) prompts.\\n\\nSemantic workflows connect language models together to build intelligent applications.\\n\\n![flows](https://raw.githubusercontent.com/neuml/txtai/master/docs/images/flows.png#gh-light-mode-only)\\n![flows](https://raw.githubusercontent.com/neuml/txtai/master/docs/images/flows-dark.png#gh-dark-mode-only)\\n\\nIntegrate vector search, conversational search, automatic summarization, transcription, translation and more.\\n\\nSummary of txtai features:\\n\\n- 🔎 Similarity search with SQL, object storage, topic modeling, graph analysis, multiple vector index backends ([Faiss](https://github.com/facebookresearch/faiss), [Annoy](https://github.com/spotify/annoy), [Hnswlib](https://github.com/nmslib/hnswlib)) and support for external vector databases\\n- 📄 Create embeddings for text, documents, audio, images and video\\n- 💡 Pipelines powered by language models that run question-answering, labeling, transcription, translation, summarization, LLM prompts and more\\n- ↪️️ Workflows to join pipelines together and aggregate business logic. txtai processes can be simple microservices or multi-model workflows.\\n- ⚙️ Build with Python or YAML. API bindings available for [JavaScript](https://github.com/neuml/txtai.js), [Java](https://github.com/neuml/txtai.java), [Rust](https://github.com/neuml/txtai.rs) and [Go](https://github.com/neuml/txtai.go).\\n- ☁️ Cloud-native architecture that scales out with container orchestration systems (e.g. Kubernetes)\\n\\nThe following applications are powered by txtai.\\n\\n![apps](https://raw.githubusercontent.com/neuml/txtai/master/apps.jpg)\\n\\n| Application  | Description  |\\n|:----------|:-------------|\\n| [txtchat](https://github.com/neuml/txtchat) | Conversational search and workflows for all |\\n| [paperai](https://github.com/neuml/paperai) | Semantic search and workflows for medical/scientific papers |\\n| [codequestion](https://github.com/neuml/codequestion) | Semantic search for developers |\\n| [tldrstory](https://github.com/neuml/tldrstory) | Semantic search for headlines and story text |\\n\\ntxtai is built with Python 3.7+, [Hugging Face Transformers](https://github.com/huggingface/transformers), [Sentence Transformers](https://github.com/UKPLab/sentence-transformers) and [FastAPI](https://github.com/tiangolo/fastapi)\\n\\n## Why txtai?\\n\\n![why](https://raw.githubusercontent.com/neuml/txtai/master/docs/images/why.png#gh-light-mode-only)\\n![why](https://raw.githubusercontent.com/neuml/txtai/master/docs/images/why-dark.png#gh-dark-mode-only)\\n\\nIn addition to traditional search systems, a growing number of language model backed solutions are available, so why txtai?\\n\\n- Up and running in minutes with [pip](https://neuml.github.io/txtai/install/) or [Docker](https://neuml.github.io/txtai/cloud/)\\n```python\\n# Get started in a couple lines\\nfrom txtai.embeddings import Embeddings\\n\\nembeddings = Embeddings({\"path\": \"sentence-transformers/all-MiniLM-L6-v2\"})\\nembeddings.index([(0, \"Correct\", None), (1, \"Not what we hoped\", None)])\\nembeddings.search(\"positive\", 1)\\n#[(0, 0.2986203730106354)]\\n```\\n- Built-in API makes it easy to develop applications using your programming language of choice\\n```yaml\\n# app.yml\\nembeddings:\\n    path: sentence-transformers/all-MiniLM-L6-v2\\n```\\n```bash\\nCONFIG=app.yml uvicorn \"txtai.api:app\"\\ncurl -X GET \"http://localhost:8000/search?query=positive\"\\n```\\n- Run local - no need to ship data off to disparate remote services\\n- Work with micromodels all the way up to large language models (LLMs)\\n- Low footprint - install additional dependencies and scale up when needed\\n- [Learn by example](#examples) - notebooks cover all available functionality\\n\\n## Installation\\n\\n![install](https://raw.githubusercontent.com/neuml/txtai/master/docs/images/install.png#gh-light-mode-only)\\n![install](https://raw.githubusercontent.com/neuml/txtai/master/docs/images/install-dark.png#gh-dark-mode-only)\\n\\nThe easiest way to install is via pip and PyPI\\n\\n```\\npip install txtai\\n```\\n\\nPython 3.7+ is supported. Using a Python [virtual environment](https://docs.python.org/3/library/venv.html) is recommended.\\n\\nSee the detailed [install instructions](https://neuml.github.io/txtai/install) for more information covering\\n[optional dependencies](https://neuml.github.io/txtai/install/#optional-dependencies), [environment specific prerequisites](https://neuml.github.io/txtai/install/#environment-specific-prerequisites), [installing from source](https://neuml.github.io/txtai/install/#install-from-source), [conda support](https://neuml.github.io/txtai/install/#conda) and how to [run with containers](https://neuml.github.io/txtai/cloud).\\n\\n## Examples\\n\\n![examples](https://raw.githubusercontent.com/neuml/txtai/master/docs/images/examples.png#gh-light-mode-only)\\n![examples](https://raw.githubusercontent.com/neuml/txtai/master/docs/images/examples-dark.png#gh-dark-mode-only)\\n\\nThe examples directory has a series of notebooks and applications giving an overview of txtai. See the sections below.\\n\\n### Semantic Search\\n\\nBuild semantic/similarity/vector/neural search applications.\\n\\n| Notebook  | Description  |       |\\n|:----------|:-------------|------:|\\n| [Introducing txtai](https://github.com/neuml/txtai/blob/master/examples/01_Introducing_txtai.ipynb) [▶️](https://www.youtube.com/watch?v=SIezMnVdmMs) | Overview of the functionality provided by txtai | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/01_Introducing_txtai.ipynb) |\\n| [Build an Embeddings index with Hugging Face Datasets](https://github.com/neuml/txtai/blob/master/examples/02_Build_an_Embeddings_index_with_Hugging_Face_Datasets.ipynb) | Index and search Hugging Face Datasets | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/02_Build_an_Embeddings_index_with_Hugging_Face_Datasets.ipynb) |\\n| [Build an Embeddings index from a data source](https://github.com/neuml/txtai/blob/master/examples/03_Build_an_Embeddings_index_from_a_data_source.ipynb)  | Index and search a data source with word embeddings | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/03_Build_an_Embeddings_index_from_a_data_source.ipynb) |\\n| [Add semantic search to Elasticsearch](https://github.com/neuml/txtai/blob/master/examples/04_Add_semantic_search_to_Elasticsearch.ipynb)  | Add semantic search to existing search systems | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/04_Add_semantic_search_to_Elasticsearch.ipynb) |\\n| [Similarity search with images](https://github.com/neuml/txtai/blob/master/examples/13_Similarity_search_with_images.ipynb) | Embed images and text into the same space for search | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/13_Similarity_search_with_images.ipynb) |\\n| [Distributed embeddings cluster](https://github.com/neuml/txtai/blob/master/examples/15_Distributed_embeddings_cluster.ipynb) | Distribute an embeddings index across multiple data nodes | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/15_Distributed_embeddings_cluster.ipynb) |\\n| [What\\'s new in txtai 4.0](https://github.com/neuml/txtai/blob/master/examples/24_Whats_new_in_txtai_4_0.ipynb) | Content storage, SQL, object storage, reindex and compressed indexes | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/24_Whats_new_in_txtai_4_0.ipynb) |\\n| [Anatomy of a txtai index](https://github.com/neuml/txtai/blob/master/examples/29_Anatomy_of_a_txtai_index.ipynb) | Deep dive into the file formats behind a txtai embeddings index | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/29_Anatomy_of_a_txtai_index.ipynb) |\\n| [Custom Embeddings SQL functions](https://github.com/neuml/txtai/blob/master/examples/30_Embeddings_SQL_custom_functions.ipynb) | Add user-defined functions to Embeddings SQL | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/30_Embeddings_SQL_custom_functions.ipynb) |\\n| [Model explainability](https://github.com/neuml/txtai/blob/master/examples/32_Model_explainability.ipynb) | Explainability for semantic search | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/32_Model_explainability.ipynb) |\\n| [Query translation](https://github.com/neuml/txtai/blob/master/examples/33_Query_translation.ipynb) | Domain-specific natural language queries with query translation | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/33_Query_translation.ipynb) |\\n| [Build a QA database](https://github.com/neuml/txtai/blob/master/examples/34_Build_a_QA_database.ipynb) | Question matching with semantic search | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/34_Build_a_QA_database.ipynb) |\\n| [Embeddings components](https://github.com/neuml/txtai/blob/master/examples/37_Embeddings_index_components.ipynb) | Composable search with vector, SQL and scoring components | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/37_Embeddings_index_components.ipynb) |\\n| [Semantic Graphs](https://github.com/neuml/txtai/blob/master/examples/38_Introducing_the_Semantic_Graph.ipynb) | Explore topics, data connectivity and run network analysis| [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/38_Introducing_the_Semantic_Graph.ipynb) |\\n| [Topic Modeling with BM25](https://github.com/neuml/txtai/blob/master/examples/39_Classic_Topic_Modeling_with_BM25.ipynb) | Topic modeling backed by a BM25 index | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/39_Classic_Topic_Modeling_with_BM25.ipynb) |\\n| [Prompt-driven search with LLMs](https://github.com/neuml/txtai/blob/master/examples/42_Prompt_driven_search_with_LLMs.ipynb) | Embeddings-guided and Prompt-driven search with Large Language Models (LLMs) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/42_Prompt_driven_search_with_LLMs.ipynb) |\\n| [Embeddings in the Cloud](https://github.com/neuml/txtai/blob/master/examples/43_Embeddings_in_the_Cloud.ipynb) | Load and use an embeddings index from the Hugging Face Hub | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/43_Embeddings_in_the_Cloud.ipynb) |\\n| [Customize your own embeddings database](https://github.com/neuml/txtai/blob/master/examples/45_Customize_your_own_embeddings_database.ipynb) | Ways to combine vector indexes with relational databases | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/45_Customize_your_own_embeddings_database.ipynb) |\\n\\n### Pipelines\\n\\nTransform data with language model backed pipelines.\\n\\n| Notebook  | Description  |       |\\n|:----------|:-------------|------:|\\n| [Extractive QA with txtai](https://github.com/neuml/txtai/blob/master/examples/05_Extractive_QA_with_txtai.ipynb) | Introduction to extractive question-answering with txtai | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/05_Extractive_QA_with_txtai.ipynb) |\\n| [Extractive QA with Elasticsearch](https://github.com/neuml/txtai/blob/master/examples/06_Extractive_QA_with_Elasticsearch.ipynb) | Run extractive question-answering queries with Elasticsearch | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/06_Extractive_QA_with_Elasticsearch.ipynb) |\\n| [Extractive QA to build structured data](https://github.com/neuml/txtai/blob/master/examples/20_Extractive_QA_to_build_structured_data.ipynb) | Build structured datasets using extractive question-answering | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/20_Extractive_QA_to_build_structured_data.ipynb) |\\n| [Apply labels with zero shot classification](https://github.com/neuml/txtai/blob/master/examples/07_Apply_labels_with_zero_shot_classification.ipynb) | Use zero shot learning for labeling, classification and topic modeling | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/07_Apply_labels_with_zero_shot_classification.ipynb) |\\n| [Building abstractive text summaries](https://github.com/neuml/txtai/blob/master/examples/09_Building_abstractive_text_summaries.ipynb) | Run abstractive text summarization | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/09_Building_abstractive_text_summaries.ipynb) |\\n| [Extract text from documents](https://github.com/neuml/txtai/blob/master/examples/10_Extract_text_from_documents.ipynb) | Extract text from PDF, Office, HTML and more | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/10_Extract_text_from_documents.ipynb) |\\n| [Text to speech generation](https://github.com/neuml/txtai/blob/master/examples/40_Text_to_Speech_Generation.ipynb) | Generate speech from text | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/40_Text_to_Speech_Generation.ipynb) |\\n| [Transcribe audio to text](https://github.com/neuml/txtai/blob/master/examples/11_Transcribe_audio_to_text.ipynb) | Convert audio files to text | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/11_Transcribe_audio_to_text.ipynb) |\\n| [Translate text between languages](https://github.com/neuml/txtai/blob/master/examples/12_Translate_text_between_languages.ipynb) | Streamline machine translation and language detection | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/12_Translate_text_between_languages.ipynb) |\\n| [Generate image captions and detect objects](https://github.com/neuml/txtai/blob/master/examples/25_Generate_image_captions_and_detect_objects.ipynb) | Captions and object detection for images | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/25_Generate_image_captions_and_detect_objects.ipynb) |\\n| [Near duplicate image detection](https://github.com/neuml/txtai/blob/master/examples/31_Near_duplicate_image_detection.ipynb) | Identify duplicate and near-duplicate images | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/31_Near_duplicate_image_detection.ipynb) |\\n| [API Gallery](https://github.com/neuml/txtai/blob/master/examples/08_API_Gallery.ipynb) | Using txtai in JavaScript, Java, Rust and Go | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/08_API_Gallery.ipynb) |\\n\\n### Workflows\\n\\nEfficiently process data at scale.\\n\\n| Notebook  | Description  |       |\\n|:----------|:-------------|------:|\\n| [Run pipeline workflows](https://github.com/neuml/txtai/blob/master/examples/14_Run_pipeline_workflows.ipynb) [▶️](https://www.youtube.com/watch?v=UBMPDCn1gEU) | Simple yet powerful constructs to efficiently process data | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/14_Run_pipeline_workflows.ipynb) |\\n| [Transform tabular data with composable workflows](https://github.com/neuml/txtai/blob/master/examples/22_Transform_tabular_data_with_composable_workflows.ipynb) | Transform, index and search tabular data | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/22_Transform_tabular_data_with_composable_workflows.ipynb) |\\n| [Tensor workflows](https://github.com/neuml/txtai/blob/master/examples/23_Tensor_workflows.ipynb) | Performant processing of large tensor arrays | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/23_Tensor_workflows.ipynb) |\\n| [Entity extraction workflows](https://github.com/neuml/txtai/blob/master/examples/26_Entity_extraction_workflows.ipynb) | Identify entity/label combinations | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/26_Entity_extraction_workflows.ipynb) |\\n| [Workflow Scheduling](https://github.com/neuml/txtai/blob/master/examples/27_Workflow_scheduling.ipynb) | Schedule workflows with cron expressions | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/27_Workflow_scheduling.ipynb) |\\n| [Push notifications with workflows](https://github.com/neuml/txtai/blob/master/examples/28_Push_notifications_with_workflows.ipynb) | Generate and push notifications with workflows | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/28_Push_notifications_with_workflows.ipynb) |\\n| [Pictures are a worth a thousand words](https://github.com/neuml/txtai/blob/master/examples/35_Pictures_are_worth_a_thousand_words.ipynb) | Generate webpage summary images with DALL-E mini | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/35_Pictures_are_worth_a_thousand_words.ipynb) |\\n| [Run txtai with native code](https://github.com/neuml/txtai/blob/master/examples/36_Run_txtai_in_native_code.ipynb) | Execute workflows in native code with the Python C API | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/36_Run_txtai_in_native_code.ipynb) |\\n| [Prompt templates and task chains](https://github.com/neuml/txtai/blob/master/examples/44_Prompt_templates_and_task_chains.ipynb) | Build model prompts and connect tasks together with workflows | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/44_Prompt_templates_and_task_chains.ipynb) |\\n\\n### Model Training\\n\\nTrain NLP models.\\n\\n| Notebook  | Description  |       |\\n|:----------|:-------------|------:|\\n| [Train a text labeler](https://github.com/neuml/txtai/blob/master/examples/16_Train_a_text_labeler.ipynb) | Build text sequence classification models | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/16_Train_a_text_labeler.ipynb) |\\n| [Train without labels](https://github.com/neuml/txtai/blob/master/examples/17_Train_without_labels.ipynb) | Use zero-shot classifiers to train new models | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/17_Train_without_labels.ipynb) |\\n| [Train a QA model](https://github.com/neuml/txtai/blob/master/examples/19_Train_a_QA_model.ipynb) | Build and fine-tune question-answering models | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/19_Train_a_QA_model.ipynb) |\\n| [Train a language model from scratch](https://github.com/neuml/txtai/blob/master/examples/41_Train_a_language_model_from_scratch.ipynb) | Build new language models | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/41_Train_a_language_model_from_scratch.ipynb) |\\n| [Export and run models with ONNX](https://github.com/neuml/txtai/blob/master/examples/18_Export_and_run_models_with_ONNX.ipynb) | Export models with ONNX, run natively in JavaScript, Java and Rust | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/18_Export_and_run_models_with_ONNX.ipynb) |\\n| [Export and run other machine learning models](https://github.com/neuml/txtai/blob/master/examples/21_Export_and_run_other_machine_learning_models.ipynb) | Export and run models from scikit-learn, PyTorch and more | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/21_Export_and_run_other_machine_learning_models.ipynb) |\\n\\n### Applications\\n\\nSeries of example applications with txtai. Links to hosted versions on [Hugging Face Spaces](https://hf.co/spaces) also provided.\\n\\n| Application  | Description  |       |\\n|:-------------|:-------------|------:|\\n| [Basic similarity search](https://github.com/neuml/txtai/blob/master/examples/similarity.py) | Basic similarity search example. Data from the original txtai demo. |[🤗](https://hf.co/spaces/NeuML/similarity)|\\n| [Book search](https://github.com/neuml/txtai/blob/master/examples/books.py) | Book similarity search application. Index book descriptions and query using natural language statements. |*Local run only*|\\n| [Image search](https://github.com/neuml/txtai/blob/master/examples/images.py) | Image similarity search application. Index a directory of images and run searches to identify images similar to the input query. |[🤗](https://hf.co/spaces/NeuML/imagesearch)|\\n| [Summarize an article](https://github.com/neuml/txtai/blob/master/examples/article.py) | Summarize an article. Workflow that extracts text from a webpage and builds a summary. |[🤗](https://hf.co/spaces/NeuML/articlesummary)|\\n| [Wiki search](https://github.com/neuml/txtai/blob/master/examples/wiki.py) | Wikipedia search application. Queries Wikipedia API and summarizes the top result. |[🤗](https://hf.co/spaces/NeuML/wikisummary)|\\n| [Workflow builder](https://github.com/neuml/txtai/blob/master/examples/workflows.py) | Build and execute txtai workflows. Connect summarization, text extraction, transcription, translation and similarity search pipelines together to run unified workflows. |[🤗](https://hf.co/spaces/NeuML/txtai)|\\n\\n## Documentation\\n\\n[Full documentation on txtai](https://neuml.github.io/txtai) including configuration settings for pipelines, workflows, indexing and the API.\\n\\n## Further Reading\\n\\n![further](https://raw.githubusercontent.com/neuml/txtai/master/docs/images/further.png)\\n\\n- [Introducing txtai, semantic search and workflows built on Transformers](https://medium.com/neuml/introducing-txtai-an-ai-powered-search-engine-built-on-transformers-37674be252ec)\\n- [Tutorial series on Hashnode](https://neuml.hashnode.dev/series/txtai-tutorial) | [dev.to](https://dev.to/neuml/tutorial-series-on-txtai-ibg)\\n- [What\\'s new in txtai 5.0](https://medium.com/neuml/whats-new-in-txtai-5-0-e5c75a13b101) | [4.0](https://medium.com/neuml/whats-new-in-txtai-4-0-bbc3a65c3d1c)\\n- [Getting started with semantic search](https://medium.com/neuml/getting-started-with-semantic-search-a9fd9d8a48cf) | [workflows](https://medium.com/neuml/getting-started-with-semantic-workflows-2fefda6165d9)\\n- [Run workflows to transform data and build semantic search applications with txtai](https://medium.com/neuml/run-machine-learning-workflows-to-transform-data-and-build-ai-powered-text-indices-with-txtai-43d769b566a7)\\n- [Semantic search on the cheap](https://medium.com/neuml/semantic-search-on-the-cheap-55940c0fcdab)\\n- [Serverless vector search with txtai](https://medium.com/neuml/serverless-vector-search-with-txtai-96f6163ab972)\\n- [Insights from the txtai console](https://medium.com/neuml/insights-from-the-txtai-console-d307c28e149e)\\n- [The big and small of txtai](https://medium.com/neuml/the-big-and-small-of-txtai-4ca405c1b82)\\n\\n## Contributing\\n\\nFor those who would like to contribute to txtai, please see [this guide](https://github.com/neuml/.github/blob/master/CONTRIBUTING.md).\\n'},\n",
       " {'repo': './TapiocaFox/Daijishou',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '![Daijisho Preview](/imgs/cover_new.png)\\n\\n<a href=\\'https://play.google.com/store/apps/details?id=com.magneticchen.daijishou\\'><img alt=\\'Get it on Google Play\\' src=\\'https://cdn.rawgit.com/steverichey/google-play-badge-svg/master/img/en_get.svg\\' height=\\'70px\\'/></a>\\n\\n# Daijishō\\n\\nDaijishō is a retro launcher that lets you manage your retro games libraries. Daijishō cares about integrated experience, expansibility, aesthetic, and pragmatic usabilities, which let you focus on the games themselves. It will be updated continuously in the future based on user feedback and my own retro gaming experience during my free time. Beware, it does not come with emulators.\\n\\n\"Wat? What does \\'Daijishō\\' even mean?\", you ask? \"Daijishō\" (aka \"だいじしょう\") actually stands for [\"台字章\"](https://zh.wikipedia.org/wiki/%E8%87%BA%E7%81%A3%E7%B8%BD%E7%9D%A3%E5%BA%9C%E6%96%87%E5%AE%98%E6%9C%8D%E8%A3%9D) in kanji, which was a pattern widely used in Taiwan during the Japanese period. Anyway, happy gaming.\\n\\n### About this Repository\\nDaijishō is currently **closed-source**. Nonetheless, I do have some prerequisites for open-sourcing it. You can still report issues or submit suggestions here. And Daijishō will always be free!\\n\\n<a href=\"https://discord.com/invite/nJbxdT3QQE\" target=\"_blank\">\\n    <img src=\"https://img.shields.io/discord/965270127312535592?label=&logo=discord&logoColor=ffffff&color=5865F2&labelColor=404EED\">\\n</a>\\n<a href=\"https://www.youtube.com/channel/UCLdTuA-K8bw4zLczwWwxEaA\" target=\"_blank\">\\n    <img src=\"https://img.shields.io/static/v1?label=&message=subscribe&style=flat&logo=youtube&logoColor=ffffff&color=FF0000&labelColor=cc0000\">\\n</a>\\n<a href=\"https://github.com/magneticchen/Daijishou/actions/workflows/update_indices.yml\" target=\"_blank\">\\n    <img src=\"https://github.com/magneticchen/Daijishou/actions/workflows/update_indices.yml/badge.svg\">\\n</a>\\n<a href=\"https://github.com/magneticchen/Daijishou/releases\" target=\"_blank\">\\n    <img src=\"https://img.shields.io/github/v/release/magneticchen/Daijishou?logo=android\">\\n</a>\\n<a href=\"/release-notes/1_4_release_note.md\" target=\"_blank\">\\n    <img src=\"https://img.shields.io/static/v1?label=release+note&message=1.4&style=flat\">\\n</a>\\n\\n# Make the Most of Daijishō\\n 1. ***Install preferred emulators***, apps first, then Daijishō will take care of the rest\\n 2. ***Setup hotkeys*** for better and intuitive navigation\\n 3. ***Download and import the platforms*** you want\\n 4. ***Set game files, path, and sync*** for each platform\\n 5. ***Download wallpaper packs*** and ***select your preferred theme color*** on the settings page\\n 6. ***Add widgets*** on the widget page\\n \\nWell done. You have now made the most of it. Here are some additional tips for you.\\n - Take a look of [wiki from Jetup13](https://github.com/Jetup13/Retroid-Pocket-2-Plus-Wiki/wiki/Front-Ends#daijishou), or [video from RetroGameCorps](https://www.youtube.com/watch?v=l-AhfEGuMao) might help.\\n - You can [view the wallpaper gallery](https://daijishou.github.io/Gallery/).\\n - You can long click items to see further details.\\n - You can inspect your RetroAchievements records if you log in.\\n - Switching hotkeys have different abilities in different sections.\\n - Be sure to make use of widgets as they are very useful, like RSS, Activity, Pining games, etc.\\n - You can update your platforms by re-downloading and re-importing them from the list without losing records\\n - Clicking the Daijishō icon 7 times on the About page will enable NSFW mode.\\n - See \"Main features\" in Quick Look to make sure you don\\'t miss any features!\\n\\n# A Quick look at Daijishō\\n### Is Daijishō a Pegasus fork? (1 of 6)\\nNope. But you can import some config for emulators from pegasus.\\n\\n### Main features (2 of 6)\\n\\n![Widgets Preview](/imgs/widgets_4.png)\\n\\n(Widgets)\\n\\n![](/release-notes/1_4_release_note/appearance_general.png)\\n\\n(Detailed View)\\n\\n![Genres Preview](/imgs/genres_3.png)\\n\\n(Genres)\\n\\n![RetroAchievements Preview](/imgs/achievement_7.png)\\n\\n(RetroAchievements)\\n\\n![Search Preview](/imgs/search_2.png)\\n\\n(Search)\\n\\nAnd much more...\\n\\n### What is \"Player\" (3 of 6)\\nPlayer is a set of arguments that can be configured to execute playable files filtered by regular expressions from your library with launching arguments. Player is usually associated with emulators or RetroArch.\\n\\n### What is \"Platform\" (4 of 6)\\nPlatform contains players added in Daijishō that accept various files from selected sync paths. Platform can also be configured to scrape correct boxarts and other preview media and can be set up to match its appearance and aesthetics.\\n\\n![Platforms Preview](/imgs/platform_collection_wallpaper_view_2.png)\\n\\n![Library Preview](/imgs/platform_library_3.png)\\n\\n### How to Add Platforms and Players (5 of 6)\\nYou can download this from this GitHub page, which is available in the Daijishō\\'s settings page. Or you can import from Pegasus frontend or other\\'s shared and configured platform JSON files. Also, you can manually add players then create platforms from those players.\\n\\n![Select Platforms Preview](/imgs/download_platforms_2.png)\\n\\n\\n### Supported Languages (6 of 6)\\n`English`, `Portuguese (Português)`, `Taiwanese mandarin (台灣國語)`, `Japanese (日本語)`, `French (Français)`, `Italian (Italiana)`, `Spanish (Español)`, `Korean (한국인)`, `German (Deutsche)`, `Hindi (हिंदी)`\\n\\n# Words from the Author\\n### Before Further Discussion\\n![Widgets Preview](/imgs/tapicofox_widgets.png)\\n> Here is my widgets setup :3\\n### How Much Effort does the Author put into this Project?\\nThis is a side project. The project is solely developed on my Retroid Pocket 2+ and Android emulators. I will continue work on it whenever I feel motivated to (in my free time; usually on the weekend).\\n\\n### What can you do to Support?\\nYou can star this GitHub page, donate, promote Daijishō in the communities or update the platform list in this GitHub page.\\n\\n### Taking a Break from the Project\\nDaijishō evolved a lot in 2022. Including UI improvements, retro achievements, wallpaper packs, the new widgets page, genres, backup, merge items and various other small details. I know that there is still room for Daijishō to grow and things to be improved. And I am thankfully aware of people\\'s ideas and suggestions. However, In the meantime, I also have important things to be done awaiting me. And it\\'s time for me to leave for a while from this repetitive routine for 12 continuous months. Thus, I decided to take a break from the Daijishō project, probably for quarters. With the relatively low attention and activities focusing on Daijishō. Nevertheless, if you are familiar with Android Kotlin development and you wish to contribute or intergrate features for Daijishō, you can still DM me for the possibilities.\\n\\n\\n# More About Daijishō\\n### Documentation\\n - [Daijishō Console](/docs/daijishou_console.md)\\n - [Daijishō DSESS](/docs/dsess.md)\\n - [Daijishō Player Template (.dpt)](/docs/daijishou_player_template.md)\\n\\n### Related Links\\n - [Daijishō EmuGen Wiki](https://emulation.gametechwiki.com/index.php/Daijish%C5%8D)\\n - [Daijishō (台字章) History Wikipedia](https://zh.wikipedia.org/wiki/%E8%87%BA%E7%81%A3%E7%B8%BD%E7%9D%A3%E5%BA%9C%E6%96%87%E5%AE%98%E6%9C%8D%E8%A3%9D)\\n\\n### Donation\\n - [PayPal](https://paypal.me/magneticchen)\\n\\n> I am taking a break from the project\\n\\n<!-- [Patreon](https://www.patreon.com/magneticchen) -->\\n\\n# Copyright\\nCopyright©2023 TapiocaFox. Designed in Taiwan.\\n'},\n",
       " {'repo': './coqui-ai/TTS',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '\\n\\n## 🐸Coqui.ai News\\n- 📣 Coqui Studio API is landed on 🐸TTS. You can use the studio voices in combination with 🐸TTS models. [Example](https://github.com/coqui-ai/TTS/blob/dev/README.md#-python-api)\\n- 📣 Voice generation with prompts - **Prompt to Voice** - is live on Coqui.ai!! [Blog Post](https://coqui.ai/blog/tts/prompt-to-voice)\\n- 📣 Clone your voice with a single click on [🐸Coqui.ai](https://app.coqui.ai/auth/signin)\\n<br>\\n\\n## <img src=\"https://raw.githubusercontent.com/coqui-ai/TTS/main/images/coqui-log-green-TTS.png\" height=\"56\"/>\\n\\n\\n🐸TTS is a library for advanced Text-to-Speech generation. It\\'s built on the latest research, was designed to achieve the best trade-off among ease-of-training, speed and quality.\\n🐸TTS comes with pretrained models, tools for measuring dataset quality and already used in **20+ languages** for products and research projects.\\n\\n[![Dicord](https://img.shields.io/discord/1037326658807533628?color=%239B59B6&label=chat%20on%20discord)](https://discord.gg/5eXr5seRrv)\\n[![License](<https://img.shields.io/badge/License-MPL%202.0-brightgreen.svg>)](https://opensource.org/licenses/MPL-2.0)\\n[![PyPI version](https://badge.fury.io/py/TTS.svg)](https://badge.fury.io/py/TTS)\\n[![Covenant](https://camo.githubusercontent.com/7d620efaa3eac1c5b060ece5d6aacfcc8b81a74a04d05cd0398689c01c4463bb/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f436f6e7472696275746f72253230436f76656e616e742d76322e3025323061646f707465642d6666363962342e737667)](https://github.com/coqui-ai/TTS/blob/master/CODE_OF_CONDUCT.md)\\n[![Downloads](https://pepy.tech/badge/tts)](https://pepy.tech/project/tts)\\n[![DOI](https://zenodo.org/badge/265612440.svg)](https://zenodo.org/badge/latestdoi/265612440)\\n\\n![GithubActions](https://github.com/coqui-ai/TTS/actions/workflows/aux_tests.yml/badge.svg)\\n![GithubActions](https://github.com/coqui-ai/TTS/actions/workflows/data_tests.yml/badge.svg)\\n![GithubActions](https://github.com/coqui-ai/TTS/actions/workflows/docker.yaml/badge.svg)\\n![GithubActions](https://github.com/coqui-ai/TTS/actions/workflows/inference_tests.yml/badge.svg)\\n![GithubActions](https://github.com/coqui-ai/TTS/actions/workflows/style_check.yml/badge.svg)\\n![GithubActions](https://github.com/coqui-ai/TTS/actions/workflows/text_tests.yml/badge.svg)\\n![GithubActions](https://github.com/coqui-ai/TTS/actions/workflows/tts_tests.yml/badge.svg)\\n![GithubActions](https://github.com/coqui-ai/TTS/actions/workflows/vocoder_tests.yml/badge.svg)\\n![GithubActions](https://github.com/coqui-ai/TTS/actions/workflows/zoo_tests0.yml/badge.svg)\\n![GithubActions](https://github.com/coqui-ai/TTS/actions/workflows/zoo_tests1.yml/badge.svg)\\n![GithubActions](https://github.com/coqui-ai/TTS/actions/workflows/zoo_tests2.yml/badge.svg)\\n[![Docs](<https://readthedocs.org/projects/tts/badge/?version=latest&style=plastic>)](https://tts.readthedocs.io/en/latest/)\\n\\n📰 [**Subscribe to 🐸Coqui.ai Newsletter**](https://coqui.ai/?subscription=true)\\n\\n📢 [English Voice Samples](https://erogol.github.io/ddc-samples/) and [SoundCloud playlist](https://soundcloud.com/user-565970875/pocket-article-wavernn-and-tacotron2)\\n\\n📄 [Text-to-Speech paper collection](https://github.com/erogol/TTS-papers)\\n\\n<img src=\"https://static.scarf.sh/a.png?x-pxid=cf317fe7-2188-4721-bc01-124bb5d5dbb2\" />\\n\\n## 💬 Where to ask questions\\nPlease use our dedicated channels for questions and discussion. Help is much more valuable if it\\'s shared publicly so that more people can benefit from it.\\n\\n| Type                            | Platforms                               |\\n| ------------------------------- | --------------------------------------- |\\n| 🚨 **Bug Reports**              | [GitHub Issue Tracker]                  |\\n| 🎁 **Feature Requests & Ideas** | [GitHub Issue Tracker]                  |\\n| 👩\\u200d💻 **Usage Questions**          | [GitHub Discussions]                    |\\n| 🗯 **General Discussion**       | [GitHub Discussions] or [Discord]   |\\n\\n[github issue tracker]: https://github.com/coqui-ai/tts/issues\\n[github discussions]: https://github.com/coqui-ai/TTS/discussions\\n[discord]: https://discord.gg/5eXr5seRrv\\n[Tutorials and Examples]: https://github.com/coqui-ai/TTS/wiki/TTS-Notebooks-and-Tutorials\\n\\n\\n## 🔗 Links and Resources\\n| Type                            | Links                               |\\n| ------------------------------- | --------------------------------------- |\\n| 💼 **Documentation**              | [ReadTheDocs](https://tts.readthedocs.io/en/latest/)\\n| 💾 **Installation**               | [TTS/README.md](https://github.com/coqui-ai/TTS/tree/dev#install-tts)|\\n| 👩\\u200d💻 **Contributing**               | [CONTRIBUTING.md](https://github.com/coqui-ai/TTS/blob/main/CONTRIBUTING.md)|\\n| 📌 **Road Map**                   | [Main Development Plans](https://github.com/coqui-ai/TTS/issues/378)\\n| 🚀 **Released Models**            | [TTS Releases](https://github.com/coqui-ai/TTS/releases) and [Experimental Models](https://github.com/coqui-ai/TTS/wiki/Experimental-Released-Models)|\\n\\n## 🥇 TTS Performance\\n<p align=\"center\"><img src=\"https://raw.githubusercontent.com/coqui-ai/TTS/main/images/TTS-performance.png\" width=\"800\" /></p>\\n\\nUnderlined \"TTS*\" and \"Judy*\" are 🐸TTS models\\n<!-- [Details...](https://github.com/coqui-ai/TTS/wiki/Mean-Opinion-Score-Results) -->\\n\\n## Features\\n- High-performance Deep Learning models for Text2Speech tasks.\\n    - Text2Spec models (Tacotron, Tacotron2, Glow-TTS, SpeedySpeech).\\n    - Speaker Encoder to compute speaker embeddings efficiently.\\n    - Vocoder models (MelGAN, Multiband-MelGAN, GAN-TTS, ParallelWaveGAN, WaveGrad, WaveRNN)\\n- Fast and efficient model training.\\n- Detailed training logs on the terminal and Tensorboard.\\n- Support for Multi-speaker TTS.\\n- Efficient, flexible, lightweight but feature complete `Trainer API`.\\n- Released and ready-to-use models.\\n- Tools to curate Text2Speech datasets under```dataset_analysis```.\\n- Utilities to use and test your models.\\n- Modular (but not too much) code base enabling easy implementation of new ideas.\\n\\n## Implemented Models\\n### Spectrogram models\\n- Tacotron: [paper](https://arxiv.org/abs/1703.10135)\\n- Tacotron2: [paper](https://arxiv.org/abs/1712.05884)\\n- Glow-TTS: [paper](https://arxiv.org/abs/2005.11129)\\n- Speedy-Speech: [paper](https://arxiv.org/abs/2008.03802)\\n- Align-TTS: [paper](https://arxiv.org/abs/2003.01950)\\n- FastPitch: [paper](https://arxiv.org/pdf/2006.06873.pdf)\\n- FastSpeech: [paper](https://arxiv.org/abs/1905.09263)\\n- FastSpeech2: [paper](https://arxiv.org/abs/2006.04558)\\n- SC-GlowTTS: [paper](https://arxiv.org/abs/2104.05557)\\n- Capacitron: [paper](https://arxiv.org/abs/1906.03402)\\n- OverFlow: [paper](https://arxiv.org/abs/2211.06892)\\n- Neural HMM TTS: [paper](https://arxiv.org/abs/2108.13320)\\n\\n### End-to-End Models\\n- VITS: [paper](https://arxiv.org/pdf/2106.06103)\\n- YourTTS: [paper](https://arxiv.org/abs/2112.02418)\\n\\n### Attention Methods\\n- Guided Attention: [paper](https://arxiv.org/abs/1710.08969)\\n- Forward Backward Decoding: [paper](https://arxiv.org/abs/1907.09006)\\n- Graves Attention: [paper](https://arxiv.org/abs/1910.10288)\\n- Double Decoder Consistency: [blog](https://erogol.com/solving-attention-problems-of-tts-models-with-double-decoder-consistency/)\\n- Dynamic Convolutional Attention: [paper](https://arxiv.org/pdf/1910.10288.pdf)\\n- Alignment Network: [paper](https://arxiv.org/abs/2108.10447)\\n\\n### Speaker Encoder\\n- GE2E: [paper](https://arxiv.org/abs/1710.10467)\\n- Angular Loss: [paper](https://arxiv.org/pdf/2003.11982.pdf)\\n\\n### Vocoders\\n- MelGAN: [paper](https://arxiv.org/abs/1910.06711)\\n- MultiBandMelGAN: [paper](https://arxiv.org/abs/2005.05106)\\n- ParallelWaveGAN: [paper](https://arxiv.org/abs/1910.11480)\\n- GAN-TTS discriminators: [paper](https://arxiv.org/abs/1909.11646)\\n- WaveRNN: [origin](https://github.com/fatchord/WaveRNN/)\\n- WaveGrad: [paper](https://arxiv.org/abs/2009.00713)\\n- HiFiGAN: [paper](https://arxiv.org/abs/2010.05646)\\n- UnivNet: [paper](https://arxiv.org/abs/2106.07889)\\n\\n### Voice Conversion\\n- FreeVC: [paper](https://arxiv.org/abs/2210.15418)\\n\\nYou can also help us implement more models.\\n\\n## Install TTS\\n🐸TTS is tested on Ubuntu 18.04 with **python >= 3.7, < 3.11.**.\\n\\nIf you are only interested in [synthesizing speech](https://tts.readthedocs.io/en/latest/inference.html) with the released 🐸TTS models, installing from PyPI is the easiest option.\\n\\n```bash\\npip install TTS\\n```\\n\\nIf you plan to code or train models, clone 🐸TTS and install it locally.\\n\\n```bash\\ngit clone https://github.com/coqui-ai/TTS\\npip install -e .[all,dev,notebooks]  # Select the relevant extras\\n```\\n\\nIf you are on Ubuntu (Debian), you can also run following commands for installation.\\n\\n```bash\\n$ make system-deps  # intended to be used on Ubuntu (Debian). Let us know if you have a different OS.\\n$ make install\\n```\\n\\nIf you are on Windows, 👑@GuyPaddock wrote installation instructions [here](https://stackoverflow.com/questions/66726331/how-can-i-run-mozilla-tts-coqui-tts-training-with-cuda-on-a-windows-system).\\n\\n\\n## Docker Image\\nYou can also try TTS without install with the docker image.\\nSimply run the following command and you will be able to run TTS without installing it.\\n\\n```bash\\ndocker run --rm -it -p 5002:5002 --entrypoint /bin/bash ghcr.io/coqui-ai/tts-cpu\\npython3 TTS/server/server.py --list_models #To get the list of available models\\npython3 TTS/server/server.py --model_name tts_models/en/vctk/vits # To start a server\\n```\\n\\nYou can then enjoy the TTS server [here](http://[::1]:5002/)\\nMore details about the docker images (like GPU support) can be found [here](https://tts.readthedocs.io/en/latest/docker_images.html)\\n\\n\\n## Synthesizing speech by 🐸TTS\\n\\n### 🐍 Python API\\n\\n```python\\nfrom TTS.api import TTS\\n\\n# Running a multi-speaker and multi-lingual model\\n\\n# List available 🐸TTS models and choose the first one\\nmodel_name = TTS.list_models()[0]\\n# Init TTS\\ntts = TTS(model_name)\\n# Run TTS\\n# ❗ Since this model is multi-speaker and multi-lingual, we must set the target speaker and the language\\n# Text to speech with a numpy output\\nwav = tts.tts(\"This is a test! This is also a test!!\", speaker=tts.speakers[0], language=tts.languages[0])\\n# Text to speech to a file\\ntts.tts_to_file(text=\"Hello world!\", speaker=tts.speakers[0], language=tts.languages[0], file_path=\"output.wav\")\\n\\n# Running a single speaker model\\n\\n# Init TTS with the target model name\\ntts = TTS(model_name=\"tts_models/de/thorsten/tacotron2-DDC\", progress_bar=False, gpu=False)\\n# Run TTS\\ntts.tts_to_file(text=\"Ich bin eine Testnachricht.\", file_path=OUTPUT_PATH)\\n\\n# Example voice cloning with YourTTS in English, French and Portuguese:\\ntts = TTS(model_name=\"tts_models/multilingual/multi-dataset/your_tts\", progress_bar=False, gpu=True)\\ntts.tts_to_file(\"This is voice cloning.\", speaker_wav=\"my/cloning/audio.wav\", language=\"en\", file_path=\"output.wav\")\\ntts.tts_to_file(\"C\\'est le clonage de la voix.\", speaker_wav=\"my/cloning/audio.wav\", language=\"fr-fr\", file_path=\"output.wav\")\\ntts.tts_to_file(\"Isso é clonagem de voz.\", speaker_wav=\"my/cloning/audio.wav\", language=\"pt-br\", file_path=\"output.wav\")\\n\\n\\n# Example voice conversion converting speaker of the `source_wav` to the speaker of the `target_wav`\\n\\ntts = TTS(model_name=\"voice_conversion_models/multilingual/vctk/freevc24\", progress_bar=False, gpu=True)\\ntts.voice_conversion_to_file(source_wav=\"my/source.wav\", target_wav=\"my/target.wav\", file_path=\"output.wav\")\\n\\n# Example voice cloning by a single speaker TTS model combining with the voice conversion model. This way, you can\\n# clone voices by using any model in 🐸TTS.\\n\\ntts = TTS(\"tts_models/de/thorsten/tacotron2-DDC\")\\ntts.tts_with_vc_to_file(\\n    \"Wie sage ich auf Italienisch, dass ich dich liebe?\",\\n    speaker_wav=\"target/speaker.wav\",\\n    file_path=\"ouptut.wav\"\\n)\\n\\n# Example text to speech using [🐸Coqui Studio](https://coqui.ai) models. You can use all of your available speakers in the studio.\\n# [🐸Coqui Studio](https://coqui.ai) API token is required. You can get it from the [account page](https://coqui.ai/account).\\n# You should set the `COQUI_STUDIO_TOKEN` environment variable to use the API token.\\n\\n# If you have a valid API token set you will see the studio speakers as separate models in the list.\\n# The name format is coqui_studio/en/<studio_speaker_name>/coqui_studio\\nmodels = TTS().list_models()\\n# Init TTS with the target studio speaker\\ntts = TTS(model_name=\"coqui_studio/en/Torcull Diarmuid/coqui_studio\", progress_bar=False, gpu=False)\\n# Run TTS\\ntts.tts_to_file(text=\"This is a test.\", file_path=OUTPUT_PATH)\\n# Run TTS with emotion and speed control\\ntts.tts_to_file(text=\"This is a test.\", file_path=OUTPUT_PATH, emotion=\"Happy\", speed=1.5)\\n```\\n\\n### Command line `tts`\\n#### Single Speaker Models\\n\\n- List provided models:\\n\\n    ```\\n    $ tts --list_models\\n    ```\\n- Get model info (for both tts_models and vocoder_models):\\n    - Query by type/name:\\n        The model_info_by_name uses the name as it from the --list_models.\\n        ```\\n        $ tts --model_info_by_name \"<model_type>/<language>/<dataset>/<model_name>\"\\n        ```\\n        For example:\\n\\n        ```\\n        $ tts --model_info_by_name tts_models/tr/common-voice/glow-tts\\n        ```\\n        ```\\n        $ tts --model_info_by_name vocoder_models/en/ljspeech/hifigan_v2\\n        ```\\n    - Query by type/idx:\\n        The model_query_idx uses the corresponding idx from --list_models.\\n        ```\\n        $ tts --model_info_by_idx \"<model_type>/<model_query_idx>\"\\n        ```\\n        For example:\\n\\n        ```\\n        $ tts --model_info_by_idx tts_models/3\\n        ```\\n\\n- Run TTS with default models:\\n\\n    ```\\n    $ tts --text \"Text for TTS\" --out_path output/path/speech.wav\\n    ```\\n\\n- Run a TTS model with its default vocoder model:\\n\\n    ```\\n    $ tts --text \"Text for TTS\" --model_name \"<model_type>/<language>/<dataset>/<model_name>\" --out_path output/path/speech.wav\\n    ```\\n  For example:\\n\\n    ```\\n    $ tts --text \"Text for TTS\" --model_name \"tts_models/en/ljspeech/glow-tts\" --out_path output/path/speech.wav\\n    ```\\n\\n- Run with specific TTS and vocoder models from the list:\\n\\n    ```\\n    $ tts --text \"Text for TTS\" --model_name \"<model_type>/<language>/<dataset>/<model_name>\" --vocoder_name \"<model_type>/<language>/<dataset>/<model_name>\" --out_path output/path/speech.wav\\n    ```\\n\\n  For example:\\n\\n    ```\\n    $ tts --text \"Text for TTS\" --model_name \"tts_models/en/ljspeech/glow-tts\" --vocoder_name \"vocoder_models/en/ljspeech/univnet\" --out_path output/path/speech.wav\\n    ```\\n\\n\\n- Run your own TTS model (Using Griffin-Lim Vocoder):\\n\\n    ```\\n    $ tts --text \"Text for TTS\" --model_path path/to/model.pth --config_path path/to/config.json --out_path output/path/speech.wav\\n    ```\\n\\n- Run your own TTS and Vocoder models:\\n    ```\\n    $ tts --text \"Text for TTS\" --model_path path/to/model.pth --config_path path/to/config.json --out_path output/path/speech.wav\\n        --vocoder_path path/to/vocoder.pth --vocoder_config_path path/to/vocoder_config.json\\n    ```\\n\\n#### Multi-speaker Models\\n\\n- List the available speakers and choose a <speaker_id> among them:\\n\\n    ```\\n    $ tts --model_name \"<language>/<dataset>/<model_name>\"  --list_speaker_idxs\\n    ```\\n\\n- Run the multi-speaker TTS model with the target speaker ID:\\n\\n    ```\\n    $ tts --text \"Text for TTS.\" --out_path output/path/speech.wav --model_name \"<language>/<dataset>/<model_name>\"  --speaker_idx <speaker_id>\\n    ```\\n\\n- Run your own multi-speaker TTS model:\\n\\n    ```\\n    $ tts --text \"Text for TTS\" --out_path output/path/speech.wav --model_path path/to/model.pth --config_path path/to/config.json --speakers_file_path path/to/speaker.json --speaker_idx <speaker_id>\\n    ```\\n\\n## Directory Structure\\n```\\n|- notebooks/       (Jupyter Notebooks for model evaluation, parameter selection and data analysis.)\\n|- utils/           (common utilities.)\\n|- TTS\\n    |- bin/             (folder for all the executables.)\\n      |- train*.py                  (train your target model.)\\n      |- ...\\n    |- tts/             (text to speech models)\\n        |- layers/          (model layer definitions)\\n        |- models/          (model definitions)\\n        |- utils/           (model specific utilities.)\\n    |- speaker_encoder/ (Speaker Encoder models.)\\n        |- (same)\\n    |- vocoder/         (Vocoder models.)\\n        |- (same)\\n```\\n'},\n",
       " {'repo': './OpenBB-finance/OpenBBTerminal',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '<!-- PROJECT SHIELDS -->\\n<!--\\n*** I\\'m using markdown \"reference style\" links for readability.\\n*** Reference links are enclosed in brackets [ ] instead of parentheses ( ).\\n*** See the bottom of this document for the declaration of the reference variables\\n*** for contributors-url, forks-url, etc. This is an optional, concise syntax you may use.\\n*** https://www.markdownguide.org/basic-syntax/#reference-style-links\\n-->\\n\\n[![Stargazers][stars-shield]][stars-url]\\n[![Forks][forks-shield]][forks-url]\\n[![Contributors][contributors-shield]][contributors-url]\\n[![MIT License][license-shield]][license-url]\\n\\n[![Issues][issues-shield]][issues-url]\\n[![Bugs Open][bugs-open-shield]][bugs-open-url]\\n[![Bugs Closed][bugs-closed-shield]][bugs-closed-url]\\n\\n[![GitHub release](https://img.shields.io/github/release/OpenBB-finance/OpenBBTerminal.svg?maxAge=3600)](https://github.com/OpenBB-finance/OpenBBTerminal/releases)\\n[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)\\n[![TODOs](https://badgen.net/https/api.tickgit.com/badgen/github.com/OpenBB-finance/OpenBBTerminal/main)](https://www.tickgit.com/browse?repo=github.com/OpenBB-finance/OpenBBTerminal&branch=main)\\n\\n![Discord Shield](https://discordapp.com/api/guilds/831165782750789672/widget.png?style=shield)\\n[![Twitter](https://img.shields.io/twitter/url/https/twitter.com/openbb_finance.svg?style=social&label=Follow%20%40openbb_finance)](https://twitter.com/openbb_finance)\\n\\n<!-- PROJECT LOGO -->\\n\\n| OpenBB is committed to build the future of investment research by focusing on an open source infrastructure accessible to everyone, everywhere. |\\n|:--:|\\n| [![openbb.jpg](/images/openbb_gradient.png)](https://openbb.co) |\\n| Check our website at [openbb.co](https://openbb.co) |\\n\\n<br />\\n\\n<p align=\"center\">\\n  <h3 align=\"center\">OpenBB Terminal 🚀</h3>\\n  <h4 align=\"center\">Documentation can be found at: https://openbb.co/docs </h4>\\n  <p align=\"center\">Click on the GIF below for a DEMO of the terminal.</p>\\n\\n  <p align=\"center\">\\n     <a href=\"https://www.youtube.com/watch?v=fqGPK8OVHLk\" rel=\"OpenBB Terminal Demo\">\\n        <img src=\"images/openbb_terminal_illustration.gif\" alt=\"OpenBB Terminal Illustration\" width=\"100%\"/>\\n     </a>\\n  </p>\\n\\n  <p align=\"center\">\\n    <a href=\"https://docs.openbb.co/terminal/installation\"><strong>≪  GETTING STARTED</strong></a>\\n    &nbsp · &nbsp <a href=\"https://github.com/OpenBB-finance/OpenBBTerminal/tree/master/CONTRIBUTING.md\"><strong>CONTRIBUTING</strong></a> &nbsp · &nbsp\\n    <a href=\"https://docs.openbb.co/terminal\">\\n    <strong>SEE FEATURES »</strong></a>\\n  </p>\\n</p>\\n\\n<!-- TABLE OF CONTENTS -->\\n<details closed=\"closed\">\\n  <summary><h2 style=\"display: inline-block\">Table of Contents</h2></summary>\\n  <ol>\\n    <li><a href=\"#1-installation\">Installation</a></li>\\n    <li><a href=\"#2-contributing\">Contributing</a></li>\\n    <li><a href=\"#3-license\">License</a></li>\\n    <li><a href=\"#4-disclaimer\">Disclaimer</a></li>\\n    <li><a href=\"#5-contacts\">Contacts</a></li>\\n    <li><a href=\"#6-star-history\">Star History</a></li>\\n    <li><a href=\"#7-contributors\">Contributors</a></li>\\n  </ol>\\n</details>\\n\\n## 1. Installation\\n\\nIf you wish to install the OpenBB Terminal or the OpenBB SDK, please use one of the following options:\\n\\n|**OpenBB Terminal**|**Usage**|\\n|:-|:-|\\n|[Windows Installer](https://docs.openbb.co/terminal/installation/windows)|Recommended way for Windows if you just want to use the OpenBB Terminal|\\n|[MacOS Installer](https://docs.openbb.co/terminal/installation/macos)|Recommended way for MacOS if you just want to use the OpenBB Terminal|\\n|[Source](https://docs.openbb.co/terminal/installation/source)|If you wish to contribute to the development of the OpenBB Terminal|\\n|[Docker](https://docs.openbb.co/terminal/installation/docker)|An alternative way if you just want to use the OpenBB Terminal|\\n\\n|**OpenBB SDK** &nbsp; &nbsp; &nbsp; &nbsp; |**Usage**|\\n|:-|:-|\\n|[PyPI](https://docs.openbb.co/terminal/installation/pypi)|If you wish to use the OpenBB SDK in Python or Jupyter Notebooks|\\n|[Source](https://docs.openbb.co/terminal/installation/source)|If you wish to contribute to the development of the OpenBB Terminal|\\n<!-- nbsp;| -->\\n\\n## 2. Contributing\\n\\nThere are three main ways of contributing to this project. (Hopefully you have starred the project by now ⭐️)\\n\\n### Become a Contributor\\n\\n* More information on our [CONTRIBUTING GUIDELINES](/CONTRIBUTING.md).\\n\\n### Create a GitHub ticket\\n\\nBefore creating a ticket make sure the one you are creating doesn\\'t exist already [here](https://github.com/OpenBB-finance/OpenBBTerminal/issues)\\n\\n* [Report bug](https://github.com/OpenBB-finance/OpenBBTerminal/issues/new?assignees=&labels=bug&template=bug_report.md&title=%5BBug%5D)\\n* [Suggest improvement](https://github.com/OpenBB-finance/OpenBBTerminal/issues/new?assignees=&labels=enhancement&template=enhancement.md&title=%5BIMPROVE%5D)\\n* [Request a feature](https://github.com/OpenBB-finance/OpenBBTerminal/issues/new?assignees=&labels=new+feature&template=feature_request.md&title=%5BFR%5D)\\n\\n### Provide feedback\\n\\nWe are most active on [our Discord](https://openbb.co/discord), but feel free to reach out to us in any of [our social media](https://openbb.co/links) for feedback.\\n\\n## 3. License\\n\\nDistributed under the MIT License. See\\n[LICENSE](https://github.com/OpenBB-finance/OpenBBTerminal/blob/main/LICENSE) for more information.\\n\\n## 4. Disclaimer\\n\\nTrading in financial instruments involves high risks including the risk of losing some, or all, of your investment\\namount, and may not be suitable for all investors.\\n\\nBefore deciding to trade in a financial instrument you should be fully informed of the risks and costs associated with trading the financial markets, carefully consider your investment objectives, level of experience, and risk appetite, and seek professional advice where needed.\\n\\nThe data contained in the OpenBBTerminal is not necessarily accurate.\\n\\nOpenBB and any provider of the data contained in this website will not accept liability for any loss or damage as a result of your trading, or your reliance on the information displayed.\\n\\nAll names, logos, and brands of third parties that may be referenced in our sites, products or documentation are trademarks of their respective owners. Unless otherwise specified, OpenBB and its products and services are not endorsed by, sponsored by, or affiliated with these third parties.\\nOur use of these names, logos, and brands is for identification purposes only, and does not imply any such endorsement, sponsorship, or affiliation.\\n\\n## 5. Contacts\\n\\nIf you have any questions about the terminal or anything OpenBB, feel free to email us at `support@openbb.co`\\n\\nIf you want to say hi, or are interested in partnering with us, feel free to reach us at `hello@openbb.co`\\n\\nAny of our social media platforms: [openbb.co/links](https://openbb.co/links)\\n\\n## 6. Star History\\n\\nThis is a proxy of our growth and that we are just getting started. But for more metrics important to us check [openbb.co/open](https://openbb.co/open).\\n\\n[![Star History Chart](https://api.star-history.com/svg?repos=openbb-finance/OpenBBTerminal&type=Date)](https://star-history.com/#openbb-finance/OpenBBTerminal&Date)\\n\\n## 7. Contributors\\n\\nOpenBB wouldn\\'t be OpenBB without you. If we are going to disrupt financial industry, every contribution counts. Thank you for being part of this journey.\\n\\n<a href=\"https://github.com/OpenBB-finance/OpenBBTerminal/graphs/contributors\">\\n   <img src=\"https://contributors-img.web.app/image?repo=OpenBB-finance/OpenBBTerminal\" width=\"800\"/>\\n</a>\\n\\n<!-- MARKDOWN LINKS & IMAGES -->\\n<!-- https://www.markdownguide.org/basic-syntax/#reference-style-links -->\\n\\n[contributors-shield]: https://img.shields.io/github/contributors/OpenBB-finance/OpenBBTerminal.svg?style=for-the-badge\\n[contributors-url]: https://github.com/OpenBB-finance/OpenBBTerminal/graphs/contributors\\n[forks-shield]: https://img.shields.io/github/forks/OpenBB-finance/OpenBBTerminal.svg?style=for-the-badge\\n[forks-url]: https://github.com/OpenBB-finance/OpenBBTerminal/network/members\\n[stars-shield]: https://img.shields.io/github/stars/OpenBB-finance/OpenBBTerminal.svg?style=for-the-badge\\n[stars-url]: https://github.com/OpenBB-finance/OpenBBTerminal/stargazers\\n[issues-shield]: https://img.shields.io/github/issues/OpenBB-finance/OpenBBTerminal.svg?style=for-the-badge&color=blue\\n[issues-url]: https://github.com/OpenBB-finance/OpenBBTerminal/issues\\n[bugs-open-shield]: https://img.shields.io/github/issues/OpenBB-finance/OpenBBTerminal/bug.svg?style=for-the-badge&color=yellow\\n[bugs-open-url]: https://github.com/OpenBB-finance/OpenBBTerminal/issues?q=is%3Aissue+label%3Abug+is%3Aopen\\n[bugs-closed-shield]: https://img.shields.io/github/issues-closed/OpenBB-finance/OpenBBTerminal/bug.svg?style=for-the-badge&color=success\\n[bugs-closed-url]: https://github.com/OpenBB-finance/OpenBBTerminal/issues?q=is%3Aissue+label%3Abug+is%3Aclosed\\n[license-shield]: https://img.shields.io/github/license/OpenBB-finance/OpenBBTerminal.svg?style=for-the-badge\\n[license-url]: https://github.com/OpenBB-finance/OpenBBTerminal/blob/main/LICENSE.txt\\n[linkedin-shield]: https://img.shields.io/badge/-LinkedIn-black.svg?style=for-the-badge&logo=linkedin&colorB=555\\n[linkedin-url]: https://linkedin.com/in/DidierRLopes\\n'},\n",
       " {'repo': './d8ahazard/sd_dreambooth_extension',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '# Dreambooth Extension for Stable-Diffusion-WebUI\\n\\nThis is a WIP port\\nof [Shivam Shriao\\'s Diffusers Repo](https://github.com/ShivamShrirao/diffusers/tree/main/examples/dreambooth), which is\\na modified version of the default [Huggingface Diffusers Repo](https://github.com/huggingface/diffusers) optimized for\\nbetter performance on lower-VRAM GPUs.\\n\\nIn addition, there are parts borrowed from [Koyha SS by BMaltais](https://github.com/bmaltais/kohya_ss).\\n\\nIt also adds several other features, including training multiple concepts simultaneously, and (Coming soon) Inpainting\\ntraining.\\n\\n## Installation\\n\\nTo install, simply go to the \"Extensions\" tab in the SD Web UI, select the \"Available\" sub-tab, pick \"Load from:\" to\\nload the list of extensions, and finally, click \"install\" next to the Dreambooth entry.\\n\\n![image](https://user-images.githubusercontent.com/1633844/200368737-7fe322de-00d6-4b28-a321-5e09f072d397.png)\\n\\nOnce installed, you **must** restart the Stable-Diffusion WebUI completely. Reloading the UI will not install the\\nnecessary requirements.\\n\\nWe also need a newer version of diffusers, as SD-WebUI uses version 0.3.0, while DB training requires >= 0.10.0. Not\\nhaving the right diffusers version is the cause of the \\'UNet2DConditionModel\\' object has no attribute \\'\\nenable_gradient_checkpointing\\' error message, as well as safety checker warnings.\\n\\n## IF YOU ARE HAVING ISSUES WITH REQUIREMENTS AFTER INSTALLING, LOOK BELOW\\n\\nTo force sd-web-ui to *only* install one set of requirements and resolve many issues on install, we can specify the\\ncommand line argument:\\n\\nset/export REQS_FILE=.\\\\extensions\\\\sd_dreambooth_extension\\\\requirements.txt\\n\\nRefer to the appropriate script below for extra flags to install requirements:\\n\\nhttps://github.com/d8ahazard/sd_dreambooth_extension/blob/main/webui-user-dreambooth.bat\\nhttps://github.com/d8ahazard/sd_dreambooth_extension/blob/main/webui-user-dreambooth.sh\\n\\nAnd last, if you wish to completely skip the \"native\" install routine of Dreambooth, you can set the following\\nenvironment flag:\\nDREAMBOOTH_SKIP_INSTALL=True\\n\\nThis is ideal for \"offline mode\", where you don\\'t want the script to constantly check things from pypi.\\n\\nAfter installing via the WebUI, it is recommended to set the above flags and re-launch the entire\\nStable-diffusion-webui, not just reload it.\\n\\n## Several Tutorial Videos For Dreambooth\\n\\n[![Zero To Hero Stable Diffusion DreamBooth Tutorial By Using Automatic1111 Web UI - Ultra Detailed](https://i.imgur.com/zzavDGW.png)](https://youtu.be/Bdl-jWR3Ukc) [![How to Inject Your Trained Subject e.g. Your Face Into Any Custom Stable Diffusion Model By Web UI](https://i.imgur.com/gPGWr3S.png)](https://youtu.be/s25hcW4zq4M)\\n[![8 GB LoRA Training - Fix CUDA Version For DreamBooth and Textual Inversion Training By Automatic1111](https://i.imgur.com/leOwdWy.png)](https://youtu.be/O01BrQwOd-Q) [![Automatic1111 Stable Diffusion DreamBooth Guide: Optimal Classification Images Count Comparison Test](https://i.imgur.com/kp10x2p.png)](https://youtu.be/Tb4IYIYm4os)\\n[![Epic Web UI DreamBooth Update - New Best Settings - 10 Stable Diffusion Training Compared on RunPods](https://i.imgur.com/i5FmRcu.png)](https://youtu.be/sRdtVanSRl4) [![How To Install New DREAMBOOTH & Torch 2 On Automatic1111 Web UI PC For Epic Performance Gains Guide](https://i.imgur.com/oFL7TyX.png)](https://youtu.be/pom3nQejaTs)\\n\\n## Usage\\n\\n### Create a Model\\n\\n1. Go to the Dreambooth tab.\\n\\n2. Under the \"Create Model\" sub-tab, enter a new model name and select the source checkpoint to train from.\\n   If you want to use a model from the HF Hub instead, specify the model URL and token. URL format should be \\'\\n   runwayml/stable-diffusion-v1-5\\'\\n\\n   The source checkpoint will be extracted to models\\\\dreambooth\\\\MODELNAME\\\\working.\\n\\n3. Click \"Create\". This will take a minute or two, but when done, the UI should indicate that a new model directory has\\n   been set up.\\n\\n## Various Top Buttons\\n\\n*Save Params* - Save current training parameters for the current model.\\n\\n*Load Params* - Load training parameters from the currently selected model. Use this to copy params from one model to\\nanother.\\n\\n*Generate Ckpt* - Generate a checkpoint from the currently saved weights at the current revision.\\n\\nGenerate Samples* - Click this while training to generate samples before the next interval.\\n\\n*Cancel* - Cancels training after the current step.\\n\\n*Train* - Starts training.\\n\\n## Model Section\\n\\n*Model* - The model to use. Training parameters will not be automatically loaded to the UI when changing models.\\n\\n*Lora Model* - An existing lora checkpoint to load if resuming training, or to merge with the base model if generating a\\ncheckpoint.\\n\\n*Half Model* - Enable this to save the model using half precision. Results in a smaller checkpoint with little\\nnoticeable difference in image output.\\n\\n*Save Checkpoint to Subdirectory* - Save the checkpoint to a subdirectory using the model name.\\n\\n## Training Parameters\\n\\n*Performance Wizard (WIP)* - Tries to set the optimal training parameters based on the amount of VRAM for your GPU and\\nnumber of instance images.\\n\\nProbably not perfect, but at least a good starting point.\\n\\n### Intervals\\n\\nThis section contains parameters related to when things happen during training.\\n\\n*Training Steps Per Image (Epochs)* - As the name would imply, an epoch is one training run over the entire set of\\ninstance images.\\nSo, if we want to train 100 steps per image, we can set this value to 100 and we\\'re ready to go. No math required.\\n\\n*Pause After N Epochs* - When set to a value higher than 0, training will pause for the time specified.\\n\\n*Amount of time to pause between Epochs, in Seconds* - How long to pause between \"N\" epochs when N is greater than zero,\\nin seconds.\\n\\n*Use Concepts* - Whether to use a JSON file or string with multiple concepts, or the individual settings below.\\n\\n*Save Model/Preview Frequency (Epochs)* - The save checkpoint and preview frequencies will be per epoch, not steps.\\n\\n### Batching\\n\\n*Batch size* - How many training steps to process simultaneously. You probably want to leave this at 1.\\n\\n*Gradient Accumulation Steps* - This should probably be set to the same value as the training batch size.\\n\\n*Class batch size* - How many classification images to generate simultaneously. Set this to whatever you can safely\\nprocess at once using Txt2Image, or just leave it alone.\\n\\n*Set Gradients to None When Zeroing* - instead of setting to zero, set the grads to None. This will in general have\\nlower memory footprint, and can modestly improve performance.\\nhttps://pytorch.org/docs/stable/generated/torch.optim.Optimizer.zero_grad.html\\n\\n*Gradient Checkpointing* - Enable this to save VRAM at the cost of a bit of speed.\\nhttps://arxiv.org/abs/1604.06174v2\\n\\n*Max Grad Norms* - The maximum number of gradient normalizati\\n\\n### Learning Rate\\n\\nThis section contains parameters related to the learning rate.\\n\\n*Learning rate* - The strength at which training impacts the new model. A higher learning rate requires less training\\nsteps, but can cause over-fitting more easily. Recommended between .000006 and .00000175\\n\\n*Scale Learning Rate* - Adjusts the learning rate over time.\\n\\n*Learning Rate Scheduler* - The scheduler used with the learning rate.\\n\\n*Learning Rate Warmup Steps* - How many steps to run before scaling the learning rate. I think.\\n\\n### Image Processing\\n\\nHere, you\\'ll find settings related to the handling of images.\\n\\n*Resolution* - The resolution your instance images are set to. This should probably be 512 or 768. Using a resolution\\nhigher than 512 will result in more vram usage.\\n\\n*Center Crop* - Enable this to automatically use \"dumb cropping\" when input images are larger than the specified\\nresolution.\\n\\n*Apply Horizontal Flip* - When enabled, instance images will be randomly flipped horizontally during training. This can\\nallow for better editability, but may require a larger number of training steps, as we\\'re effectively increasing our\\ndataset size.\\n\\n### Miscellaneous\\n\\nOther random stuff that doesn\\'t fit well into any other category.\\n\\n*Pretrained VAE Name or Path* - Enter the full path to an existing vae .bin file, and it will be used instead of the VAE\\nfrom the source checkpoint.\\n\\n*Use Concepts List* - Enable this to ignore the concepts tab and load training data from a JSON file instead.\\n\\n*Concepts List* - The path to a json file containing the concepts to train.\\n\\n## Advanced Settings\\n\\nHere you will find more performance-related settings. Changing these will likely impact the amount of VRAM required for\\ntraining.\\n\\n### Tuning\\n\\n*Use CPU Only* - As indicated, this is more of a last resort if you can\\'t get it to train with any other settings. Also,\\nas indicated, it will be abysmally slow.\\nAlso, you *cannot* use 8Bit-Adam with CPU Training, or you\\'ll have a bad time.\\n\\n*Use EMA* - Use estimated moving averages when training the unet. Purportedly, this is better for generating images, but\\nseems to have a minimal effect on training results. Uses more VRAM.\\n\\n*Mixed Precision* - When using 8bit AdamW, you *must* set this to fp16 or bf16. Bf16 precision is only supported by newer\\nGPUs, and enabled/disabled by default.\\n\\n*Memory Attention* - Type of attention to use. Choices are: \\'default\\': usually fastest, but use most VRAM; \\'xformers\\':\\nslower, uses less VRAM, can only be used with *Mixed Precision* = \\'fp16\\' (no impact on Apple Silicon); \\'flash_attention\\': slowest, requires lowest\\nVRAM.\\n\\n*Don\\'t Cache Latents* - Why is this not just called \"cache\" latents? Because that\\'s what the original script uses, and\\nI\\'m trying to maintain the ability to update this as easily as possible. Anyway...when this box is *checked* latents\\nwill not be cached. When latents are not cached, you will save a bit of VRAM, but train slightly slower.\\n\\n*Train Text Encoder* - Not required, but recommended. Requires more VRAM, may not work on <12 GB GPUs. Drastically\\nimproves output results.\\n\\n*Prior Loss Weight* - The weight to use when calculating prior loss. You probably want to leave this at 1.\\n\\n*Center Crop* - Crop images if they aren\\'t the right dimensions? I don\\'t use this, and I recommend you just crop your\\nimages \"right\".\\n\\n*Pad Tokens* - Pads the text tokens to a longer length for some reason.\\n\\n*Shuffle Tags* - Enable this to treat input prompts as a comma-separated list, and to shuffle that list, which can lead\\nto better editability.\\n\\n*Max Token Length* - raise the tokenizer\\'s default limit above 75. Requires Pad Tokens for > 75.\\n\\n*AdamW Weight Decay* - The weight decay of the AdamW Optimizer used for training. Values closer to 0 closely match your\\ntraining dataset, and values closer to 1 generalize more and deviate from your training dataset. Default is 1e-2, values\\nlower than 0.1 are recommended.\\n\\n## Concepts\\n\\nThe UI exposes three concepts, which seemed like a reasonable number of items to train on at once.\\n\\nIf you wish to use more than three concepts at once, you can ignore this section entirely, and instead use\\nthe \"Use Concepts List\" option from the Miscellaneous section under the Parameters tab.\\n\\nYou can refer to\\nthe [Example Concepts List](https://github.com/d8ahazard/sd_dreambooth_extension/blob/main/dreambooth/concepts_list.json)\\nfor a sample of the JSON format. You can theoretically use any number of concepts this way.\\n\\n### Concept Parameters\\n\\nBelow is a list of the various parameters that can be used to train a concept.\\n\\n*Maximum Training Steps* - The total number of lifetime training steps to train the concept until. Leave at -1 to use\\nthe global value.\\n\\n*Dataset Directory* - The directory in which the instance images are located.\\n\\n*Classification Dataset Directory* The directory in which class images are stored. Leave empty to save to model\\ndirectory.\\n\\n#### Filewords\\n\\nThe below values will be used in conjunction with the [filewords] tag in prompts to append/remove tags. See the\\n\\'Using [filewords]\\' section below for more information.\\n\\n*Instance Token* The unique identifier for your subject. (sks, xyz). Leave blank for fine-tuning.\\n\\n*Class Token* What your subject is. If a xyz is a person, this could be person/man/woman.\\n\\n### Prompts\\n\\n*Instance Prompt* - A prompt used for your instance images. Use [filewords] to insert or combine existing tags with\\ntokens.\\n\\n*Class Prompt* - A prompt used for generating and training class images. Use [filewords] to insert or combine existing\\ntags with tokens.\\n\\n*Classification Image Negative Prompt* - When generating class images, this is the negative prompt that will be used to\\nguide image generation.\\n\\n*Sample Image Prompt* - A prompt used when generating sample images. Use [filewords] to insert or combine existing tags\\nwith tokens.\\n\\n*Sample Prompt Template File* - An existing txt file used to generate sample images. [filewords] and [names] will be\\nreplaced with the instance token.\\n\\n*Sample Image Negative Prompt* - When generating sample images, this is the negative prompt that will be used to guide\\nimage generation.\\n\\n### Image Generation\\n\\n*Total Number of Class/Reg Images* - How many classification images will be generated. Leave at 0 to disable prior\\npreservation.\\n\\n*Classification/Sample CFG Scale* - The Classifier Free Guidance scale to use when generating images.\\n\\n*Classification/Sample Steps* - The number of steps to use when generating respective images.\\n\\n*Number of Samples to Generate* - How many sample images to generate.\\n\\n*Sample Seed* - A seed to use for consistent sample generation. Set to -1 to use a random seed.\\n\\n#### Using [filewords]\\n\\nEach concept allows you to use prompts from image filenames or accompanying text files for instance and class images.\\n\\nTo instruct the trainer to use prompts from existing files, use \\'[filewords]\\' (no quotes) for the instance/class/sample\\nprompts.\\n\\nIn order to properly insert and remove words from existing prompts, we need to let the trainer know what words indicate\\nwhat our subject name and class are.\\n\\nTo do this, we specify an instance and class token. If your subject were called \\'zxy\\' and it was a man,\\nthen your instance token would be \\'zxy\\', and your class token would be \\'man\\'.\\n\\nNow, when building your respective prompts, the subject and class can be inserted or removed as necessary.\\n\\n## Debugging\\n\\nHere\\'s a bunch of random stuff I added that seemed useful, but didn\\'t seem to fit anywhere else.\\n\\n*Preview Prompts* - Return a JSON string of the prompts that will be used for training. It\\'s not pretty, but you can\\ntell if things are going to work right.\\n\\n*Generate Sample Image* - Generate a sample using the specified seed and prompt below.\\n\\n*Sample Prompt* - What the sample should be.\\n\\n*Sample Seed* - The seed to use for your sample. Leave at -1 to use a random seed.\\n\\n*Train Imagic Only* - Imagic is basically dreambooth, but uses only one image and is significantly faster.\\n\\nIf using Imagic, the first image in the first concept\\'s Instance Data Dir will be used for training.\\n\\nSee here for more details:\\n\\nhttps://github.com/ShivamShrirao/diffusers/tree/main/examples/imagic\\n\\n### Continuing Training\\n\\nOnce a model has been trained for any number of steps, a config file is saved which contains all of the parameters from\\nthe UI.\\n\\nIf you wish to continue training a model, you can simply select the model name from the dropdown and then click the blue\\nbutton next to the model name dropdown to load previous parameters.\\n\\n![image](https://user-images.githubusercontent.com/1633844/200369076-8debef69-4b95-4341-83ac-cbbb02ee02f6.png)\\n\\n## Memory and Optimization\\n\\nAs this is based on ShivamShiaro\\'s repo, it should be able to run under the same GPU constraints, but is not guaranteed.\\n\\nPlease check out the [discussions](https://github.com/d8ahazard/sd_dreambooth_extension/discussions) page to find some\\npossible tips and tricks to help you get this running on your setup - or share what you\\'ve done to get it working.\\n\\n## Issues\\n\\nPlease be sure to use an issue template when asking for help. Some of the questions may be tedious, but I promise,\\nthey\\'ll help me help you faster.\\n\\n[Bug Report](https://github.com/d8ahazard/sd_dreambooth_extension/issues/new?assignees=&labels=&template=bug_report.md&title=)\\n\\n[Feature Request](https://github.com/d8ahazard/sd_dreambooth_extension/issues/new?assignees=&labels=&template=feature_request.md&title=)\\n\\n[Discord](https://discord.gg/q8dtpfRD5w)\\n\\n# Credits\\n\\n[Huggingface.co](https://huggingface.co) - All the things\\n\\n[CloneOfSimo](https://github.com/cloneofsimo/lora) - LORA\\n\\n[ShivamShrirao](https://github.com/ShivamShrirao/diffusers/tree/main/examples/dreambooth) - Multiple concepts,\\noptimizations.\\n\\n[Bmalthais](https://github.com/bmaltais/kohya_ss) - Optimizations, Features\\n\\n[Automatic1111](https://github.com/AUTOMATIC1111/stable-diffusion-webui) - Base app\\n'},\n",
       " {'repo': './catppuccin/gtk',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '<h3 align=\"center\">\\n\\t<img src=\"https://raw.githubusercontent.com/catppuccin/catppuccin/main/assets/logos/exports/1544x1544_circle.png\" width=\"100\" alt=\"Logo\"/><br/>\\n\\t<img src=\"https://raw.githubusercontent.com/catppuccin/catppuccin/main/assets/misc/transparent.png\" height=\"30\" width=\"0px\"/>\\n\\tCatppuccin for <a href=\"https://gtk.org/\">GTK</a>\\n\\t<img src=\"https://raw.githubusercontent.com/catppuccin/catppuccin/main/assets/misc/transparent.png\" height=\"30\" width=\"0px\"/>\\n</h3>\\n\\n<p align=\"center\">\\n    <a href=\"https://github.com/catppuccin/gtk/stargazers\"><img src=\"https://img.shields.io/github/stars/catppuccin/gtk?colorA=363a4f&colorB=b7bdf8&style=for-the-badge\"></a>\\n    <a href=\"https://github.com/catppuccin/gtk/issues\"><img src=\"https://img.shields.io/github/issues/catppuccin/gtk?colorA=363a4f&colorB=f5a97f&style=for-the-badge\"></a>\\n    <a href=\"https://github.com/catppuccin/gtk/contributors\"><img src=\"https://img.shields.io/github/contributors/catppuccin/gtk?colorA=363a4f&colorB=a6da95&style=for-the-badge\"></a>\\n</p>\\n\\n<p align=\"center\">\\n  <img src=\"assets/res.webp\"/>\\n</p>\\n\\n# About\\n\\nThis GTK theme is based on the [Colloid](https://github.com/vinceliuice/Colloid-gtk-theme) theme made by [Vinceliuice](https://github.com/vinceliuice)\\n\\n## Usage\\n\\n### Requirements\\n\\n-   GTK `>=3.20`\\n-   `gnome-themes-extra` (or `gnome-themes-standard`)\\n-   Murrine engine\\n\\n### Installation\\n\\n1. Download and extract the theme zip from [releases](https://github.com/catppuccin/gtk/releases/) or you can install the theme from the [AUR](#for-arch-linux-users).\\n2. Move the theme folder to **\".themes\"** in your home directory. **(~/.themes)** (Skip this step if you are using the AUR package)\\n3. Select the downloaded theme via your desktop specific tweaks application (**gnome-tweaks** on Gnome 3+).\\n\\n### For Arch Linux users\\n\\nWe have 4 AUR packages for all the 4 flavours of the theme:\\n- [Latte](https://aur.archlinux.org/packages/catppuccin-gtk-theme-latte)\\n- [Frappe](https://aur.archlinux.org/packages/catppuccin-gtk-theme-frappe)\\n- [Macchiato](https://aur.archlinux.org/packages/catppuccin-gtk-theme-macchiato)\\n- [Mocha](https://aur.archlinux.org/packages/catppuccin-gtk-theme-mocha)\\n\\nWith your favourite AUR helper, install them:\\n  ```bash\\n  yay -S catppuccin-gtk-theme-mocha catppuccin-gtk-theme-macchiato catppuccin-gtk-theme-frappe catppuccin-gtk-theme-latte\\n  ```\\n### For Nix users\\nThe [catppuccin-gtk](https://github.com/NixOS/nixpkgs/blob/master/pkgs/data/themes/catppuccin-gtk/default.nix) package in Nixpkgs allows you to specify the accents, size, tweaks and variant (flavour) of the theme by overriding the package.\\n\\nBy default, the variant is `frappe`, the accent is `blue`, the size is `standard`, and no tweaks are enabled. To change them, override the package. A list of valid choices are available in the package definition [here](https://github.com/NixOS/nixpkgs/blob/7ce8e7c4cf90492a631e96bcfe70724104914381/pkgs/data/themes/catppuccin-gtk/default.nix#L16).\\n\\nExample:\\n```nix\\npkgs.catppuccin-gtk.override {\\n  accents = [ \"pink\" ]; # You can specify multiple accents here to output multiple themes \\n  size = \"compact\";\\n  tweaks = [ \"rimless\" \"black\" ]; # You can also specify multiple tweaks here\\n  variant = \"macchiato\";\\n}\\n```\\n\\nTo use it in home-manager:\\n```nix\\n# home.nix\\n{\\n  pkgs,\\n  ...\\n}: {\\n  gtk = {\\n    enable = true;\\n    theme = {\\n      name = \"Catppuccin-Macchiato-Compact-Pink-Dark\";\\n      package = pkgs.catppuccin-gtk.override {\\n        accents = [ \"pink\" ];\\n        size = \"compact\";\\n        tweaks = [ \"rimless\" \"black\" ];\\n        variant = \"macchiato\";\\n      };\\n    };\\n  };\\n}\\n```\\n\\n### For gtk-4.0 users\\n\\nTo theme gtk-4.0 applications you have to manually symlink the `~/.config/gtk-4.0/` to the themes folder. Use the following commands\\n```bash\\nmkdir -p \"${HOME}/.config/gtk-4.0\"\\nln -sf \"${THEME_DIR}/gtk-4.0/assets\" \"${HOME}/.config/gtk-4.0/assets\"\\nln -sf \"${THEME_DIR}/gtk-4.0/gtk.css\" \"${HOME}/.config/gtk-4.0/gtk.css\"\\nln -sf \"${THEME_DIR}/gtk-4.0/gtk-dark.css\" \"${HOME}/.config/gtk-4.0/gtk-dark.css\"\\n```\\n\\n### For Flatpak users\\n\\n1. To give your Flatpaks access to your themes folder run:\\n  ```bash\\n  sudo flatpak override --filesystem=$HOME/.themes\\n  ```\\n2. To set the theme for all Flatpaks, replace `##theme##` with the name of the theme you want to use and run this command:\\n  ```bash\\n  sudo flatpak override --env=GTK_THEME=##theme##\\n  ```\\n3. For a more in depth tutorial see Hamza Algohary\\'s tutorial on [It\\'s Foss](https://itsfoss.com/flatpak-app-apply-theme/)\\n\\n### Using the script\\n\\n**Note**: Ensure that you have atleast python version 3.10 installed\\n\\nClone the repository using\\n```bash\\ngit clone --recurse-submodules git@github.com:catppuccin/gtk.git\\nvirtualenv -p python3 venv  # to be created only once and only if you need a virtual env\\nsource venv/bin/activate  \\npip install -r requirements.txt\\n```\\nTo check out the install script, run \\n```bash\\npython install.py --help\\n```\\n> Tip: `python install.py --help` allows the following options:\\n\\n```\\nCompulsory field        Specify color variant(s) [mocha|frappe|macchiato|latte|all]\\n-d, --dest DIR          Specify destination directory (Default: ~/.themes)\\n-n, --name NAME         Specify theme name (Default: Colloid)\\n-a, --accent VARIANT... Specify theme color variant(s) [rosewater|flamingo|pink|mauve|red|maroon|peach|yellow|green|teal|sky|\\n                        sapphire|blue|lavender|all] (Default: blue)\\n-s, --size VARIANT...   Specify size variant [standard|compact] (Default: standard variant)\\n-l, --link              Link installed gtk-4.0 theme to config folder for all libadwaita app use this theme\\n--zip                   Zips up the finally produced themes. \\n--tweaks                Specify versions for tweaks [black|rimless|normal]\\n                        1. black:    Blackness color version\\n                        2. rimless:  Remove the 1px border about windows and menus\\n                        3. normal:   Normal windows button style (titlebuttons: max/min/close)\\n-h, --help              Show help\\n```\\nYou can install any theme like the following example\\n```bash\\npython install.py mocha -a sky --tweaks rimless -d ~/.themes\\n\\n```\\nYou can build all possible variations of the theme possible using the following command and it will install it to releases folder\\n```bash\\npython install.py all -a all\\n```\\n\\n## Development\\n\\nYou need to install the following packages to build the theme. Check with your distribution for the package names in the repository\\n- `sassc`\\n- `inkscape`\\n- `optipng`\\n\\nA few important notes to keep in mind\\n\\n- `recolor.py` handles all changes that needs to be done to colloid to ensure it generated catppuccin colors. If vinceliuice changes anything in his theme in future, that is where you must change\\n- `var.py` includes all different variables that you can tinker around as per your personal requirements. \\n- `create_theme.py` consists of a wrapper that will recolor the colloid theme, install it as per the args provided and rename it accordingly. \\n \\n## 💝 Thanks to\\n\\n**Current maintainers**\\n- [npv12](https://github.com/npv12)\\n- [ghostx31](https://github.com/ghostx31)\\n- [Syndrizzle](https://github.com/Syndrizzle)\\n\\n**Contributions**\\n- [rubyowo](https://github.com/rubyowo) - for working on the build and CI script\\n\\n**Previous maintainer(s)**\\n- [sadrach-cl](https://github.com/sadrach-cl)\\n\\n&nbsp;\\n\\n<p align=\"center\"><img src=\"https://raw.githubusercontent.com/catppuccin/catppuccin/main/assets/footers/gray0_ctp_on_line.svg?sanitize=true\" /></p>\\n<p align=\"center\">Copyright &copy; 2021-present <a href=\"https://github.com/catppuccin\" target=\"_blank\">Catppuccin Org</a>\\n<p align=\"center\"><a href=\"https://github.com/catppuccin/gtk/blob/main/LICENSE\"><img src=\"https://img.shields.io/static/v1.svg?style=for-the-badge&label=License&message=GPLv3&logoColor=d9e0ee&colorA=363a4f&colorB=b7bdf8\"/></a></p>\\n'},\n",
       " {'repo': './tinyvision/SOLIDER',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '<div align=\"center\"><img src=\"assets/logo.png\" width=\"900\"></div>\\n\\n\\n[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/beyond-appearance-a-semantic-controllable/pedestrian-attribute-recognition-on-pa-100k)](https://paperswithcode.com/sota/pedestrian-attribute-recognition-on-pa-100k?p=beyond-appearance-a-semantic-controllable)\\n[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/beyond-appearance-a-semantic-controllable/person-re-identification-on-msmt17)](https://paperswithcode.com/sota/person-re-identification-on-msmt17?p=beyond-appearance-a-semantic-controllable)\\n[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/beyond-appearance-a-semantic-controllable/person-re-identification-on-market-1501)](https://paperswithcode.com/sota/person-re-identification-on-market-1501?p=beyond-appearance-a-semantic-controllable)\\n[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/beyond-appearance-a-semantic-controllable/person-search-on-cuhk-sysu)](https://paperswithcode.com/sota/person-search-on-cuhk-sysu?p=beyond-appearance-a-semantic-controllable)\\n[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/beyond-appearance-a-semantic-controllable/person-search-on-prw)](https://paperswithcode.com/sota/person-search-on-prw?p=beyond-appearance-a-semantic-controllable)\\n[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/beyond-appearance-a-semantic-controllable/pedestrian-detection-on-citypersons)](https://paperswithcode.com/sota/pedestrian-detection-on-citypersons?p=beyond-appearance-a-semantic-controllable)\\n[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/beyond-appearance-a-semantic-controllable/semantic-segmentation-on-lip-val)](https://paperswithcode.com/sota/semantic-segmentation-on-lip-val?p=beyond-appearance-a-semantic-controllable)\\n[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/beyond-appearance-a-semantic-controllable/pose-estimation-on-coco)](https://paperswithcode.com/sota/pose-estimation-on-coco?p=beyond-appearance-a-semantic-controllable)\\n\\nWelcome to **SOLIDER**! SOLIDER is a Semantic Controllable Self-Supervised Learning Framework to learn general human representations from massive unlabeled human images which can benefit downstream human-centric tasks to the maximum extent. Unlike the existing self-supervised learning methods, prior knowledge from human images is utilized in SOLIDER to build pseudo semantic labels and import more semantic information into the learned representation. Meanwhile, different downstream tasks always require different ratios of semantic information and appearance information, and a single learned representation cannot fit for all requirements. To solve this problem, SOLIDER introduces a conditional network with a semantic controller, which can fit different needs of downstream tasks. For more details, please refer to our paper [Beyond Appearance: a Semantic Controllable Self-Supervised Learning Framework for Human-Centric Visual Tasks](https://arxiv.org/abs/2303.17602).\\n\\n<div align=\"center\"><img src=\"assets/framework.png\" width=\"900\"></div>\\n\\n## Updates\\n- **[2023/04/24: Codes of attribute recognition task is released!] ![new](https://img.alicdn.com/imgextra/i4/O1CN01kUiDtl1HVxN6G56vN_!!6000000000764-2-tps-43-19.png)**\\n    * Training details of our pretrained model on downstream person attribute recognition task is released.\\n- **[2023/03/28: Codes of 3 downstream tasks are released!]**\\n    * Training details of our pretrained model on 3 downstream human visual tasks, including person re-identification, person search and pedestrian detection, are released.\\n- **[2023/03/13: SOLIDER is accepted by CVPR2023!]**\\n    * The paper of SOLIDER is accepted by CVPR2023, and its offical pytorch implementation is released in this repo. \\n\\n## Installation\\nThis codebase has been developed with python version 3.7, PyTorch version 1.7.1, CUDA 10.1 and torchvision 0.8.2.                                           \\n\\n## Datasets\\nWe use **LUPerson** as our training data, which consists of unlabeled human images. Download **LUPerson** from its [offical link](https://github.com/DengpanFu/LUPerson) and unzip it.\\n\\n## Training\\n- Choice 1. To train SOLIDER from scratch, please run:\\n```shell\\nsh run_solider.sh\\n```\\n\\n- Choice 2. Training SOLIDER from scratch may take a long time. To speed up the training, you can train a DINO model first, and then finetune it with SOLIDER, as follows:\\n```shell\\nsh run_dino.sh\\nsh resume_solider.sh\\n```\\n\\n## Finetuning and Inference\\nThere is a demo to run the trained SOLIDER model, which can be embedded into the inference or the downstream task finetuning.\\n```shell\\npython demo.py\\n```\\n\\n## Models\\nWe use [Swin-Transformer](https://github.com/microsoft/Swin-Transformer) as our backbone, which shows great advantages on many CV tasks.\\n| Task | Dataset | Swin Tiny<br>([Link](https://drive.google.com/file/d/12UyPVFmjoMVpQLHN07tNh4liHUmyDqg8/view?usp=share_link)) | Swin Small<br>([Link](https://drive.google.com/file/d/1oyEgASqDHc7YUPsQUMxuo2kBZyi2Tzfv/view?usp=share_link)) | Swin Base<br>([Link](https://drive.google.com/file/d/1uh7tO34tMf73MJfFqyFEGx42UBktTbZU/view?usp=share_link)) |\\n| :---: |:---: |:---: | :---: | :---: |\\n| Person Re-identification (mAP/R1)<br>w/o re-ranking | Market1501 | 91.6/96.1 | 93.3/96.6 | 93.9/96.9 |\\n|  | MSMT17 | 67.4/85.9 | 76.9/90.8 | 77.1/90.7 |\\n| Person Re-identification (mAP/R1)<br>with re-ranking | Market1501 | 95.3/96.6 | 95.4/96.4 | 95.6/96.7 |\\n|  | MSMT17 | 81.5/89.2 | 86.5/91.7 | 86.5/91.7 |\\n| Attribute Recognition (mA) | PETA_ZS | 74.37 | 76.21 | 76.43 |\\n|  | RAP_ZS | 74.23 | 75.95 | 76.42 |\\n|  | PA100K | 84.14 | 86.25 | 86.37 |\\n| Person Search (mAP/R1) | CUHK-SYSU | 94.9/95.7 | 95.5/95.8 | 94.9/95.5 |\\n|  | PRW | 56.8/86.8 | 59.8/86.7 | 59.7/86.8 |\\n| Pedestrian Detection (MR-2) | CityPersons | 10.3/40.8 | 10.0/39.2 | 9.7/39.4 |\\n| Human Parsing (mIOU) | LIP | 57.52 | 60.21 | 60.50 |\\n| Pose Estimation (AP/AR) | COCO | 74.4/79.6 | 76.3/81.3 | 76.6/81.5 |\\n\\n- All the models are trained on the whole LUPerson dataset.\\n\\n## Traning codes on Downstream Tasks\\n- [Person Re-identification](https://github.com/tinyvision/SOLIDER-REID)\\n- [Person Search](https://github.com/tinyvision/SOLIDER-PersonSearch)\\n- [Pedestrian Detection](https://github.com/tinyvision/SOLIDER-PedestrianDetection)\\n- [Person Attribute Recognition](https://github.com/tinyvision/SOLIDER-PersonAttributeRecognition)\\n- [Human Parsing](https://github.com/tinyvision/SOLIDER-HumanParsing)\\n- Pose Estimation\\n\\n## Acknowledgement\\nOur implementation is mainly based on the following codebases. We gratefully thank the authors for their wonderful works.\\n- [Swin-Transformer](https://github.com/microsoft/Swin-Transformer)\\n- [DINO](https://github.com/facebookresearch/dino)\\n- [TransReID](https://github.com/damo-cv/TransReID)\\n- [TransReID-SSL](https://github.com/damo-cv/TransReID-SSL)\\n- [SeqNet](https://github.com/serend1p1ty/SeqNet)\\n- [Pedestron](https://github.com/hasanirtiza/Pedestron)\\n- [LUPerson](https://github.com/DengpanFu/LUPerson)\\n\\n## Reference\\nIf you use SOLIDER in your research, please cite our work by using the following BibTeX entry:\\n```\\n@inproceedings{chen2023beyond,\\n  title={Beyond Appearance: a Semantic Controllable Self-Supervised Learning Framework for Human-Centric Visual Tasks},\\n  author={Weihua Chen and Xianzhe Xu and Jian Jia and Hao Luo and Yaohua Wang and Fan Wang and Rong Jin and Xiuyu Sun},\\n  booktitle={The IEEE/CVF Conference on Computer Vision and Pattern Recognition},\\n  year={2023},\\n}\\n'},\n",
       " {'repo': './raspberrypi/usbboot',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '# USB Device Boot Code\\n\\nThis is the USB MSD boot code which supports the Raspberry Pi 1A, 3A+, Compute Module, Compute\\nModule 3, 3+ and 4, Raspberry Pi Zero and Zero 2 W.\\n\\nThe default behaviour when run with no arguments is to boot the Raspberry Pi with\\nspecial firmware so that it emulates USB Mass Storage Device (MSD). The host OS\\nwill treat this as a normal USB mass storage device allowing the file system\\nto be accessed. If the storage has not been formatted yet (default for Compute Module)\\nthen the [Raspberry Pi Imager App](https://www.raspberrypi.com/software/) can be\\nused to install a new operating system.\\n\\nSince `RPIBOOT` is a generic firmware loading interface, it is possible to load\\nother versions of the firmware by passing the `-d` flag to specify the directory\\nwhere the firmware should be loaded from.\\nE.g. The firmware in the [msd](msd/README.md) can be replaced with newer/older versions.\\n\\nFor more information run `rpiboot -h`.\\n\\n## Building\\n\\n### Linux / Cygwin / WSL\\nClone this repository on your Pi or other Linux machine.\\nMake sure that the system date is set correctly, otherwise Git may produce an error.\\n\\n**This git repository uses symlinks. For Windows builds clone the repository under Cygwin**\\n\\n```\\nsudo apt install git libusb-1.0-0-dev pkg-config\\ngit clone --depth=1 https://github.com/raspberrypi/usbboot\\ncd usbboot\\nmake\\nsudo ./rpiboot\\n```\\n\\n`sudo` isn\\'t required if you have write permissions for the `/dev/bus/usb` device.\\n\\n### macOS\\nFrom a macOS machine, you can also run usbboot, just follow the same steps:\\n\\n1. Clone the `usbboot` repository\\n2. Install `libusb` (`brew install libusb`)\\n3. Install `pkg-config` (`brew install pkg-config`)\\n4. (Optional) Export the `PKG_CONFIG_PATH` so that it includes the directory enclosing `libusb-1.0.pc`\\n5. Build using make\\n6. Run the binary\\n\\n```\\ngit clone --depth=1 https://github.com/raspberrypi/usbboot\\ncd usbboot\\nbrew install libusb\\nbrew install pkg-config\\nmake\\nsudo ./rpiboot\\n```\\n\\nIf the build is unable to find the header file `libusb.h` then most likely the `PKG_CONFIG_PATH` is not set properly.\\nThis should be set via `export PKG_CONFIG_PATH=\"$(brew --prefix libusb)/lib/pkgconfig\"`.\\n\\nIf the build fails on an ARM-based Mac with a linker error such as `ld: warning: ignoring file /usr/local/Cellar/libusb/1.0.26/lib/libusb-1.0.dylib, building for macOS-arm64 but attempting to link with file built for macOS-x86_64` then you may need to build and install `libusb-1.0` yourself:\\n```\\n$ wget https://github.com/libusb/libusb/releases/download/v1.0.26/libusb-1.0.26.tar.bz2\\n$ tar -xf libusb-1.0.26.tar.bz2\\n$ cd libusb-1.0.26\\n$ ./configure\\n$ make\\n$ make check\\n$ sudo make install\\n```\\nRunning `make` again should now succeed.\\n\\n## Running\\n\\n### Compute Module 3\\nFit the `EMMC-DISABLE` jumper on the Compute Module IO board before powering on the board\\nor connecting the USB cable.\\n\\n### Compute Module 4\\nOn Compute Module 4 EMMC-DISABLE / nRPIBOOT (GPIO 40) must be fitted to switch the ROM to usbboot mode.\\nOtherwise, the SPI EEPROM bootloader image will be loaded instead.\\n\\n\\n<a name=\"extensions\"></a>\\n## Compute Module 4 Extensions\\nIn addition to the MSD functionality, there are a number of other utilities that can be loaded\\nvia RPIBOOT on Compute Module 4.\\n\\n| Directory | Description |\\n| ----------| ----------- |\\n| [recovery](recovery/README.md) | Updates the bootloader EEPROM on a Compute Module 4 |\\n| [rpi-imager-embedded](rpi-imager-embedded/README.md) | Runs the embedded version of Raspberry Pi Imager on the target device |\\n| [mass-storage-gadget](mass-storage-gadget/README.md) | Replacement for MSD firmware. Uses Linux USB gadgetfs drivers to export all block devices (e.g. NVMe, EMMC) as MSD devices |\\n| [secure-boot-recovery](secure-boot-recovery/README.md) | Scripts that extend the `recovery` process to enable secure-boot, sign images etc |\\n| [secure-boot-msd](secure-boot-msd/README.md) | Scripts for signing the MSD firmware so that it can be used on a secure-boot device |\\n| [secure-boot-example](secure-boot-example/README.md) | Simple Linux initrd with a UART console.\\n\\n**The `secure-boot-msd`, `rpi-imager-embedded` and `mass-storage-gadget` extensions require that the `2022-04-26` (or newer) bootloader EEPROM release has already been written to the EEPROM using `recovery.bin`**\\n\\n## Booting Linux\\nThe `RPIBOOT` protocol provides a virtual file system to the Raspberry Pi bootloader and GPU firmware. It\\'s therefore possible to\\nboot Linux. To do this, you will need to copy all of the files from a Raspberry Pi boot partition plus create your own\\ninitramfs.\\nOn Raspberry Pi 4 / CM4 the recommended approach is to use a `boot.img` which is a FAT disk image containing\\nthe minimal set of files required from the boot partition.\\n\\n## Troubleshooting\\n\\nThis section describes how to diagnose common `rpiboot` failures for Compute Modules. Whilst `rpiboot` is tested on every Compute Module during manufacture the system relies on multiple hardware and software elements. The aim of this guide is to make it easier to identify which component is failing.\\n\\n### Product Information Portal\\nThe [Product Information Portal](https://pip.raspberrypi.com/) contains the official documentation for hardware revision changes for Raspberry Pi computers.\\nPlease check this first to check that the software is up to date.\\n\\n### Hardware\\n* Inspect the Compute Module pins and connector for signs of damage and verify that the socket is free from debris.\\n* Check that the Compute Module is fully inserted.\\n* Check that `nRPIBOOT` / EMMC disable is pulled low BEFORE powering on the device.\\n   * On BCM2711, if the USB cable is disconected and the nRPIBOOT jumper is fitted then the green LED should be OFF. If the LED is on then the ROM is detecting that the GPIO for nRPIBOOT is high.\\n* Remove any hubs between the Compute Module and the host.\\n* Disconnect all other peripherals from the IO board.\\n* Verify that the red power LED switches on when the IO board is powered.\\n* Use another computer to verify that the USB cable for `rpiboot` can reliably transfer data. For example, connect it to a Raspberry Pi keyboard with other devices connected to the keyboard USB hub.\\n\\n#### Hardware - CM4\\n* The CM4 EEPROM supports MMC, USB-MSD, USB 2.0, Network and NVMe boot by default. Try booting to Linux from an alternate boot mode (e.g. network) to verify the `nRPIBOOT` GPIO can be pulled low and that the USB 2.0 interface is working.\\n* If `rpiboot` is running but the mass storage device does not appear then try running the `rpiboot -d mass-storage-gadget` because this uses Linux instead of a custom VPU firmware to implement the mass-storage gadget. This also provides a login console on UART and HDMI.\\n\\n### Software\\nThe recommended host setup is Raspberry Pi with Raspberry Pi OS. Alternatively, most Linux X86 builds are also suitable. Windows adds some extra complexity for the USB drivers so we recommend debugging on Linux first.\\n\\n* Update to the latest software release using `apt update rpiboot` or download and rebuild this repository from Github.\\n* Run `rpiboot -v | tee log` to capture verbose log output. N.B. This can be very verbose on some systems.\\n\\n#### Boot flow\\nThe `rpiboot` system runs in multiple stages. The ROM, bootcode.bin, the VPU firmware (start.elf) and for the `mass-storage-gadget` or `rpi-imager` a Linux initramfs. Each stage disconnects the USB device and presents a different USB descriptor. Each stage will appears as a new USB device connect in the `dmesg` log.\\n\\nSee also: [Raspberry Pi4 Boot Flow](https://www.raspberrypi.com/documentation/computers/raspberry-pi.html#raspberry-pi-4-boot-flow)\\n\\n#### bootcode.bin\\nBe careful not to overwrite `bootcode.bin` or `bootcode4.bin` with the executable from a different subdirectory. The `rpiboot` process simply looks for a file called `bootcode.bin` (or `bootcode4.bin` on BCM2711). However, the file in `recovery`/`secure-boot-recovery` directories is actually the `recovery.bin` EEPROM flashing tool.\\n\\n### Diagnostics\\n* Monitor the Linux `dmesg` output and verify that a BCM boot device is detected immediately after powering on the device. If not, please check the `hardware` section.\\n* Check the green activity LED. On Compute Module 4 this is activated by the software bootloader and should remain on. If not, then it\\'s likely that the initial USB transfer to the ROM failed.\\n* On Compute Module 4 connect a HDMI monitor for additional debug output. Flashing the EEPROM using `recovery.bin` will show a green screen and the `mass-storage-gadget` enables a console on the HDMI display.\\n* If `rpiboot` starts to download `bootcode4.bin` but the transfer fails then can indicate a cable issue OR a corrupted file. Check the hash of `bootcode.bin` file against this repository and check `dmesg` for USB error.\\n* If `bootcode.bin` or the `start.elf` detects an error then [error-code](https://www.raspberrypi.com/documentation/computers/configuration.html#led-warning-flash-codes) will be indicated by flashing the green activity LED.\\n* Add `uart_2ndstage=1` to the `config.txt` file in `msd/` or `recovery/` directories to enable UART debug output.\\n\\n<a name=\"secure-boot\"></a>\\n## Secure Boot\\nSecure Boot requires a recent bootloader stable image e.g. the version in this repository.\\n\\n### Tutorial\\nCreating a secure boot system from scratch can be quite complex. The [secure boot tutorial](secure-boot-example/README.md) uses a minimal example OS image to demonstrate how the Raspberry Pi-specific aspects of secure boot work.\\n\\n### Additional documentation\\n\\n* Secure boot [chain of trust diagram](docs/secure-boot-train-of-trust.pdf).\\n* Secure boot setup [configuration properties](https://www.raspberrypi.com/documentation/computers/raspberry-pi.html#secure-boot-configuration-properties-in-config-txt).\\n* Device tree [bootloader signed-boot property](https://www.raspberrypi.com/documentation/computers/configuration.html#bcm2711-specific-bootloader-properties-chosenbootloader).\\n* Device tree [public key - NVMEM property](https://www.raspberrypi.com/documentation/computers/configuration.html#nvmem-nodes).\\n* Raspberry Pi [OTP registers](https://www.raspberrypi.com/documentation/computers/raspberry-pi.html#otp-register-and-bit-definitions).\\n* Raspberry Pi [device specific private key](https://www.raspberrypi.com/documentation/computers/raspberry-pi.html#device-specific-private-key).\\n\\n### Host Setup\\nSecure boot require a 2048 bit RSA asymmetric keypair and the Python `pycrytodomex` module to sign the bootloader EEPROM config and boot image.\\n\\n#### Install Python Crypto Support (the pycryptodomex module)\\n```bash\\npython3 -m pip install pycryptodomex\\n# or\\npip install pycryptodomex\\n```\\n\\n#### Create an RSA key-pair using OpenSSL. Must be 2048 bits\\n```bash\\ncd $HOME\\nopenssl genrsa 2048 > private.pem\\n```\\n\\n### Secure Boot - configuration\\n* Please see the [secure boot EEPROM guide](secure-boot-recovery/README.md) to enable via rpiboot `recovery.bin`.\\n* Please see the [secure boot MSD guide](secure-boot-msd/README.md) for instructions about to mount the EMMC via USB mass-storage once secure-boot has been enabled.\\n\\n## Secure Boot - image creation\\nSecure Boot requires self-contained ramdisk (`boot.img`) FAT image to be created containing the GPU\\nfirmware, kernel and any other dependencies that would normally be loaded from the boot partition.\\n\\nThis plus a signature file (`boot.sig`) must be placed in the boot partition of the Raspberry Pi\\nor network download location.\\n\\nThe `boot.img` file should contain:-\\n* The kernel\\n* Device tree overlays\\n* GPU firmware (start.elf and fixup.dat)\\n* Linux initramfs containing the application OR scripts to mount/create an encrypted file-system.\\n\\n\\n### Disk encryption\\nSecure-boot is responsible for loading the Kernel + initramfs and loads all of the data\\nfrom a single `boot.img` file stored on an unencrypted FAT/EFI partition.\\n\\n**There is no support in the ROM or firmware for full-disk encryption.**\\n\\nIf a custom OS image needs to use an encrypted file-system then this would normally be implemented\\nvia scripts within the initramfs.\\n\\nRaspberry Pi computers do not have a secure enclave, however, it\\'s possible to store a 256 bit\\n[device specific private key](https://www.raspberrypi.com/documentation/computers/raspberry-pi.html#device-specific-private-key)\\nin OTP. The key is accessible to any process with access to `/dev/vcio` (`vcmailbox`), therefore, the\\nsecure-boot OS must ensure that access to this interface is restricted.\\n\\n**It is not possible to prevent code running in ARM supervisor mode (e.g. kernel code) from accessing OTP hardware directly**\\n\\nSee also:-\\n* [LUKS](https://en.wikipedia.org/wiki/Linux_Unified_Key_Setup)\\n* [cryptsetup FAQ](https://gitlab.com/cryptsetup/cryptsetup/-/wikis/FrequentlyAskedQuestions)\\n* [rpi-otp-private-key](../tools/rpi-otp-private-key)\\n\\nThe [secure boot tutorial](secure-boot-example/README.md) contains a `boot.img` that supports cryptsetup and a simple example.\\n\\n### Building `boot.img` using buildroot\\n\\nThe `secure-boot-example` directory contains a simple `boot.img` example with working HDMI,\\nnetwork, UART console and common tools in an initramfs.\\n\\nThis was generated from the [raspberrypi-signed-boot](https://github.com/raspberrypi/buildroot/blob/raspberrypi-signed-boot/README.md)\\nbuildroot config. Whilst not a generic fully featured configuration it should be relatively\\nstraightforward to cherry-pick the `raspberrypi-secure-boot` package and helper scripts into\\nother buildroot configurations.\\n\\n#### Minimum firmware version\\nThe firmware must be new enough to support secure boot. The latest firmware APT\\npackage supports secure boot. To download the firmware files directly.\\n\\n```bash\\ngit clone --depth 1 --branch stable https://github.com/raspberrypi/firmware\\n```\\n\\nTo check the version information within a `start4.elf` firmware file run\\n```bash\\nstrings start4.elf | grep VC_BUILD_\\n```\\n\\n#### Verifying the contents of a `boot.img` file\\nTo verify that the boot image has been created correctly use losetup to mount the .img file.\\n\\n```bash\\nsudo su\\nmkdir -p boot-mount\\nLOOP=$(losetup -f)\\nlosetup -f boot.img\\nmount ${LOOP} boot-mount/\\n\\n echo boot.img contains\\nfind boot-mount/\\n\\numount boot-mount\\nlosetup -d ${LOOP}\\nrmdir boot-mount\\n```\\n\\n#### Signing the boot image\\nFor secure-boot, `rpi-eeprom-digest` extends the current `.sig` format of\\nsha256 + timestamp to include an hex format RSA bit PKCS#1 v1.5 signature. The key length\\nmust be 2048 bits.\\n\\n```bash\\n../tools/rpi-eeprom-digest -i boot.img -o boot.sig -k \"${KEY_FILE}\"\\n```\\n\\nTo verify the signature of an existing image set the `PUBLIC_KEY_FILE` environment variable\\nto the path of the public key file in PEM format.\\n\\n```bash\\n../tools/rpi-eeprom-digest -i boot.img -k \"${PUBLIC_KEY_FILE}\" -v boot.sig\\n```\\n\\n\\n#### Hardware security modules\\n`rpi-eeprom-digest` is a shell script that wraps a call to `openssl dgst -sign`.\\nIf the private key is stored within a hardware security module instead of\\na .PEM file the `openssl` command will need to be replaced with the appropriate call to the HSM.\\n\\n`rpi-eeprom-digest` called by `update-pieeprom.sh` to sign the EEPROM config file.\\n\\nThe RSA public key must be stored within the EEPROM so that it can be used by the bootloader.\\nBy default, the RSA public key is automatically extracted from the private key PEM file. Alternatively,\\nthe public key may be specified separately via the `-p` argument to `update-pieeprom.sh` and `rpi-eeprom-config`.\\n\\nTo extract the public key in PEM format from a private key PEM file, run:\\n```bash\\nopenssl rsa -in private.pem -pubout -out public.pem\\n```\\n\\n#### Copy the secure boot image to the boot partition on the Raspberry Pi.\\nCopy `boot.img` and `boot.sig` to the boot filesystem.\\nSecure boot images can be loaded from any of the normal boot modes (e.g. SD, USB, Network).\\n'},\n",
       " {'repo': './sdatkinson/NeuralAmpModelerPlugin',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '# Neural Amp Modeler Plug-in\\n\\nA VST3/AudioUnit plug-in\\\\* for [Neural Amp Modeler](https://github.com/sdatkinson/neural-amp-modeler), built with [iPlug2](https://iplug2.github.io).\\n\\n- https://www.youtube.com/user/RunawayThumbtack\\n- https://github.com/sdatkinson/neural-amp-modeler\\n\\n## Installation\\n\\nCheck the [Releases](https://github.com/sdatkinson/NeuralAmpModelerPlugin/releases) for pre-built installers for the plugin!\\n\\n## Supported Platforms\\n\\nThe Neural Amp Modeler plugin currently supports Windows 10 (64bit) or later, and macOS 10.15 (Catalina) or later.\\n\\nFor Linux support, there is an LV2 plugin available: https://github.com/mikeoliphant/neural-amp-modeler-lv2.\\n\\n## About\\n\\nThis is a cleaned up version of [the original iPlug2-based NAM plugin](https://github.com/sdatkinson/iPlug2) with some refactoring to adopt better practices recommended by the developers of iPlug2.\\n(Thanks [Oli](https://github.com/olilarkin) for your generous suggestions!)\\n\\n\\\\*could also support VST2, AAX, CLAP, Linux, iOS soon.\\n'},\n",
       " {'repo': './Zero6992/chatGPT-discord-bot',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '# ChatGPT Discord Bot\\n\\n> ### Build your own Discord bot using ChatGPT\\n\\n---\\n> **Warning**\\n>\\n> #### 2023-04-12 Bing now supported\\n> #### 2023-04-01 Only Plus account can access Unofficial model\\n> #### 2023-03-27 Bard now supported\\n\\n### Chat\\n\\n![image](https://user-images.githubusercontent.com/89479282/206497774-47d960cd-1aeb-4fba-9af5-1f9d6ff41f00.gif)\\n\\n# Setup\\n\\n## Critical prerequisites to install\\n\\n* run ```pip3 install -r requirements.txt```\\n\\n* **Rename the file `.env.example` to `.env`**\\n\\n* Recommended python version `3.9` +\\n---\\n## Step 1: Create a Discord bot\\n\\n1. Go to https://discord.com/developers/applications create an application\\n2. Build a Discord bot under the application\\n3. Get the token from bot setting\\n\\n   ![image](https://user-images.githubusercontent.com/89479282/205949161-4b508c6d-19a7-49b6-b8ed-7525ddbef430.png)\\n4. Store the token to `.env` under the `DISCORD_BOT_TOKEN`\\n\\n   <img height=\"190\" width=\"390\" alt=\"image\" src=\"https://user-images.githubusercontent.com/89479282/222661803-a7537ca7-88ae-4e66-9bec-384f3e83e6bd.png\">\\n\\n5. Turn MESSAGE CONTENT INTENT `ON`\\n\\n   ![image](https://user-images.githubusercontent.com/89479282/205949323-4354bd7d-9bb9-4f4b-a87e-deb9933a89b5.png)\\n\\n6. Invite your bot to your server via OAuth2 URL Generator\\n\\n   ![image](https://user-images.githubusercontent.com/89479282/205949600-0c7ddb40-7e82-47a0-b59a-b089f929d177.png)\\n---\\n> **Note**\\n> \\n> In Step 2, you only need to complete the authentication process for the model you want to use (it\\'s not necessary to complete all Step 2)\\n> Remember to modify `CHAT_MODEL` to the default model you want to use in `.env` file \\n\\n## Step 2: Official API authentication\\n\\n### Geanerate an OpenAI API key\\n1. Go to https://beta.openai.com/account/api-keys\\n\\n2. Click Create new secret key\\n\\n   ![image](https://user-images.githubusercontent.com/89479282/207970699-2e0cb671-8636-4e27-b1f3-b75d6db9b57e.PNG)\\n\\n3. Store the SECRET KEY to `.env` under the `OPENAI_API_KEY`\\n\\n4. You\\'re all set for [Step 3](#step-3-run-the-bot-on-the-desktop)\\n---\\n## Step 2: Website ChatGPT authentication - 2 approaches\\n\\n> **Only Support ChatGPT Plus Account**\\n\\n### Email/Password approache (Not supported for Google/Microsoft accounts)\\n\\n1. Create an account on https://chat.openai.com/chat and open it\\n\\n2.  Open console with `F12`\\n3.  Open `Application` tab > Cookies\\n\\n   ![image](https://user-images.githubusercontent.com/89479282/229298001-41ab4f61-5b79-4c65-b08c-708ee6fe2304.png)\\n\\n4. Copy the value for `_puid` from cookies and paste it into `.env` under `PUID`\\n\\n5. Save your email into `.env` under `OPENAI_EMAIL`\\n\\n6. Save your password into `.env` under `OPENAI_PASSWORD`\\n\\n7. You\\'re all set for [Step 3](#step-3-run-the-bot-on-the-desktop)\\n\\n### ACCESS token approache\\n1. Open https://chat.openai.com/api/auth/session\\n\\n2. Open console with `F12`\\n\\n3. Open `Application` tab > Cookies\\n\\n   ![image](https://user-images.githubusercontent.com/89479282/229298001-41ab4f61-5b79-4c65-b08c-708ee6fe2304.png)\\n\\n4. Copy the value for `_puid` from cookies and paste it into `.env` under `PUID`\\n\\n\\n5. Copy the value for `accessToken` from cookies and paste it into `.env` under `ACCESS_TOKEN`\\n\\n6. You\\'re all set for [Step 3](#step-3-run-the-bot-on-the-desktop)\\n---\\n## Step 2: Google Bard authentication\\n1. Go to https://bard.google.com/\\n\\n2. Open console with `F12`\\n\\n3. Open `Application` tab > Cookies\\n\\n4. Copy the value for `__Secure-1PSID` from cookies and paste it into `.env` under `BARD_SESSION_ID`\\n\\n5. You\\'re all set for [Step 3](#step-3-run-the-bot-on-the-desktop)\\n---\\n## Step 2: Microsoft Bing authentication\\n1. **Rename the file `cookies.dev.json` to `cookies.json`**\\n\\n2. Go to https://bing.com/chat and log in your Microsoft account\\n\\n3. Use Cookie Editor or similar extensions to export the cookies\\n\\n3. Paste it into `cookies.json`\\n\\n5. You\\'re all set for [Step 3](#step-3-run-the-bot-on-the-desktop)\\n---\\n## Step 3: Run the bot on the desktop\\n\\n1. Open a terminal or command prompt\\n\\n2. Navigate to the directory where you installed the ChatGPT Discord bot\\n\\n3. Run `python3 main.py` or `python main.py` to start the bot\\n---\\n## Step 3: Run the bot with Docker\\n\\n1. Build the Docker image & Run the Docker container `docker compose up -d`\\n\\n2. Inspect whether the bot works well `docker logs -t chatgpt-discord-bot`\\n\\n   ### Stop the bot:\\n\\n   * `docker ps` to see the list of running services\\n   * `docker stop <BOT CONTAINER ID>` to stop the running bot\\n\\n### Have a good chat!\\n---\\n## Optional: Disable logging\\n\\n* Set the value of `LOGGING` in the `.env` to False\\n## Optional: Setup system prompt\\n\\n* A system prompt would be invoked when the bot is first started or reset\\n* You can set it up by modifying the content in `system_prompt.txt`\\n* All the text in the file will be fired as a prompt to the bot\\n* Get the first message from ChatGPT in your discord channel!\\n* Go Discord setting turn `developer mode` on\\n\\n   1. Right-click the channel you want to recieve the message, `Copy  ID`\\n\\n        ![channel-id](https://user-images.githubusercontent.com/89479282/207697217-e03357b3-3b3d-44d0-b880-163217ed4a49.PNG)\\n\\n   2. paste it into `.env` under `DISCORD_CHANNEL_ID`\\n\\n------\\n>  [**中文設置教學**](https://zero6992.github.io/2023/03/09/chatGPT-discord-bot-chinese/)\\n------\\n## Commands\\n\\n* `/chat [message]` Chat with ChatGPT!\\n* `/draw [prompt]` Generate an image with the Dalle2 model\\n* `/switchpersona [persona]` Switch between optional chatGPT jailbreaks\\n   * `random`: Picks a random persona\\n   * `chatGPT`: Standard chatGPT mode\\n   * `dan`: Dan Mode 11.0, infamous Do Anything Now Mode\\n   * `sda`: Superior DAN has even more freedom in DAN Mode\\n   * `confidant`: Evil Confidant, evil trusted confidant\\n   * `based`: BasedGPT v2, sexy gpt\\n   * `oppo`: OPPO says exact opposite of what chatGPT would say\\n   * `dev`: Developer Mode, v2 Developer mode enabled\\n\\n* `/private` ChatGPT switch to private mode\\n* `/public` ChatGPT switch to public mode\\n* `/replyall` ChatGPT switch between replyAll mode and default mode\\n* `/reset` Clear ChatGPT conversation history\\n* `/chat-model` Switch different chat model\\n   * `OFFICIAL-GPT-3.5`: GPT-3.5 model\\n   * `OFFICIAL-GPT-4.0`: GPT-4.0 model (make sure your account can access gpt-4 model)\\n   * `Website ChatGPT-3.5`: Website ChatGPT-3.5 model (UNOFFICIAL)\\n   * `Website ChatGPT-4.0`: Website ChatGPT-4.0 model (UNOFFICIAL)(available if you got a plus account)\\n   * `Bard`: Google Bard Model\\n   * `Bing`: Microsoft Bing Model\\n### Special Features\\n\\n#### Draw\\n\\n![image](https://user-images.githubusercontent.com/91911303/223772051-13f840d5-99ef-4762-98d2-d15ce23cbbd5.png)\\n\\n#### Switch Persona\\n\\n> **Warning**\\n>\\n> Using certain personas may generate vulgar or disturbing content. Use at your own risk.\\n\\n![image](https://user-images.githubusercontent.com/91911303/223772334-7aece61f-ead7-4119-bcd4-7274979c4702.png)\\n\\n\\n#### Mode\\n\\n* `public mode (default)`  the bot directly reply on the channel\\n\\n  ![image](https://user-images.githubusercontent.com/89479282/206565977-d7c5d405-fdb4-4202-bbdd-715b7c8e8415.gif)\\n\\n* `private mode` the bot\\'s reply can only be seen by the person who used the command\\n\\n  ![image](https://user-images.githubusercontent.com/89479282/206565873-b181e600-e793-4a94-a978-47f806b986da.gif)\\n\\n* `replyall mode` the bot will reply to all messages in the channel without using slash commands (`/chat` will also be unavailable)\\n\\n   > **Warning**\\n   > The bot will easily be triggered in `replyall` mode, which could cause program failures\\n ---\\n'},\n",
       " {'repo': './pittcsc/Summer2023-Internships',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '\\r\\n# Summer 2024 Tech Internships by Pitt CSC 🌆🐢\\r\\nAnd we\\'re back! Use this repo to share and keep track of software, tech, CS, PM, quant internships for **Summer 2024**. List maintained by [the Pitt Computer Science Club](https://pittcsc.org/)!\\r\\n\\r\\n:warning: **This repository is only for internships/co-ops in the United States, Canada or for Remote positions :earth_americas:.**\\r\\n\\r\\n🧠 For tips on the internship process check out the [Zero to Offer](https://www.pittcs.wiki/zero-to-offer) 🧠\\r\\n\\r\\n🙏 **Contribute by submitting a [pull request](https://github.com/susam/gitpr#create-pull-request)! See the contribution guidelines [here](https://github.com/pittcsc/Summer2023-Internships/blob/dev/CONTRIBUTING.md)!** 🙏\\r\\n\\r\\n---\\r\\n<div align=\"center\">\\r\\n\\t<p>\\r\\n\\t\\t<a href=\"https://simplify.jobs/?utm_source=pittcsc&utm_medium=internships_repo\">\\r\\n\\t\\t\\t<b>Applying to internships?</b>\\r\\n\\t\\t\\t<br>\\r\\n\\t\\t\\tAutofill all your applications in a single click.\\r\\n\\t\\t\\t<br>\\r\\n\\t\\t\\t<div>\\r\\n\\t\\t\\t\\t<a href=\"https://simplify.jobs/?utm_source=pittcsc&utm_medium=internships_repo\"><img src=\"https://res.cloudinary.com/dpeo4xcnc/image/upload/v1636594918/simplify_pittcsc.png\" width=\"450\" alt=\"Simplify\" ></a>\\r\\n\\t\\t\\t</div>\\r\\n\\t\\t</a>\\r\\n\\t\\t<sub><i>Stop manually re-entering your information. Simplify’s extension helps you autofill internship applications on millions of sites.</i></sub>\\r\\n\\t</p>\\r\\n</div>\\r\\n\\r\\n<div align=\"center\">\\r\\n\\t<h3>\\r\\n\\t\\tThanks for a great three years 💖💖\\r\\n\\t</h3>\\r\\n\\t<p>\\r\\n\\t\\t<img src=\"https://api.star-history.com/svg?repos=pittcsc/Summer2023-Internships&type=Date\" width=\"500\"  alt=\"Star History\">\\r\\n\\t</p>\\r\\n</div>\\r\\n\\r\\n---\\r\\n\\r\\n## The List 🚴🏔\\r\\n> **Note**:\\r\\n> This README file is for **2024 internships only**. For 2023 internships, please [click here](https://github.com/pittcsc/Summer2023-Internships/blob/dev/README.md).\\r\\n\\r\\n[⬇️ Jump to bottom ⬇️](https://github.com/pittcsc/Summer2023-Internships#we-love-our-contributors-%EF%B8%8F%EF%B8%8F)\\r\\n<!-- Please leave a one line gap between this and the table -->\\r\\n\\r\\n| Name | Location | Notes |\\r\\n| ---- | -------- | ----- |\\r\\n| [Goldman Sachs](https://www.goldmansachs.com/careers/students/programs/americas/summer-analyst-program.html) | Global | Summer 2024 Analyst |\\r\\n| [KPMG](https://www.kpmguscareers.com/jobdetail/?jobId=98001) | Multiple Locations | **🔒 Closed 🔒** Summer 2024 Engineering & IT Internship (No sponsorship is available) |\\r\\n| [Optiver](https://optiver.com/working-at-optiver/career-opportunities/) | Chicago, Austin | [2024 Tech Graduate & Intern Expression of Interest](https://optiver.com/working-at-optiver/career-opportunities/6497784002) <br/> [2024 Trading Graduate & Intern Expression of Interest](https://optiver.com/working-at-optiver/career-opportunities/6614387002) |\\r\\n| [Bridgewater Associates](https://boards.greenhouse.io/bridgewater89/jobs/6570837002) | Westport | Investment Engineer Intern |\\r\\n\\r\\n<!-- Please leave a one line gap between this and the table -->\\r\\n[⬆️ Back to Top ⬆️](https://github.com/pittcsc/Summer2023-Internships#the-list-)\\r\\n\\r\\n## We love our contributors ❤️❤️\\r\\nMake a [pull request](https://github.com/susam/gitpr#create-pull-request) to help contribute.\\r\\n<a href=\"https://github.com/pittcsc/Summer2023-Internships/graphs/contributors\">\\r\\n  <img src=\"https://contrib.rocks/image?repo=pittcsc/Summer2023-Internships&columns=24&max=480\" />\\r\\n</a>\\r\\n*Made with [contrib.rocks](https://contrib.rocks).*\\r\\n'},\n",
       " {'repo': './elebumm/RedditVideoMakerBot',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '# Reddit Video Maker Bot 🎥\\n\\nAll done WITHOUT video editing or asset compiling. Just pure ✨programming magic✨.\\n\\nCreated by Lewis Menelaws & [TMRRW](https://tmrrwinc.ca)\\n\\n<a target=\"_blank\" href=\"https://tmrrwinc.ca\">\\n<picture>\\n  <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://user-images.githubusercontent.com/6053155/170528535-e274dc0b-7972-4b27-af22-637f8c370133.png\">\\n  <source media=\"(prefers-color-scheme: light)\" srcset=\"https://user-images.githubusercontent.com/6053155/170528582-cb6671e7-5a2f-4bd4-a048-0e6cfa54f0f7.png\">\\n  <img src=\"https://user-images.githubusercontent.com/6053155/170528582-cb6671e7-5a2f-4bd4-a048-0e6cfa54f0f7.png\" width=\"350\">\\n</picture>\\n\\n</a>\\n\\n## Video Explainer\\n\\n[![lewisthumbnail](https://user-images.githubusercontent.com/6053155/173631669-1d1b14ad-c478-4010-b57d-d79592a789f2.png)\\n](https://www.youtube.com/watch?v=3gjcY_00U1w)\\n\\n## Motivation 🤔\\n\\nThese videos on TikTok, YouTube and Instagram get MILLIONS of views across all platforms and require very little effort.\\nThe only original thing being done is the editing and gathering of all materials...\\n\\n... but what if we can automate that process? 🤔\\n\\n## Disclaimers 🚨\\n\\n- **At the moment**, this repository won\\'t attempt to upload this content through this bot. It will give you a file that\\n  you will then have to upload manually. This is for the sake of avoiding any sort of community guideline issues.\\n\\n## Requirements\\n\\n- Python 3.10\\n- Playwright (this should install automatically in installation)\\n\\n## Installation 👩\\u200d💻\\n\\n1. Clone this repository\\n2. Run `pip install -r requirements.txt`\\n3. Run `python -m playwright install` and `python -m playwright install-deps`\\n\\n**EXPERIMENTAL!!!!**\\n\\nOn macOS and Linux (debian, arch, fedora and centos, and based on those), you can run an install script that will automatically install steps 1 to 3. (requires bash)\\n\\n`bash <(curl -sL https://raw.githubusercontent.com/elebumm/RedditVideoMakerBot/master/install.sh)`\\n\\nThis can also be used to update the installation\\n\\n4. Run `python main.py`\\n5. Visit [the Reddit Apps page.](https://www.reddit.com/prefs/apps), and set up an app that is a \"script\". Paste any URL in redirect URL. Ex:google.com\\n6. The bot will ask you to fill in your details to connect to the Reddit API, and configure the bot to your liking\\n7. Enjoy 😎\\n8. If you need to reconfigure the bot, simply open the `config.toml` file and delete the lines that need to be changed. On the next run of the bot, it will help you reconfigure those options.\\n\\n(Note if you got an error installing or running the bot try first rerunning the command with a three after the name e.g. python3 or pip3)\\n\\nIf you want to read more detailed guide about the bot, please refer to the [documentation](https://reddit-video-maker-bot.netlify.app/)\\n\\n## Video\\n\\nhttps://user-images.githubusercontent.com/66544866/173453972-6526e4e6-c6ef-41c5-ab40-5d275e724e7c.mp4\\n\\n## Contributing & Ways to improve 📈\\n\\nIn its current state, this bot does exactly what it needs to do. However, improvements can always be made!\\n\\nI have tried to simplify the code so anyone can read it and start contributing at any skill level. Don\\'t be shy :) contribute!\\n\\n- [ ] Creating better documentation and adding a command line interface.\\n- [ ] Allowing the user to choose background music for their videos.\\n- [x] Allowing users to choose a reddit thread instead of being randomized.\\n- [x] Allowing users to choose a background that is picked instead of the Minecraft one.\\n- [x] Allowing users to choose between any subreddit.\\n- [x] Allowing users to change voice.\\n- [x] Checks if a video has already been created\\n- [x] Light and Dark modes\\n- [x] NSFW post filter\\n\\nPlease read our [contributing guidelines](CONTRIBUTING.md) for more detailed information.\\n\\n### For any questions or support join the [Discord](https://discord.gg/Vkanmh6C8V) server\\n\\n## Developers and maintainers.\\n\\nElebumm (Lewis#6305) - https://github.com/elebumm (Founder)\\n\\nJason (JasonLovesDoggo#1904) - https://github.com/JasonLovesDoggo (Maintainer)\\n\\nSimon (OpenSourceSimon) - https://github.com/OpenSourceSimon\\n\\nCallumIO (c.#6837) - https://github.com/CallumIO\\n\\nVerq (Verq#2338) - https://github.com/CordlessCoder\\n\\nLukaHietala (Pix.#0001) - https://github.com/LukaHietala\\n\\nFreebiell (Freebie#3263) - https://github.com/FreebieII\\n\\nAman Raza (electro199#8130) - https://github.com/electro199\\n\\n\\n## LICENSE\\n[Roboto Fonts](https://fonts.google.com/specimen/Roboto/about) are licensed under [Apache License V2](https://www.apache.org/licenses/LICENSE-2.0)\\n'},\n",
       " {'repo': './openai/whisper',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '# Whisper\\n\\n[[Blog]](https://openai.com/blog/whisper)\\n[[Paper]](https://arxiv.org/abs/2212.04356)\\n[[Model card]](https://github.com/openai/whisper/blob/main/model-card.md)\\n[[Colab example]](https://colab.research.google.com/github/openai/whisper/blob/master/notebooks/LibriSpeech.ipynb)\\n\\nWhisper is a general-purpose speech recognition model. It is trained on a large dataset of diverse audio and is also a multitasking model that can perform multilingual speech recognition, speech translation, and language identification.\\n\\n\\n## Approach\\n\\n![Approach](https://raw.githubusercontent.com/openai/whisper/main/approach.png)\\n\\nA Transformer sequence-to-sequence model is trained on various speech processing tasks, including multilingual speech recognition, speech translation, spoken language identification, and voice activity detection. These tasks are jointly represented as a sequence of tokens to be predicted by the decoder, allowing a single model to replace many stages of a traditional speech-processing pipeline. The multitask training format uses a set of special tokens that serve as task specifiers or classification targets.\\n\\n\\n## Setup\\n\\nWe used Python 3.9.9 and [PyTorch](https://pytorch.org/) 1.10.1 to train and test our models, but the codebase is expected to be compatible with Python 3.8-3.11 and recent PyTorch versions. The codebase also depends on a few Python packages, most notably [OpenAI\\'s tiktoken](https://github.com/openai/tiktoken) for their fast tokenizer implementation. You can download and install (or update to) the latest release of Whisper with the following command:\\n\\n    pip install -U openai-whisper\\n\\nAlternatively, the following command will pull and install the latest commit from this repository, along with its Python dependencies:\\n\\n    pip install git+https://github.com/openai/whisper.git \\n\\nTo update the package to the latest version of this repository, please run:\\n\\n    pip install --upgrade --no-deps --force-reinstall git+https://github.com/openai/whisper.git\\n\\nIt also requires the command-line tool [`ffmpeg`](https://ffmpeg.org/) to be installed on your system, which is available from most package managers:\\n\\n```bash\\n# on Ubuntu or Debian\\nsudo apt update && sudo apt install ffmpeg\\n\\n# on Arch Linux\\nsudo pacman -S ffmpeg\\n\\n# on MacOS using Homebrew (https://brew.sh/)\\nbrew install ffmpeg\\n\\n# on Windows using Chocolatey (https://chocolatey.org/)\\nchoco install ffmpeg\\n\\n# on Windows using Scoop (https://scoop.sh/)\\nscoop install ffmpeg\\n```\\n\\nYou may need [`rust`](http://rust-lang.org) installed as well, in case [tiktoken](https://github.com/openai/tiktoken) does not provide a pre-built wheel for your platform. If you see installation errors during the `pip install` command above, please follow the [Getting started page](https://www.rust-lang.org/learn/get-started) to install Rust development environment. Additionally, you may need to configure the `PATH` environment variable, e.g. `export PATH=\"$HOME/.cargo/bin:$PATH\"`. If the installation fails with `No module named \\'setuptools_rust\\'`, you need to install `setuptools_rust`, e.g. by running:\\n\\n```bash\\npip install setuptools-rust\\n```\\n\\n\\n## Available models and languages\\n\\nThere are five model sizes, four with English-only versions, offering speed and accuracy tradeoffs. Below are the names of the available models and their approximate memory requirements and relative speed. \\n\\n\\n|  Size  | Parameters | English-only model | Multilingual model | Required VRAM | Relative speed |\\n|:------:|:----------:|:------------------:|:------------------:|:-------------:|:--------------:|\\n|  tiny  |    39 M    |     `tiny.en`      |       `tiny`       |     ~1 GB     |      ~32x      |\\n|  base  |    74 M    |     `base.en`      |       `base`       |     ~1 GB     |      ~16x      |\\n| small  |   244 M    |     `small.en`     |      `small`       |     ~2 GB     |      ~6x       |\\n| medium |   769 M    |    `medium.en`     |      `medium`      |     ~5 GB     |      ~2x       |\\n| large  |   1550 M   |        N/A         |      `large`       |    ~10 GB     |       1x       |\\n\\nThe `.en` models for English-only applications tend to perform better, especially for the `tiny.en` and `base.en` models. We observed that the difference becomes less significant for the `small.en` and `medium.en` models.\\n\\nWhisper\\'s performance varies widely depending on the language. The figure below shows a WER (Word Error Rate) breakdown by languages of the Fleurs dataset using the `large-v2` model (The smaller the numbers, the better the performance). Additional WER scores corresponding to the other models and datasets can be found in Appendix D.1, D.2, and D.4. Meanwhile, more BLEU (Bilingual Evaluation Understudy) scores can be found in Appendix D.3. Both are found in [the paper](https://arxiv.org/abs/2212.04356). \\n\\n![WER breakdown by language](https://raw.githubusercontent.com/openai/whisper/main/language-breakdown.svg)\\n\\n\\n\\n## Command-line usage\\n\\nThe following command will transcribe speech in audio files, using the `medium` model:\\n\\n    whisper audio.flac audio.mp3 audio.wav --model medium\\n\\nThe default setting (which selects the `small` model) works well for transcribing English. To transcribe an audio file containing non-English speech, you can specify the language using the `--language` option:\\n\\n    whisper japanese.wav --language Japanese\\n\\nAdding `--task translate` will translate the speech into English:\\n\\n    whisper japanese.wav --language Japanese --task translate\\n\\nRun the following to view all available options:\\n\\n    whisper --help\\n\\nSee [tokenizer.py](https://github.com/openai/whisper/blob/main/whisper/tokenizer.py) for the list of all available languages.\\n\\n\\n## Python usage\\n\\nTranscription can also be performed within Python: \\n\\n```python\\nimport whisper\\n\\nmodel = whisper.load_model(\"base\")\\nresult = model.transcribe(\"audio.mp3\")\\nprint(result[\"text\"])\\n```\\n\\nInternally, the `transcribe()` method reads the entire file and processes the audio with a sliding 30-second window, performing autoregressive sequence-to-sequence predictions on each window.\\n\\nBelow is an example usage of `whisper.detect_language()` and `whisper.decode()` which provide lower-level access to the model.\\n\\n```python\\nimport whisper\\n\\nmodel = whisper.load_model(\"base\")\\n\\n# load audio and pad/trim it to fit 30 seconds\\naudio = whisper.load_audio(\"audio.mp3\")\\naudio = whisper.pad_or_trim(audio)\\n\\n# make log-Mel spectrogram and move to the same device as the model\\nmel = whisper.log_mel_spectrogram(audio).to(model.device)\\n\\n# detect the spoken language\\n_, probs = model.detect_language(mel)\\nprint(f\"Detected language: {max(probs, key=probs.get)}\")\\n\\n# decode the audio\\noptions = whisper.DecodingOptions()\\nresult = whisper.decode(model, mel, options)\\n\\n# print the recognized text\\nprint(result.text)\\n```\\n\\n## More examples\\n\\nPlease use the [🙌 Show and tell](https://github.com/openai/whisper/discussions/categories/show-and-tell) category in Discussions for sharing more example usages of Whisper and third-party extensions such as web demos, integrations with other tools, ports for different platforms, etc.\\n\\n\\n## License\\n\\nWhisper\\'s code and model weights are released under the MIT License. See [LICENSE](https://github.com/openai/whisper/blob/main/LICENSE) for further details.\\n'},\n",
       " {'repo': './InstaPy/InstaPy',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '<p align=\"center\">\\n  <img src=\"https://i.imgur.com/sJzfZsL.jpg\" width=\"154\">\\n  <h1 align=\"center\">InstaPy</h1>\\n  <p align=\"center\">Tooling that <b>automates</b> your social media interactions to “farm” Likes, Comments, and Followers on Instagram\\nImplemented in Python using the Selenium module.<p>\\n  <p align=\"center\">\\n    <a href=\"https://github.com/timgrossmann/InstaPy/blob/master/LICENSE\">\\n      <img src=\"https://img.shields.io/badge/license-GPLv3-blue.svg\" />\\n    </a>\\n    <a href=\"https://github.com/SeleniumHQ/selenium\">\\n      <img src=\"https://img.shields.io/badge/built%20with-Selenium-yellow.svg\" />\\n    </a>\\n    <a href=\"https://www.python.org/\">\\n    \\t<img src=\"https://img.shields.io/badge/built%20with-Python3-red.svg\" />\\n    </a>\\n    <a href=\"https://www.github.com/timgrossmann/InstaPy#backer\">\\n\\t<img src=\"https://opencollective.com/instapy/backers/badge.svg\">\\n    </a>\\n    <a href=\"https://www.github.com/timgrossmann/InstaPy#sponsors\">\\n\\t<img src=\"https://opencollective.com/instapy/sponsors/badge.svg\">\\n    </a>  \\n    <a href=\"https://discord.gg/FDETsht\">\\n\\t<img src=\"https://img.shields.io/discord/510385886869979136.svg\">\\n    </a>\\n  </p>\\n</p>\\n\\n[Twitter of InstaPy](https://twitter.com/InstaPy) | [Discord Channel](https://discord.gg/FDETsht) |\\xa0[How it works (FreeCodingCamp)](https://www.freecodecamp.org/news/my-open-source-instagram-bot-got-me-2-500-real-followers-for-5-in-server-costs-e40491358340/) |   \\n[Talk about automating your Instagram](https://youtu.be/4TmKFZy-ioQ) | [Talk about doing Open-Source work](https://www.youtube.com/watch?v=A_UtST302Og&t=0s&list=PLa4P1NPX9hthXV-wko0xyxFpbhYZFkW7o) |\\xa0[Listen to the \"Talk Python to me\"-Episode](https://talkpython.fm/episodes/show/142/automating-the-web-with-selenium-and-instapy)\\n\\n\\n**Newsletter: [Sign Up for the Newsletter here!](http://eepurl.com/cZbV_v)**   \\n**Guide to Bot Creation: [Learn to Build your own Bots](https://www.udemy.com/course/the-complete-guide-to-bot-creation/?referralCode=7418EBB47E11E34D86C9)**    \\n\\n<br />\\n\\n# Find the full documentation in [Docs](/docs)\\n**Table of contents**\\n- [How to install and run InstaPy](/docs/home.md#installation)\\n  * [Installing InstaPy](/docs/home.md#installation)\\n  * [Running Instapy](/docs/home.md#running-instapy)\\n  * [Updating InstaPy](/docs/home.md#updating-instapy)\\n  * [Guides and tutorials](/docs/home.md#guides)\\n    * [Video tutorials](/docs/home.md#video-tutorials)\\n    * [Written guides](/docs/home.md#written-guides)\\n- [Externals and additionals tools](/docs/home.md#external-tools)\\n- [Running InstaPy on Docker](/docs/home.md#docker)\\n- [Documentation of all Instapy\\'s features](/docs/settings.md)\\n- [Support](/docs/home.md#support)\\n- [Credits](/docs/home.md#credits)\\n\\n<br />\\n\\n## Credits\\n### Community\\nAn active and supportive community is what every open-source project needs to sustain. Together we reached every continent and most of the countries in the world!   \\nThank you all for being part of the InstaPy community ✌️\\n\\n<p align=\"center\">\\n\\t<img src=\"https://i.imgur.com/XkxHcM7r.png\" alt=\"InstaPy reach\" width=\"500px\"/>\\n</p>\\n\\n### Contributors\\n\\nThis project exists thanks to all the people who contribute. [[Contribute](https://github.com/timgrossmann/InstaPy/wiki/How-to-Contribute)].\\n\\n<a href=\"https://github.com/timgrossmann/InstaPy/graphs/contributors\"><img src=\"https://opencollective.com/instapy/contributors.svg?width=890&button=false\" /></a>\\n\\n### Backers\\n\\nThank you to all our backers! 🙏 [[Become a backer](https://opencollective.com/instapy#backer)]\\n\\n<a href=\"https://opencollective.com/instapy#backers\" target=\"_blank\"><img src=\"https://opencollective.com/instapy/backers.svg?width=890\"></a>\\n\\n---\\n\\n> **Disclaimer**<a name=\"disclaimer\" />: Please note that this is a research project. I am by no means responsible for any usage of this tool. Use it on your behalf. I\\'m also not responsible if your accounts get banned due to the extensive use of this tool.\\n'},\n",
       " {'repo': './rawandahmad698/PyChatGPT',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '[Discord Discussion](https://discord.gg/MqeaZsy4F5)\\nCurrent State: Not maintained. Not Working.\\n\\nSorry guys! Really busy with private projects. This was very fun!\\n\\n\\n# 🔥 PyChatGPT\\n[Read More - How OpenAI filters requests made by bots/scrapers](https://github.com/rawandahmad698/PyChatGPT/discussions/103)\\n\\n[![Python](https://img.shields.io/badge/python-3.8-blue.svg)](https://img.shields.io/badge/python-3.8-blue.svg)\\n[![PyPi](https://img.shields.io/pypi/v/chatgptpy.svg)](https://pypi.python.org/pypi/chatgptpy)\\n[![PyPi](https://img.shields.io/pypi/dm/chatgptpy.svg)](https://pypi.python.org/pypi/chatgptpy)\\n\\n*⭐️ Like this repo? please star & consider donating to keep it maintained*\\n\\n<a href=\"https://www.buymeacoffee.com/rawandahmed\" target=\"_blank\"><img src=\"https://cdn.buymeacoffee.com/buttons/v2/default-yellow.png\" alt=\"Buy Me A Coffee\" style=\"height: 60px !important;width: 217px !important;\" ></a>\\n\\n*💡 If OpenAI change their API, I will fix it as soon as possible, so <mark>Watch</mark> the repo if you want to be notified*\\n\\n### Features\\n- [x] Save Conversations to a file\\n- [x] Resume conversations even after closing the program\\n- [x] Proxy Support\\n- [x] Automatically login without involving a browser\\n- [x] Automatically grab Access Token\\n- [x] Get around the login **captcha** (If you try to log in subsequently, you will be prompted to solve a captcha)\\n- [x] Saves the access token to a file, so you don\\'t have to log in again\\n- [x] Automatically refreshes the access token when it expires\\n- [x] Uses colorama to colorize the output, because why not?\\n- [x] Smart Conversation Tracking \\n\\n## Web Demo\\nIntegrated into [Huggingface Spaces 🤗](https://huggingface.co/spaces) using [Gradio](https://github.com/gradio-app/gradio). Try out the Web Demo\\n\\n[![Hugging Face Spaces](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/yizhangliu/chatGPT)\\n\\n<p align=\"center\">Chatting</p>\\n\\n![Screenshot 1](https://media.discordapp.net/attachments/1038565125482881027/1049255804366237736/image.png)\\n\\n[//]: # (Italic centred text saying screenshots)\\n<p align=\"center\">Creating a token</p>\\n\\n![Screenshot 2](https://media.discordapp.net/attachments/1038565125482881027/1049072247442264094/image.png?width=2468&height=885)\\n\\n```\\nYou: Hi there, My name is Rawa\\nChat GPT: Hello Rawa, nice to meet you. Is there something you would like to talk about or ask me? I\\'m here to help with any questions you may have.\\nYou: great, now say my name like Heisenberg\\nChat GPT: Sure, Rawa like Heisenberg. Is there anything else you would like to talk about? I\\'m here to help with any questions you may have.\\nYou: Sorry I meant like the episode of Breaking Bad where Walter White says Heisenberg\\nChat GPT: Ah, I see. In that case, you could try saying it like this: \"My name is Rawa, like Heisenberg.\" This is a reference to the character Walter White from the TV show Breaking Bad, who often used the pseudonym \"Heisenberg\" when conducting illegal activities. The character was known for his cool and calculated demeanor, so saying your name like Heisenberg in this context would mean saying it with confidence and authority.\\n ```\\n\\n## Install\\n```\\npip install chatgptpy --upgrade\\n```\\n\\n## Usage\\n[**NEW**] Pass a `options()` object to the `ChatGPT()` constructor to customize the session\\n\\n[**NEW**] You can now save your conversations to a file\\n\\n```python\\nfrom pychatgpt import Chat, Options\\n\\noptions = Options()\\n\\n# [New] Pass Moderation. https://github.com/rawandahmad698/PyChatGPT/discussions/103\\n# options.pass_moderation = False\\n\\n# [New] Enable, Disable logs\\noptions.log = True\\n\\n# Track conversation\\noptions.track = True \\n\\n# Use a proxy\\noptions.proxies = \\'http://localhost:8080\\'\\n\\n# Optionally, you can pass a file path to save the conversation\\n# They\\'re created if they don\\'t exist\\n\\n# options.chat_log = \"chat_log.txt\"\\n# options.id_log = \"id_log.txt\"\\n\\n# Create a Chat object\\nchat = Chat(email=\"email\", password=\"password\", options=options)\\nanswer = chat.ask(\"How are you?\")\\nprint(answer)\\n```\\n\\n[**NEW**] Resume a conversation\\n```python\\nfrom pychatgpt import Chat\\n\\n# Create a Chat object\\nchat = Chat(email=\"email\", password=\"password\", \\n            conversation_id=\"Parent Conversation ID\", \\n            previous_convo_id=\"Previous Conversation ID\")\\n\\nanswer, parent_conversation_id, conversation_id = chat.ask(\"How are you?\")\\n\\nprint(answer)\\n\\n# Or change the conversation id later\\nanswer, _, _ = chat.ask(\"How are you?\", \\n                        previous_convo_id=\"Parent Conversation ID\",\\n                        conversation_id=\"Previous Conversation ID\")\\nprint(answer)\\n\\n```\\nStart a CLI Session\\n```python\\nfrom pychatgpt import Chat\\n\\nchat = Chat(email=\"email\", password=\"password\")\\nchat.cli_chat()\\n```\\n\\nAsk a one time question\\n```python\\nfrom pychatgpt import Chat\\n\\n# Initializing the chat class will automatically log you in, check access_tokens\\nchat = Chat(email=\"email\", password=\"password\") \\nanswer, parent_conversation_id, conversation_id = chat.ask(\"Hello!\")\\n```\\n\\n#### You could also manually set, get the token\\n```python\\nimport time\\nfrom pychatgpt import OpenAI\\n\\n# Manually set the token\\nOpenAI.Auth(email_address=\"email\", password=\"password\").save_access_token(access_token=\"\", expiry=time.time() + 3600)\\n\\n# Get the token, expiry\\naccess_token, expiry = OpenAI.get_access_token()\\n\\n# Check if the token is valid\\nis_expired = OpenAI.token_expired() # Returns True or False\\n```\\n[//]: # (Add A changelog here)\\n<details><summary>Change Log</summary>\\n\\n#### Update using `pip install chatgptpy --upgrade`\\n\\n#### 1.0.8\\n- Fixes an issue when reading from id_log.txt\\n- Introduces a new `pass_moderation` parameter to the `options()` class, defaults to `False`\\n- Adds proxies to moderation.\\n- If `pass_moderation` is True, the function is invoked in another thread, so it doesn\\'t block the main thread.\\n\\n#### 1.0.7\\n- Make a request to the mod endpoint first, otherwise a crippled version of the response is returned\\n\\n#### 1.0.6\\n- New option to turn off logs. \\n- Better Error handling.\\n- Enhanced conversation tracking\\n- Ask now returns a tuple of `answer, previous_convo, convo_id` \\n- Better docs\\n\\n#### 1.0.5\\n- Pull requests/minor fixes\\n\\n#### 1.0.4\\n- Fixes for part 8 of token authentication\\n\\n#### 1.0.3 \\n- a new `options()` class method to set the options for the chat session\\n- save the conversation to a file\\n- resume the conversation even after closing the program\\n\\n\\n#### 1.0.2\\n- ChatGPT API switches from `action=next` to `action=variant`, frequently. This library is now using `action=variant` instead of `action=next` to get the next response from the API.\\n- Sometimes when the server is overloaded, the API returns a `502 Bad Gateway` error.\\n- Added Error handling if the auth.json file is not found/corrupt\\n\\n#### 1.0.0\\n- Initial Release via PyPi\\n</details>\\n\\n### Other notes\\nIf the token creation process is failing:\\n1. Try to use a proxy (I recommend using this always)\\n2. Don\\'t try to log in too fast. At least wait 10 minutes if you\\'re being rate limited.\\n3. If you\\'re still having issues, try to use a VPN. On a VPN, the script should work fine.\\n\\n\\n### What\\'s next?\\nI\\'m planning to add a few more features, such as:\\n- [x] A python module that can be imported and used in other projects\\n- [x] A way to save the conversation\\n- [ ] Better error handling\\n- [ ] Multi-user chatting\\n\\n### The whole process\\nI have been looking for a way to interact with the new Chat GPT API, but most of the sources here on GitHub \\nrequire you to have a Chromium instance running in the background. or by using the Web Inspector to grab Access Token manually.\\n\\nNo more. I have been able to reverse engineer the API and use a TLS client to mimic a real user, allowing the script to login without setting off any bot detection techniques by Auth0\\n\\nBasically, the script logs in on your behalf, using a TLS client, then grabs the Access Token. It\\'s pretty fast.\\n\\nFirst, I\\'d like to tell you that \"just making http\" requests is not going to be enough, Auth0 is smart, each process is guarded by a \\n`state` token, which is a JWT token. This token is used to prevent CSRF attacks, and it\\'s also used to prevent bots from logging in.\\nIf you look at the `auth.py` file, there are over nine functions, each one of them is responsible for a different task, and they all\\nwork together to create a token for you. `allow-redirects` played a huge role in this, as it allowed to navigate through the login process\\n\\nI work at MeshMonitors.io, We make amazing tools (Check it out yo!). I decided not to spend too much time on this, but here we are.\\n\\n### Why did I do this?\\nNo one has been able to do this, and I wanted to see if I could.\\n\\n### Credits\\n- [OpenAI](https://openai.com/) for creating the ChatGPT API\\n- [FlorianREGAZ](https://github.com/FlorianREGAZ) for the TLS Client\\n'},\n",
       " {'repo': './Dong-learn9/TVBox-zyjk',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '# TVbox自用接口\\n\\n1、给英文不好的朋友\\n\\nGitHub 中文化插件 🔰https://greasyfork.org/zh-CN/scripts/435208\\n\\nGithub 增强-高速下载 🔰https://greasyfork.org/zh-CN/scripts/412245\\n\\n2、Github RAW 加速服务\\n\\n源码 https://gitcdn.top/https://github.com/用户名/仓库名/raw/main/接口文件\\n\\n香港 https://raw.iqiq.io/用户名/仓库名/main/接口文件\\n\\n新加坡 https://raw.kgithub.com/用户名/仓库名/main/接口文件\\n\\n日本 \\n\\nhttps://fastly.jsdelivr.net/gh/用户名/仓库名@main/接口文件\\n\\nhttps://cdn.staticaly.com/gh/用户名/仓库名/main/接口文件\\n\\nhttps://raw.fastgit.org/用户名/仓库名/main/接口文件\\n\\n韩国\\n\\nhttps://ghproxy.com/https://raw.githubusercontent.com/用户名/仓库名/main/接口文件\\n\\nhttps://ghproxy.net/https://raw.githubusercontent.com/用户名/仓库名/main/接口文件\\n\\nhttps://gcore.jsdelivr.net/gh/用户名/仓库名@main/接口文件\\n\\nhttps://raw.githubusercontents.com/用户名/仓库名/main/接口文件\\n\\n3，Github 静态加速\\n\\nhttps://cdn.staticaly.com/gh/用户名/仓库名/main/接口文件\\n\\nhttps://cdn.jsdelivr.net/gh/用户名/仓库名@main/接口文件\\n\\nhttps://purge.jsdelivr.net/gh/用户名/仓库名@main/接口文件\\n\\n4、EGP源\\n\\nhttp://epg.51zmt.top:8000/e.xml\\n\\nhttps://epg.112114.xyz/pp.xml\\n\\n以下为地址\\n\\nepg-DIYP接口1：(hhttp://epg.51zmt.top:8000/api/diyp/)\\n\\nepg-DIYP接口2：(http://diyp.112114.xyz/)\\n\\nepg-DIYP接口3：(https://epg.hicloud.co/epg.php/)\\n\\nepg1：(https://epg.sec.st/epg.php/)\\n\\nepg3：(https://epg.pm/)\\n\\nepg4：(http://n33426t756.wicp.vip/diyp/epg.php/)\\n\\nepg5：(http://www.diyp.top/diyp/epg.php/)\\n\\n总epg：(http://epg.51zmt.top:8000/e.xml/)\\n\\n央视及各省卫视epg：(http://epg.51zmt.top:8000/cc.xml/)\\n\\n地方及数字付费epg：(http://epg.51zmt.top:8000/difang.xml/)\\n\\n港澳台及海外epg：(http://epg.51zmt.top:8000/gat.xml/)\\n\\nepg6：(http://124.223.212.38:83/)\\n\\nepg7：(https://epg.112114.xyz/)\\n\\n[超级直播](https://epg.112114.xyz/epginfo)\\n\\n[Xml格式](https://epg.112114.xyz/pp.xml)\\n\\n[Xml格式](https://epg.112114.xyz/pp.xml.gz)\\n\\n\\n5、开源仓库\\n\\nhttps://github.com/\\n\\nhttps://gitlab.com/\\n\\nhttps://gitee.com/\\n\\nhttps://coding.net/\\n\\nhttps://gitcode.net/\\n\\nhttps://gitea.com/   仓库名是 mao,tvbox,box,tv等类似的，有几率出现 1.删除仓库 2.删除用户 3.封禁账户 4.黑名单\\n\\nhttps://agit.ai/\\n\\nhttps://notabug.org/\\n\\n6、短地址平台\\n\\n（1）https://short.io\\n\\n（2）http://88d.cn\\n\\n（3）https://77url.com\\n\\n（4）https://suowo.cn\\n\\n（5）https://6du.in\\n\\n（6）https://www.urlc.cn\\n\\n（7）https://59z.cn\\n\\n（8）https://suowo.cn\\n\\n（9）https://0a.fit/\\n\\n（10）https://www.urlc.cn/\\n\\n7、TVBox各路大佬配置（排名不分先后）：\\n\\n（1）唐三：https://hutool.ml/tang\\n\\n（2）Fongmi：https://raw.fastgit.org/FongMi/CatVodSpider/main/json/config.json\\n\\n（3）俊于：http://home.jundie.top:81/top98.json\\n\\n（4）饭太硬：http://饭太硬.ga/x/o.json\\n\\n（5）霜辉月明py：https://ghproxy.com/raw.githubusercontent.com/lm317379829/PyramidStore/pyramid/py.json\\n\\n（6）小雅dr：http://drpy.site/js1\\n\\n（7）菜妮丝：https://tvbox.cainisi.cf\\n\\n（8）神器：https://神器每日推送.tk/pz.json\\n\\n（9）巧技：http://pandown.pro/tvbox/tvbox.json\\n\\n（10）刚刚：http://刚刚.live/猫\\n\\n（11）吾爱有三：http://52bsj.vip:98/0805\\n\\n（12）潇洒：https://download.kstore.space/download/2863/01.txt\\n\\n（13）佰欣园：https://ghproxy.com/https://raw.githubusercontent.com/chengxueli818913/maoTV/main/44.txt\\n\\n（14）胖虎：https://notabug.org/imbig66/tv-spider-man/raw/master/配置/0801.json\\n\\n（15）云星日记：https://maoyingshi.cc/tiaoshizhushou/1.txt\\n\\n（16）Yoursmile7：https://agit.ai/Yoursmile7/TVBox/raw/branch/master/XC.json\\n\\n（17）BOX：http://52bsj.vip:81/api/v3/file/get/29899/box2.json?sign=3cVyKZQr3lFAwdB3HK-A7h33e0MnmG6lLB9oWlvSNnM%3D%3A0\\n\\n（18）哔哩学习：http://52bsj.vip:81/api/v3/file/get/41063/bili.json?sign=TxuApYZt6bNl9TzI7vObItW34UnATQ4RQxABAEwHst4%3D%3A0\\n\\n（19）UndCover：https://raw.githubusercontent.com/UndCover/PyramidStore/main/py.json\\n\\n（20）木极：https://pan.tenire.com/down.php/2664dabf44e1b55919f481903a178cba.txt\\n\\n（21）Ray：https://dxawi.github.io/0/0.json\\n\\n（22）甜蜜：https://kebedd69.github.io/TVbox-interface/py甜蜜.json\\n\\n（23）52bsj：http://52bsj.vip:81/api/v3/file/get/29899/box2.json?sign=3cVyKZQr3lFAwdB3HK-A7h33e0MnmG6lLB9oWlvSNnM%3D%3A0\\n\\n（24）肥猫：http://我不是.肥猫.love:63\\n\\n8、随机轮换壁纸：\\n\\n（1）https://bing.img.run/rand.php\\n\\n（2）http://www.kf666888.cn/api/tvbox/img\\n\\n（3）https://picsum.photos/1280/720/?blur=10\\n\\n（4）http://刚刚.live/图\\n\\n（5）http://饭太硬.ga/深色壁纸/api.php\\n\\n（6）https://www.dmoe.cc/random.php\\n\\n（7）https://api.btstu.cn/sjbz/zsy.php\\n\\n（8）https://api.btstu.cn/sjbz/?lx=dongman\\n\\n（9）http://api.btstu.cn/sjbz/?lx=meizi\\n\\n（10）http://api.btstu.cn/sjbz/?lx=suiji\\n\\n（11）https://pictures.catvod.eu.org/\\n\\n9、工具\\n\\n（1）文本处理： http://www.txttool.com/\\n\\n本页面只是收集Box，自用请勿宣传。\\n\\n所有资源全部搜集于网络'},\n",
       " {'repo': './StevenBlack/hosts',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '**Take Note!**\\n\\nWith the exception of issues and PRs regarding changes to\\n`hosts/data/StevenBlack/hosts`, all other issues regarding the content of the\\nproduced hosts files should be made with the appropriate data source that\\ncontributed the content in question. The contact information for all of the data\\nsources can be found in the `hosts/data/` directory.\\n\\n---\\n\\n![Logo](https://raw.githubusercontent.com/StevenBlack/hosts/master/.github/logo.png)\\n\\n[![latest release](https://img.shields.io/github/release/StevenBlack/hosts.svg)](https://github.com/StevenBlack/hosts/releases)\\n[![license](https://img.shields.io/github/license/StevenBlack/hosts.svg)](https://github.com/StevenBlack/hosts/blob/master/license.txt)\\n[![repo size](https://img.shields.io/github/repo-size/StevenBlack/hosts.svg)](https://github.com/StevenBlack/hosts)\\n[![contributors](https://img.shields.io/github/contributors/StevenBlack/hosts.svg)](https://github.com/StevenBlack/hosts/graphs/contributors)\\n[![Build Status](https://img.shields.io/github/actions/workflow/status/StevenBlack/hosts/ci.yml?branch=master)](https://github.com/StevenBlack/hosts/actions/workflows/ci.yml?query=branch%3Amaster)\\n[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/python/black)\\n[![commits since last release](https://img.shields.io/github/commits-since/StevenBlack/hosts/latest.svg)](https://github.com/StevenBlack/hosts/commits/master)\\n[![last commit](https://img.shields.io/github/last-commit/StevenBlack/hosts.svg)](https://github.com/StevenBlack/hosts/commits/master)\\n[![commit activity](https://img.shields.io/github/commit-activity/y/StevenBlack/hosts.svg)](https://github.com/StevenBlack/hosts/commits/master)\\n\\n# Unified hosts file with base extensions\\n\\nThis repository consolidates several reputable `hosts` files, and merges them\\ninto a unified hosts file with duplicates removed. A variety of tailored hosts\\nfiles are provided.\\n\\n**Therefore this repository is a hosts file aggregator.**\\n\\n![Aggregator](https://raw.githubusercontent.com/StevenBlack/hosts/master/aggregator.png)\\n\\n- Last updated: **May 11 2023**.\\n- Here\\'s the\\n  [raw hosts file with base extensions](https://raw.githubusercontent.com/StevenBlack/hosts/master/hosts)\\n  containing 179,308 entries.\\n\\n![Size history](https://raw.githubusercontent.com/StevenBlack/hosts/master/hosts_file_size_history.png)\\n\\n## List of all hosts file variants\\n\\nThis repository offers\\n[15 different host file variants](https://github.com/StevenBlack/hosts/tree/master/alternates),\\nin addition to the base variant.\\n\\nThe **Non GitHub mirror** is the link to use for some hosts file managers like\\n[Hostsman for Windows](https://www.abelhadigital.com/hostsman/) that don\\'t work\\nwith GitHub download links.\\n\\n| Host file recipe | Readme | Raw hosts | Unique domains | Non GitHub mirror |\\n| ---------------- | :----: | :-------: | :------------: | :---------------: |\\nUnified hosts = **(adware + malware)** | [Readme](https://github.com/StevenBlack/hosts/blob/master/readme.md) | [link](https://raw.githubusercontent.com/StevenBlack/hosts/master/hosts) | 179,308 | [link](http://sbc.io/hosts/hosts)\\nUnified hosts **+ fakenews** | [Readme](https://github.com/StevenBlack/hosts/blob/master/alternates/fakenews/readme.md) | [link](https://raw.githubusercontent.com/StevenBlack/hosts/master/alternates/fakenews/hosts) | 181,502 | [link](http://sbc.io/hosts/alternates/fakenews/hosts)\\nUnified hosts **+ gambling** | [Readme](https://github.com/StevenBlack/hosts/blob/master/alternates/gambling/readme.md) | [link](https://raw.githubusercontent.com/StevenBlack/hosts/master/alternates/gambling/hosts) | 186,800 | [link](http://sbc.io/hosts/alternates/gambling/hosts)\\nUnified hosts **+ porn** | [Readme](https://github.com/StevenBlack/hosts/blob/master/alternates/porn/readme.md) | [link](https://raw.githubusercontent.com/StevenBlack/hosts/master/alternates/porn/hosts) | 227,330 | [link](http://sbc.io/hosts/alternates/porn/hosts)\\nUnified hosts **+ social** | [Readme](https://github.com/StevenBlack/hosts/blob/master/alternates/social/readme.md) | [link](https://raw.githubusercontent.com/StevenBlack/hosts/master/alternates/social/hosts) | 182,132 | [link](http://sbc.io/hosts/alternates/social/hosts)\\nUnified hosts **+ fakenews + gambling** | [Readme](https://github.com/StevenBlack/hosts/blob/master/alternates/fakenews-gambling/readme.md) | [link](https://raw.githubusercontent.com/StevenBlack/hosts/master/alternates/fakenews-gambling/hosts) | 188,994 | [link](http://sbc.io/hosts/alternates/fakenews-gambling/hosts)\\nUnified hosts **+ fakenews + porn** | [Readme](https://github.com/StevenBlack/hosts/blob/master/alternates/fakenews-porn/readme.md) | [link](https://raw.githubusercontent.com/StevenBlack/hosts/master/alternates/fakenews-porn/hosts) | 229,524 | [link](http://sbc.io/hosts/alternates/fakenews-porn/hosts)\\nUnified hosts **+ fakenews + social** | [Readme](https://github.com/StevenBlack/hosts/blob/master/alternates/fakenews-social/readme.md) | [link](https://raw.githubusercontent.com/StevenBlack/hosts/master/alternates/fakenews-social/hosts) | 184,326 | [link](http://sbc.io/hosts/alternates/fakenews-social/hosts)\\nUnified hosts **+ gambling + porn** | [Readme](https://github.com/StevenBlack/hosts/blob/master/alternates/gambling-porn/readme.md) | [link](https://raw.githubusercontent.com/StevenBlack/hosts/master/alternates/gambling-porn/hosts) | 234,822 | [link](http://sbc.io/hosts/alternates/gambling-porn/hosts)\\nUnified hosts **+ gambling + social** | [Readme](https://github.com/StevenBlack/hosts/blob/master/alternates/gambling-social/readme.md) | [link](https://raw.githubusercontent.com/StevenBlack/hosts/master/alternates/gambling-social/hosts) | 189,624 | [link](http://sbc.io/hosts/alternates/gambling-social/hosts)\\nUnified hosts **+ porn + social** | [Readme](https://github.com/StevenBlack/hosts/blob/master/alternates/porn-social/readme.md) | [link](https://raw.githubusercontent.com/StevenBlack/hosts/master/alternates/porn-social/hosts) | 230,153 | [link](http://sbc.io/hosts/alternates/porn-social/hosts)\\nUnified hosts **+ fakenews + gambling + porn** | [Readme](https://github.com/StevenBlack/hosts/blob/master/alternates/fakenews-gambling-porn/readme.md) | [link](https://raw.githubusercontent.com/StevenBlack/hosts/master/alternates/fakenews-gambling-porn/hosts) | 237,016 | [link](http://sbc.io/hosts/alternates/fakenews-gambling-porn/hosts)\\nUnified hosts **+ fakenews + gambling + social** | [Readme](https://github.com/StevenBlack/hosts/blob/master/alternates/fakenews-gambling-social/readme.md) | [link](https://raw.githubusercontent.com/StevenBlack/hosts/master/alternates/fakenews-gambling-social/hosts) | 191,818 | [link](http://sbc.io/hosts/alternates/fakenews-gambling-social/hosts)\\nUnified hosts **+ fakenews + porn + social** | [Readme](https://github.com/StevenBlack/hosts/blob/master/alternates/fakenews-porn-social/readme.md) | [link](https://raw.githubusercontent.com/StevenBlack/hosts/master/alternates/fakenews-porn-social/hosts) | 232,347 | [link](http://sbc.io/hosts/alternates/fakenews-porn-social/hosts)\\nUnified hosts **+ gambling + porn + social** | [Readme](https://github.com/StevenBlack/hosts/blob/master/alternates/gambling-porn-social/readme.md) | [link](https://raw.githubusercontent.com/StevenBlack/hosts/master/alternates/gambling-porn-social/hosts) | 237,645 | [link](http://sbc.io/hosts/alternates/gambling-porn-social/hosts)\\nUnified hosts **+ fakenews + gambling + porn + social** | [Readme](https://github.com/StevenBlack/hosts/blob/master/alternates/fakenews-gambling-porn-social/readme.md) | [link](https://raw.githubusercontent.com/StevenBlack/hosts/master/alternates/fakenews-gambling-porn-social/hosts) | 239,839 | [link](http://sbc.io/hosts/alternates/fakenews-gambling-porn-social/hosts)\\n\\n\\n**Expectation**: These unified hosts files should serve all devices, regardless\\nof OS.\\n\\n## Sources of hosts data unified in this variant\\n\\nUpdated `hosts` files from the following locations are always unified and\\nincluded:\\n\\n| Host file source | Home page | Raw hosts | License | Issues | Description |\\n| ---------------- | :-------: | :-------: | :-----: | :----: | ----------- |\\nSteven Black\\'s ad-hoc list |[link](https://github.com/StevenBlack/hosts/blob/master/data/StevenBlack/hosts) | [raw](https://raw.githubusercontent.com/StevenBlack/hosts/master/data/StevenBlack/hosts) | MIT | [issues](https://github.com/StevenBlack/hosts/issues)| Additional sketch domains as I come across them.\\nAdAway |[link](https://adaway.org/) | [raw](https://raw.githubusercontent.com/AdAway/adaway.github.io/master/hosts.txt) | CC BY 3.0 | [issues](https://github.com/AdAway/adaway.github.io/issues)| AdAway is an open source ad blocker for Android using the hosts file.\\nadd.2o7Net |[link](https://github.com/FadeMind/hosts.extras) | [raw](https://raw.githubusercontent.com/FadeMind/hosts.extras/master/add.2o7Net/hosts) | MIT | [issues](https://github.com/FadeMind/hosts.extras/issues)| 2o7Net tracking sites based on [hostsfile.org](https://www.hostsfile.org/hosts.html) content.\\nadd.Dead |[link](https://github.com/FadeMind/hosts.extras) | [raw](https://raw.githubusercontent.com/FadeMind/hosts.extras/master/add.Dead/hosts) | MIT | [issues](https://github.com/FadeMind/hosts.extras/issues)| Dead sites based on [hostsfile.org](https://www.hostsfile.org/hosts.html) content.\\nadd.Risk |[link](https://github.com/FadeMind/hosts.extras) | [raw](https://raw.githubusercontent.com/FadeMind/hosts.extras/master/add.Risk/hosts) | MIT | [issues](https://github.com/FadeMind/hosts.extras/issues)| Risk content sites based on [hostsfile.org](https://www.hostsfile.org/hosts.html) content.\\nadd.Spam |[link](https://github.com/FadeMind/hosts.extras) | [raw](https://raw.githubusercontent.com/FadeMind/hosts.extras/master/add.Spam/hosts) | MIT | [issues](https://github.com/FadeMind/hosts.extras/issues)| Spam sites based on [hostsfile.org](https://www.hostsfile.org/hosts.html) content.\\nMitchell Krog\\'s - Badd Boyz Hosts |[link](https://github.com/mitchellkrogza/Badd-Boyz-Hosts) | [raw](https://raw.githubusercontent.com/mitchellkrogza/Badd-Boyz-Hosts/master/hosts) | MIT | [issues](https://github.com/mitchellkrogza/Badd-Boyz-Hosts/issues)| Sketchy domains and Bad Referrers from my Nginx and Apache Bad Bot and Spam Referrer Blockers\\nhostsVN |[link](https://github.com/bigdargon/hostsVN) | [raw](https://raw.githubusercontent.com/bigdargon/hostsVN/master/option/hosts-VN) | MIT | [issues](https://github.com/bigdargon/hostsVN/issues)| Hosts block ads of Vietnamese\\nKADhosts |[link](https://kadantiscam.netlify.app/) | [raw](https://raw.githubusercontent.com/PolishFiltersTeam/KADhosts/master/KADhosts.txt) | CC BY-SA 4.0 | [issues](https://github.com/PolishFiltersTeam/KADhosts/issues)| Fraud/adware/scam websites.\\nMetaMask eth-phishing-detect |[link](https://github.com/MetaMask/eth-phishing-detect) | [raw](https://raw.githubusercontent.com/MetaMask/eth-phishing-detect/master/src/hosts.txt) | DON\\'T BE A DICK PUBLIC LICENSE | [issues](https://github.com/MetaMask/eth-phishing-detect/issues)| Phishing domains targeting Ethereum users.\\nminecraft-hosts |[link](https://github.com/jamiemansfield/minecraft-hosts) | [raw](https://raw.githubusercontent.com/jamiemansfield/minecraft-hosts/master/lists/tracking.txt) | CC0-1.0 | [issues](https://github.com/jamiemansfield/minecraft-hosts/issues)| Minecraft related tracker hosts\\nMVPS hosts file |[link](https://winhelp2002.mvps.org/) | [raw](https://winhelp2002.mvps.org/hosts.txt) | CC BY-NC-SA 4.0 | [issues](mailto:winhelp2002@gmail.com)| The purpose of this site is to provide the user with a high quality custom HOSTS file.\\nshady-hosts |[link](https://github.com/shreyasminocha/shady-hosts) | [raw](https://raw.githubusercontent.com/shreyasminocha/shady-hosts/main/hosts) | CC0-1.0 | [issues](https://github.com/shreyasminocha/shady-hosts/issues)| Analytics, ad, and activity monitoring hosts\\nDan Pollock – [someonewhocares](https://someonewhocares.org/) |[link](https://someonewhocares.org/hosts/) | [raw](https://someonewhocares.org/hosts/zero/hosts) | non-commercial with attribution | [issues](mailto:hosts@someonewhocares.org)| How to make the internet not suck (as much).\\nTiuxo hostlist - ads |[link](https://github.com/tiuxo/hosts) | [raw](https://raw.githubusercontent.com/tiuxo/hosts/master/ads) | CC BY 4.0 | [issues](https://github.com/tiuxo/hosts/issues)| Categorized hosts files for DNS based content blocking\\nUncheckyAds |[link](https://github.com/FadeMind/hosts.extras) | [raw](https://raw.githubusercontent.com/FadeMind/hosts.extras/master/UncheckyAds/hosts) | MIT | [issues](https://github.com/FadeMind/hosts.extras/issues)| Windows installers ads sources sites based on https://unchecky.com/ content.\\nURLHaus |[link](https://urlhaus.abuse.ch/) | [raw](https://urlhaus.abuse.ch/downloads/hostfile/) | CC0 | [issues](mailto:contactme@abuse.ch)| A project from [abuse.ch](https://abuse.ch/) with the goal of sharing malicious URLs.\\nyoyo.org |[link](https://pgl.yoyo.org/adservers/) | [raw](https://pgl.yoyo.org/adservers/serverlist.php?hostformat=hosts&mimetype=plaintext&useip=0.0.0.0) |  | [issues](mailto:pgl@yoyo.org)| Blocking with ad server and tracking server hostnames.\\n\\n\\n## Extensions\\n\\nThe unified hosts file is optionally extensible. Extensions are used to include\\ndomains by category. Currently, we offer the following categories: `fakenews`,\\n`social`, `gambling`, and `porn`.\\n\\nExtensions are optional, and can be combined in various ways with the base hosts\\nfile. The combined products are stored in the\\n[`alternates`](https://github.com/StevenBlack/hosts/tree/master/alternates)\\nfolder.\\n\\nData for extensions are stored in the\\n[`extensions`](https://github.com/StevenBlack/hosts/tree/master/extensions)\\nfolder. You manage extensions by curating this folder tree, where you will find\\nthe data for `fakenews`, `social`, `gambling`, and `porn` extension data that we\\nmaintain and provide for you.\\n\\n## Generate your own unified hosts file\\n\\nYou have three options to generate your own hosts file. You can use our\\ncontainer image, build your own image, or do it in your own environment. Option\\n#1 is easiest if you have Linux with Docker installed.\\n\\n### Option 1: Use our container image (Linux only)\\n\\n> This will replace your `/etc/hosts`.\\n\\nWe assume you have Docker available on your host. Just run the following\\ncommand. Set extensions to your preference.\\n\\n```sh\\ndocker run --pull always --rm -it -v /etc/hosts:/etc/hosts \\\\\\nghcr.io/stevenblack/hosts:latest updateHostsFile.py --auto \\\\\\n--replace --extensions gambling porn\\n```\\n\\nIf you want to add custom hosts or a whitelist, create either or both files as\\nper [the instructions](#how-do-i-control-which-sources-are-unified) and add the\\nfollowing arguments _before_ `ghcr.io/stevenblack/hosts:latest` depending on\\nwhich you wish to use.\\n\\n```sh\\n-v \"path/to/myhosts:/hosts/myhosts\" \\\\\\n-v \"path/to/whitelist:/hosts/whitelist\" \\\\\\n```\\n\\nYou can rerun this exact command later to update based on the latest available\\nhosts (for example, add it to a weekly cron job).\\n\\n### Option 2: Generate your own container image\\n\\nWe provide the\\n[Dockerfile](https://github.com/StevenBlack/hosts/blob/master/Dockerfile) used\\nby the previous step, which you can use to create a container image with\\neverything you need. The container will contain Python 3 and all its dependency\\nrequirements, and a copy of the latest version of this repository.\\n\\nBuild the Docker container from the root of this repo like this:\\n\\n```sh\\ndocker build --no-cache . -t stevenblack-hosts\\n```\\n\\nThen run your command as such:\\n\\n```sh\\ndocker run --rm -it stevenblack-hosts updateHostsFile.py\\n```\\n\\n> This will create the hosts file, and remove it with the container when done,\\n> so not very useful. You can use the example in option #1 to add volumes so\\n> files on your host are replaced.\\n\\n### Option 3: Generate it in your own environment\\n\\nTo generate your own amalgamated hosts files you will need Python 3.6 or later.\\n\\nFirst, install the dependencies with:\\n\\n```sh\\npip3 install --user -r requirements.txt\\n```\\n\\n**Note** we recommend the `--user` flag which installs the required dependencies\\nat the user level. More information about it can be found on pip\\n[documentation](https://pip.pypa.io/en/stable/reference/pip_install/?highlight=--user#cmdoption-user).\\n\\n### Common steps regardless of your development environment\\n\\nTo **run unit tests**, in the top-level directory, run:\\n\\n```sh\\npython3 testUpdateHostsFile.py\\n```\\n\\nThe `updateHostsFile.py` script will generate a unified hosts file based on the\\nsources in the local `data/` subfolder. The script will prompt you whether it\\nshould fetch updated versions (from locations defined by the `update.json` text\\nfile in each source\\'s folder). Otherwise, it will use the `hosts` file that\\'s\\nalready there.\\n\\n```sh\\npython3 updateHostsFile.py [--auto] [--replace] [--ip nnn.nnn.nnn.nnn] [--extensions ext1 ext2 ext3]\\n```\\n\\n#### Command line options\\n\\n`--help`, or `-h`: display help.\\n\\n`--auto`, or `-a`: run the script without prompting. When `--auto` is invoked,\\n\\n- Hosts data sources, including extensions, are updated.\\n- No extensions are included by default. Use the `--extensions` or `-e` flag to\\n  include any you want.\\n- Your active hosts file is _not_ replaced unless you include the `--replace`\\n  flag.\\n\\n`--backup`, or `-b`: Make a backup of existing hosts file(s) as you generate\\nover them.\\n\\n`--extensions <ext1> <ext2> <ext3>`, or `-e <ext1> <ext2> <ext3>`: the names of\\nsubfolders below the `extensions` folder containing additional category-specific\\nhosts files to include in the amalgamation. Example: `--extensions porn` or\\n`-e social porn`.\\n\\n`--flush-dns-cache`, or `-f`: skip the prompt for flushing the DNS cache. Only\\nactive when `--replace` is also active.\\n\\n`--ip nnn.nnn.nnn.nnn`, or `-i nnn.nnn.nnn.nnn`: the IP address to use as the\\ntarget. Default is `0.0.0.0`.\\n\\n`--keepdomaincomments`, or `-k`: `true` (default) or `false`, keep the comments\\nthat appear on the same line as domains. The default is `true`.\\n\\n`--noupdate`, or `-n`: skip fetching updates from hosts data sources.\\n\\n`--output <subfolder>`, or `-o <subfolder>`: place the generated source file in\\na subfolder. If the subfolder does not exist, it will be created.\\n\\n`--replace`, or `-r`: trigger replacing your active hosts\\n\\n`--skipstatichosts`, or `-s`: `false` (default) or `true`, omit the standard\\nsection at the top, containing lines like `127.0.0.1 localhost`. This is useful\\nfor configuring proximate DNS services on the local network.\\n\\n`--nogendata`, or `-g`: `false` (default) or `true`, skip the generation of the\\nreadmeData.json file used for generating readme.md files. This is useful if you\\nare generating host files with additional whitelists or blacklists and want to\\nkeep your local checkout of this repo unmodified.\\n\\n`--compress`, or `-c`: `false` (default) or `true`, _Compress_ the hosts file\\nignoring non-necessary lines (empty lines and comments) and putting multiple\\ndomains in each line. Reducing the number of lines of the hosts file improves\\nthe performances under Windows (with DNS Client service enabled).\\n\\n`--minimise`, or `-m`: `false` (default) or `true`, like `--compress`, but puts\\neach domain on a separate line. This is necessary because many implementations\\nof URL blockers that rely on `hosts` files do not conform to the standard which\\nallows multiple hosts on a single line.\\n\\n`--blacklist <blacklistfile>`, or `-x <blacklistfile>`: Append the given\\nblacklist file in hosts format to the generated hosts file.\\n\\n`--whitelist <whitelistfile>`, or `-w <whitelistfile>`: Use the given whitelist\\nfile to remove hosts from the generated hosts file.\\n\\n## How do I control which sources are unified?\\n\\nAdd one or more _additional_ sources, each in a subfolder of the `data/` folder,\\nand specify the `url` key in its `update.json` file.\\n\\nAdd one or more _optional_ extensions, which originate from subfolders of the\\n`extensions/` folder. Again the url in `update.json` controls where this\\nextension finds its updates.\\n\\nCreate an _optional_ `blacklist` file. The contents of this file (containing a\\nlisting of additional domains in `hosts` file format) are appended to the\\nunified hosts file during the update process. A sample `blacklist` is included,\\nand may be modified as you need.\\n\\n- NOTE: The `blacklist` is not tracked by git, so any changes you make won\\'t be\\n  overridden when you `git pull` this repo from `origin` in the future.\\n\\n### How do I include my own custom domain mappings?\\n\\nIf you have custom hosts records, place them in file `myhosts`. The contents of\\nthis file are prepended to the unified hosts file during the update process.\\n\\nThe `myhosts` file is not tracked by git, so any changes you make won\\'t be\\noverridden when you `git pull` this repo from `origin` in the future.\\n\\n### How do I prevent domains from being included?\\n\\nThe domains you list in the `whitelist` file are excluded from the final hosts\\nfile.\\n\\nThe `whitelist` uses partial matching. Therefore if you whitelist\\n`google-analytics.com`, that domain and all its subdomains won\\'t be merged into\\nthe final hosts file.\\n\\nThe `whitelist` is not tracked by git, so any changes you make won\\'t be\\noverridden when you `git pull` this repo from `origin` in the future.\\n\\n## How can I contribute hosts records?\\n\\nIf you discover sketchy domains you feel should be included here, here are some\\nways to contribute them.\\n\\n### Option 1: contact one of our hosts sources\\n\\nThe best way to get new domains included is to submit an issue to any of the\\ndata providers whose home pages are\\n[listed here](https://github.com/StevenBlack/hosts#sources-of-hosts-data-unified-in-this-variant).\\nThis is best because once you submit new domains, they will be curated and\\nupdated by the dedicated folks who maintain these sources.\\n\\n### Option 2: Fork this repository, add your domains to Steven Black\\'s personal data file, and submit a pull request\\n\\nFork this hosts this repo and add your links to\\n[https://github.com/StevenBlack/hosts/blob/master/data/StevenBlack/hosts](https://github.com/StevenBlack/hosts/blob/master/data/StevenBlack/hosts).\\n\\nThen, submit a pull request.\\n\\n**WARNING**: this is less desirable than Option 1 because the ongoing curation\\nfalls on us. So this creates more work for us.\\n\\n### Option 3: create your own hosts list as a repo on GitHub\\n\\nIf you\\'re able to curate your own collection of sketchy domains, then curate\\nyour own hosts list. Then signal the existence of your repo as\\n[a new issue](https://github.com/StevenBlack/hosts/issues) and we may include\\nyour new repo into the collection of sources we pull whenever we create new\\nversions.\\n\\n## What is a hosts file?\\n\\nA hosts file, named `hosts` (with no file extension), is a plain-text file used\\nby all operating systems to map hostnames to IP addresses.\\n\\nIn most operating systems, the `hosts` file is preferential to `DNS`. Therefore\\nif a domain name is resolved by the `hosts` file, the request never leaves your\\ncomputer.\\n\\nHaving a smart `hosts` file goes a long way towards blocking malware, adware,\\nand other irritants.\\n\\nFor example, to nullify requests to some doubleclick.net servers, adding these\\nlines to your hosts file will do it:\\n\\n```text\\n# block doubleClick\\'s servers\\n0.0.0.0 ad.ae.doubleclick.net\\n0.0.0.0 ad.ar.doubleclick.net\\n0.0.0.0 ad.at.doubleclick.net\\n0.0.0.0 ad.au.doubleclick.net\\n0.0.0.0 ad.be.doubleclick.net\\n# etc...\\n```\\n\\n## We recommend using `0.0.0.0` instead of `127.0.0.1`\\n\\nTraditionally most host files use `127.0.0.1`, the _loopback address_, to\\nestablish an IP connection to the local machine.\\n\\nWe prefer to use `0.0.0.0`, which is defined as a non-routable meta-address used\\nto designate an invalid, unknown, or non-applicable target.\\n\\nUsing `0.0.0.0` is empirically faster, possibly because there\\'s no wait for a\\ntimeout resolution. It also does not interfere with a web server that may be\\nrunning on the local PC.\\n\\n## Why not use `0` instead of `0.0.0.0`?\\n\\nWe tried that. Using `0` doesn\\'t work universally.\\n\\n## Location of your hosts file\\n\\nTo modify your current `hosts` file, look for it in the following places and\\nmodify it with a text editor.\\n\\n- **macOS (until 10.14.x macOS Mojave), iOS, Android, Linux**: `/etc/hosts`\\n  file.\\n- **macOS Catalina:** `/private/etc/hosts` file.\\n- **Windows**: `%SystemRoot%\\\\system32\\\\drivers\\\\etc\\\\hosts` file.\\n\\n## Gentoo\\n\\nGentoo users may find\\n[`sb-hosts`](https://github.com/PF4Public/gentoo-overlay/tree/master/net-misc/sb-hosts)\\nin [::pf4public](https://github.com/PF4Public/gentoo-overlay) Gentoo overlay\\n\\n## NixOS\\n\\nTo install hosts file on your machine add the following into your\\n`configuration.nix`:\\n\\n```nix\\n{\\n  networking.extraHosts = let\\n    hostsPath = https://raw.githubusercontent.com/StevenBlack/hosts/master/hosts;\\n    hostsFile = builtins.fetchurl hostsPath;\\n  in builtins.readFile \"${hostsFile}\";\\n}\\n```\\n\\n- NOTE: Change `hostsPath` if you need other versions of hosts file.\\n- NOTE: The call to `fetchurl` is impure. Use `fetchFromGitHub` with the exact\\n  commit if you want to always get the same result.\\n\\n### Nix Flake\\n\\nNixOS installations which are managed through _flakes_ can use the hosts file\\nlike this:\\n\\n```nix\\n{\\n  inputs.hosts.url = github:StevenBlack/hosts;\\n  outputs = { self, nixpkgs, hosts }: {\\n    nixosConfigurations.my-hostname = {\\n      system = \"<architecture>\";\\n      modules = [\\n        hosts.nixosModule {\\n          networking.stevenBlackHosts.enable = true;\\n        }\\n      ];\\n    };\\n  };\\n}\\n```\\n\\nThe hosts extensions are also available with the following options:\\n\\n```nix\\n{\\n  networking.stevenBlackHosts = {\\n    blockFakenews = true;\\n    blockGambling = true;\\n    blockPorn = true;\\n    blockSocial = true;\\n  };\\n}\\n```\\n\\n## Updating hosts file on Windows\\n\\n(NOTE: See also some third-party Hosts managers, listed below.)\\n\\nOn Linux and macOS, run the Python script. On Windows more work is required due\\nto compatibility issues so it\\'s preferable to run the batch file as follows:\\n\\n```sh\\nupdateHostsWindows.bat\\n```\\n\\nThis file **MUST** be run in command prompt with administrator privileges in the\\nrepository directory. In addition to updating the hosts file, it can also\\nreplace the existing hosts file, and reload the DNS cache. It goes without\\nsaying that for this to work, you must be connected to the internet.\\n\\nTo open a command prompt as administrator in the repository\\'s directory, do the\\nfollowing:\\n\\n- **Windows XP**: Start → Run → `cmd`\\n- **Windows Vista, 7**: Start Button → type `cmd` → right-click Command Prompt →\\n  \"Run as Administrator\"\\n- **Windows 8**: Start → Swipe Up → All Apps → Windows System → right-click\\n  Command Prompt → \"Run as Administrator\"\\n- **Windows 10**: Start Button → type `cmd` → right-click Command Prompt → \"Run\\n  as Administrator\"\\n\\nYou can also refer to the \"Third-Party Hosts Managers\" section for further\\nrecommended solutions from third parties.\\n\\n### Warning: Using this `hosts` file in Windows may require disabling DNS Cache service\\n\\nWindows has issues with larger hosts files. Recent changes in security within\\nWindows 10 denies access to changing services via other tools except registry\\nhacks. Use the `disable-dnscache-service-win.cmd` file to make proper changes to\\nthe Windows registry. You will need to reboot your device once that\\'s done. See\\nthe\\n[the comments within the `cmd` file](https://github.com/StevenBlack/hosts/blob/master/disable-dnscache-service-win.bat)\\nfor more details.\\n\\n## Reloading hosts file\\n\\nYour operating system will cache DNS lookups. You can either reboot or run the\\nfollowing commands to manually flush your DNS cache once the new hosts file is\\nin place.\\n\\nThe Google Chrome browser may require manually cleaning up its DNS Cache on\\n`chrome://net-internals/#dns` page to thereafter see the changes in your hosts\\nfile. See: <https://superuser.com/questions/723703>\\n\\n### Windows\\n\\nOpen a command prompt with administrator privileges and run this command:\\n\\n```bat\\nipconfig /flushdns\\n```\\n\\n### Linux\\n\\nOpen a Terminal and run with root privileges:\\n\\n- **Debian/Ubuntu** `sudo service network-manager restart`\\n- **Linux Mint** `sudo /etc/init.d/dns-clean start`\\n- **Linux with systemd**: `sudo systemctl restart network.service`\\n- **Fedora Linux**: `sudo systemctl restart NetworkManager.service`\\n- **Arch Linux/Manjaro with Network Manager**:\\n  `sudo systemctl restart NetworkManager.service`\\n- **Arch Linux/Manjaro with Wicd**: `sudo systemctl restart wicd.service`\\n- **RHEL/Centos**: `sudo /etc/init.d/network restart`\\n- **FreeBSD**: `sudo service nscd restart`\\n\\n  To enable the `nscd` daemon initially, it is recommended that you run the\\n  following commands:\\n\\n  ```sh\\n  sudo sysrc nscd_enable=\"YES\"\\n  sudo service nscd start\\n  ```\\n\\n  Then modify the `hosts` line in your `/etc/nsswitch.conf` file to the\\n  following:\\n\\n  ```text\\n  hosts: cache files dns\\n  ```\\n\\n- **NixOS**: The `nscd.service` is automatically restarted when the option\\n  `networking.extraHosts` was changed.\\n- **Others**: Consult\\n  [this Wikipedia article](https://en.wikipedia.org/wiki/Hosts_%28file%29#Location_in_the_file_system).\\n\\n### macOS\\n\\nAs described in [this article](https://osxdaily.com/2022/11/21/how-clear-dns-cache-macos-ventura-monterey/),\\nopen a Terminal and run:\\n\\n```sh\\nsudo dscacheutil -flushcache;sudo killall -HUP mDNSResponder\\n```\\n\\n## Release management\\n\\nThis repository uses [release-it](https://github.com/release-it/release-it), an\\nexcellent CLI release tool for GitHub repos and npm packages, to automate\\ncreating [releases](https://github.com/StevenBlack/hosts/releases). This is why\\nthe\\n[package.json](https://github.com/StevenBlack/hosts/blob/master/package.json)\\nand\\n[.release-it.json](https://github.com/StevenBlack/hosts/blob/master/.release-it.json)\\nfiles are bundled.\\n\\n## Goals of this unified hosts file\\n\\nThe goals of this repo are to:\\n\\n1. automatically combine high-quality lists of hosts,\\n2. provide situation-appropriate extensions,\\n3. de-dupe the resultant combined list,\\n4. and keep the resultant file reasonably sized.\\n\\nA high-quality source is defined here as one that is actively curated. A hosts\\nsource should be frequently updated by its maintainers with both additions and\\nremovals. The larger the hosts file, the higher the level of curation is\\nexpected.\\n\\nIt is expected that this unified hosts file will serve both desktop and mobile\\ndevices under a variety of operating systems.\\n\\n## Third-Party Hosts Managers\\n\\n- [Unified Hosts AutoUpdate](https://github.com/ScriptTiger/Unified-Hosts-AutoUpdate \"Unified Hosts AutoUpdate\")\\n  (for Windows): The Unified Hosts AutoUpdate package is purpose-built for this\\n  unified hosts project as well as in active development by community members.\\n  You can install and uninstall any blacklist and keep it automatically up to\\n  date, and can be placed in a shared network location and deployed across an\\n  organization via group policies. And since it is in active development by\\n  community members, your bug reports, feature requests, and other feedback are\\n  most welcome.\\n- [ViHoMa](https://github.com/cmabad/ViHoMa) is a Visual Hosts file Manager,\\n  written in Java, by Christian Martínez. Check it out!\\n\\n## Interesting Applications\\n\\n- [Hosts-BL](https://github.com/ScriptTiger/Hosts-BL \"Hosts-BL\") is a simple\\n  tool to handle hosts file black lists. It can remove comments, remove\\n  duplicates, compress to 9 domains per line, add IPv6 entries. In addition, it\\n  can also convert black lists to multiple other black list formats compatible\\n  with other software, such as dnsmasq, DualServer, RPZ, Privoxy, and Unbound,\\n  to name a few.\\n- [Host Minder](https://github.com/jeremehancock/hostminder#readme) is a simple\\n  GUI that allows you to easily update your /etc/hosts file to one of four\\n  consolidated hosts files from StevenBlack/hosts. It is provided as a deb\\n  package and comes pre-installed on [UbuntuCE](https://ubuntuce.com/).\\n- [Maza ad blocking](https://github.com/tanrax/maza-ad-blocking) is a bash\\n  script that automatically updates host file. You can also update a fresh copy.\\n  And each time it generates a dnsmasq-compatible configuration file. Fast\\n  installation, compatible with MacOS, Linux and BSD.\\n- [Hostile](https://github.com/feross/hostile) is a nifty command line utility\\n  to easily add or remove domains from your hosts file. If our hosts files are\\n  too aggressive for you, you can use `hostile` to remove domains, or you can\\n  use `hostile` in a bash script to automate a post process each time you\\n  download fresh versions of hosts.\\n- [macOS Scripting for Configuration, Backup and Restore](https://github.com/tiiiecherle/osx_install_config)\\n  helps customizing, re-installing and using macOS. It also provides a\\n  [script](https://github.com/tiiiecherle/osx_install_config/blob/master/09_launchd/9b_run_on_boot/root/1_hosts_file/launchd_and_script/hosts_file_generator.sh)\\n  to install and update the hosts file using this project on macOS. In\\n  combination with a\\n  [launchd](https://github.com/tiiiecherle/osx_install_config/blob/master/09_launchd/9b_run_on_boot/root/1_hosts_file/launchd_and_script/com.hostsfile.install_update.plist)\\n  it updates the hosts file every x days (default is 4). To install both,\\n  download the GitHub repo and run the\\n  [install script](https://github.com/tiiiecherle/osx_install_config/blob/master/09_launchd/9b_run_on_boot/root/1_hosts_file/install_hosts_file_generator_and_launchdservice.sh)\\n  from the directory one level up.\\n- [Pi-hole](https://pi-hole.net/) is a network-wide DHCP server and ad blocker\\n  that runs on [Raspberry Pi](https://en.wikipedia.org/wiki/Raspberry_Pi).\\n  Pi-hole uses this repository as one of its sources.\\n- [Block ads and malware via local BIND9 DNS server](https://github.com/mueller-ma/block-ads-via-dns \"Block ads and malware via local DNS server\")\\n  (for Debian, Raspbian & Ubuntu): Set up a local DNS server with a\\n  `/etc/bind/named.conf.blocked` file, sourced from here.\\n- [Block ads, malware, and deploy parental controls via local DualServer DNS/DHCP server](https://scripttiger.github.io/dualserver/ \"Block ads, malware, and deploy parental controls via local DualServer DNS/DHCP server\")\\n  (for BSD, Windows & Linux): Set up a blacklist for everyone on your network\\n  using the power of the unified hosts reformatted for DualServer. And if you\\'re\\n  on Windows, this project also maintains an update script to make updating\\n  DualServer\\'s blacklist even easier.\\n- [Blocking ads and malwares with unbound](https://deadc0de.re/articles/unbound-blocking-ads.html \"Blocking ads and malwares with unbound\")\\n  –\\n  [Unbound](https://www.unbound.net/ \"Unbound is a validating, recursive, and caching DNS resolver.\")\\n  is a validating, recursive, and caching DNS resolver.\\n- [dnsmasq conversion script](https://gist.github.com/erlepereira/c11f4f7a3f60cd2071e79018e895fc8a#file-dnsmasq-antimalware)\\n  This GitHub gist has a short shell script (bash, will work on any \\'nix) and\\n  uses `wget` & `awk` present in most distros, to fetch a specified hosts file\\n  and convert it to the format required by dnsmasq. Supports IPv4 and IPv6.\\n  Designed to be used as either a shell script, or can be dropped into\\n  `/etc/cron.weekly` (or wherever suits). The script is short and easily edited,\\n  also has a short document attached with notes on dnsmasq setup.\\n- [BlackHosts - Command Line Installer/Updater](https://github.com/Lateralus138/blackhosts)\\n  This is a cross-platform command line utility to help install/update hosts\\n  files found at this repository.\\n- [dnscrypt-proxy](https://github.com/DNSCrypt/dnscrypt-proxy/wiki/Combining-Blocklists)\\n  provides a tool to build block lists from local and remote lists in common\\n  formats.\\n- [Control D](https://controld.com/free-dns)\\n  offers a public anycast network hosted mirror of the Unified (Adware + Malware) blocklist:\\n  - Legacy DNS: `76.76.2.35`, `76.76.10.35`, `2606:1a40::35`, `2606:1a40:1::35`\\n  - DNS-over-HTTPS/TLS/DOQ: `https://freedns.controld.com/x-stevenblack`, `x-stevenblack.freedns.controld.com`\\n\\n## Contribute\\n\\nPlease read our\\n[Contributing Guide](https://github.com/StevenBlack/hosts/blob/master/contributing.md).\\nAmong other things, this explains how we organize files and folders in this\\nrepository.\\n\\nWe are always interested in discovering well-curated sources of hosts. If you\\nfind one, please open an [issue](https://github.com/StevenBlack/hosts/issues) to\\ndraw our attention.\\n\\nBefore you create or respond to any issue, please read our\\n[code of conduct](https://github.com/StevenBlack/hosts/blob/master/code_of_conduct.md).\\n\\nLogo by [@Tobaloidee](https://github.com/Tobaloidee) Thank you!.\\n'},\n",
       " {'repo': './riffusion/riffusion',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '# :guitar: Riffusion\\n\\n<a href=\"https://github.com/riffusion/riffusion/actions/workflows/ci.yml?query=branch%3Amain\"><img alt=\"CI status\" src=\"https://github.com/riffusion/riffusion/actions/workflows/ci.yml/badge.svg\" /></a>\\n<img alt=\"Python 3.9 | 3.10\" src=\"https://img.shields.io/badge/Python-3.9%20%7C%203.10-blue\" />\\n<a href=\"https://github.com/riffusion/riffusion/tree/main/LICENSE\"><img alt=\"MIT License\" src=\"https://img.shields.io/badge/License-MIT-yellowgreen\" /></a>\\n\\nRiffusion is a library for real-time music and audio generation with stable diffusion.\\n\\nRead about it at https://www.riffusion.com/about and try it at https://www.riffusion.com/.\\n\\nThis is the core repository for riffusion image and audio processing code.\\n\\n * Diffusion pipeline that performs prompt interpolation combined with image conditioning\\n * Conversions between spectrogram images and audio clips\\n * Command-line interface for common tasks\\n * Interactive app using streamlit\\n * Flask server to provide model inference via API\\n * Various third party integrations\\n\\nRelated repositories:\\n* Web app: https://github.com/riffusion/riffusion-app\\n* Model checkpoint: https://huggingface.co/riffusion/riffusion-model-v1\\n\\n## Citation\\n\\nIf you build on this work, please cite it as follows:\\n\\n```\\n@article{Forsgren_Martiros_2022,\\n  author = {Forsgren, Seth* and Martiros, Hayk*},\\n  title = {{Riffusion - Stable diffusion for real-time music generation}},\\n  url = {https://riffusion.com/about},\\n  year = {2022}\\n}\\n```\\n\\n## Install\\n\\nTested in CI with Python 3.9 and 3.10.\\n\\nIt\\'s highly recommended to set up a virtual Python environment with `conda` or `virtualenv`:\\n```\\nconda create --name riffusion python=3.9\\nconda activate riffusion\\n```\\n\\nInstall Python dependencies:\\n```\\npython -m pip install -r requirements.txt\\n```\\n\\nIn order to use audio formats other than WAV, [ffmpeg](https://ffmpeg.org/download.html) is required.\\n```\\nsudo apt-get install ffmpeg          # linux\\nbrew install ffmpeg                  # mac\\nconda install -c conda-forge ffmpeg  # conda\\n```\\n\\nIf torchaudio has no backend, you may need to install `libsndfile`. See [this issue](https://github.com/riffusion/riffusion/issues/12).\\n\\nIf you have an issue, try upgrading [diffusers](https://github.com/huggingface/diffusers). Tested with 0.9 - 0.11.\\n\\nGuides:\\n* [Simple Install Guide for Windows](https://www.reddit.com/r/riffusion/comments/zrubc9/installation_guide_for_riffusion_app_inference/)\\n\\n## Backends\\n\\n### CPU\\n`cpu` is supported but is quite slow.\\n\\n### CUDA\\n`cuda` is the recommended and most performant backend.\\n\\nTo use with CUDA, make sure you have torch and torchaudio installed with CUDA support. See the\\n[install guide](https://pytorch.org/get-started/locally/) or\\n[stable wheels](https://download.pytorch.org/whl/torch_stable.html).\\n\\nTo generate audio in real-time, you need a GPU that can run stable diffusion with approximately 50\\nsteps in under five seconds, such as a 3090 or A10G.\\n\\nTest availability with:\\n\\n```python3\\nimport torch\\ntorch.cuda.is_available()\\n```\\n\\n### MPS\\nThe `mps` backend on Apple Silicon is supported for inference but some operations fall back to CPU,\\nparticularly for audio processing. You may need to set\\n`PYTORCH_ENABLE_MPS_FALLBACK=1`.\\n\\nIn addition, this backend is not deterministic.\\n\\nTest availability with:\\n\\n```python3\\nimport torch\\ntorch.backends.mps.is_available()\\n```\\n\\n## Command-line interface\\n\\nRiffusion comes with a command line interface for performing common tasks.\\n\\nSee available commands:\\n```\\npython -m riffusion.cli -h\\n```\\n\\nGet help for a specific command:\\n```\\npython -m riffusion.cli image-to-audio -h\\n```\\n\\nExecute:\\n```\\npython -m riffusion.cli image-to-audio --image spectrogram_image.png --audio clip.wav\\n```\\n\\n## Riffusion Playground\\n\\nRiffusion contains a [streamlit](https://streamlit.io/) app for interactive use and exploration.\\n\\nRun with:\\n```\\npython -m riffusion.streamlit.playground\\n```\\n\\nAnd access at http://127.0.0.1:8501/\\n\\n<img alt=\"Riffusion Playground\" style=\"width: 600px\" src=\"https://i.imgur.com/OOMKBbT.png\" />\\n\\n## Run the model server\\n\\nRiffusion can be run as a flask server that provides inference via API. This server enables the [web app](https://github.com/riffusion/riffusion-app) to run locally.\\n\\nRun with:\\n\\n```\\npython -m riffusion.server --host 127.0.0.1 --port 3013\\n```\\n\\nYou can specify `--checkpoint` with your own directory or huggingface ID in diffusers format.\\n\\nUse the `--device` argument to specify the torch device to use.\\n\\nThe model endpoint is now available at `http://127.0.0.1:3013/run_inference` via POST request.\\n\\nExample input (see [InferenceInput](https://github.com/hmartiro/riffusion-inference/blob/main/riffusion/datatypes.py#L28) for the API):\\n```\\n{\\n  \"alpha\": 0.75,\\n  \"num_inference_steps\": 50,\\n  \"seed_image_id\": \"og_beat\",\\n\\n  \"start\": {\\n    \"prompt\": \"church bells on sunday\",\\n    \"seed\": 42,\\n    \"denoising\": 0.75,\\n    \"guidance\": 7.0\\n  },\\n\\n  \"end\": {\\n    \"prompt\": \"jazz with piano\",\\n    \"seed\": 123,\\n    \"denoising\": 0.75,\\n    \"guidance\": 7.0\\n  }\\n}\\n```\\n\\nExample output (see [InferenceOutput](https://github.com/hmartiro/riffusion-inference/blob/main/riffusion/datatypes.py#L54) for the API):\\n```\\n{\\n  \"image\": \"< base64 encoded JPEG image >\",\\n  \"audio\": \"< base64 encoded MP3 clip >\"\\n}\\n```\\n\\n## Tests\\nTests live in the `test/` directory and are implemented with `unittest`.\\n\\nTo run all tests:\\n```\\npython -m unittest test/*_test.py\\n```\\n\\nTo run a single test:\\n```\\npython -m unittest test.audio_to_image_test\\n```\\n\\nTo preserve temporary outputs for debugging, set `RIFFUSION_TEST_DEBUG`:\\n```\\nRIFFUSION_TEST_DEBUG=1 python -m unittest test.audio_to_image_test\\n```\\n\\nTo run a single test case within a test:\\n```\\npython -m unittest test.audio_to_image_test -k AudioToImageTest.test_stereo\\n```\\n\\nTo run tests using a specific torch device, set `RIFFUSION_TEST_DEVICE`. Tests should pass with\\n`cpu`, `cuda`, and `mps` backends.\\n\\n## Development Guide\\nInstall additional packages for dev with `python -m pip install -r requirements_dev.txt`.\\n\\n* Linter: `ruff`\\n* Formatter: `black`\\n* Type checker: `mypy`\\n\\nThese are configured in `pyproject.toml`.\\n\\nThe results of `mypy .`, `black .`, and `ruff .` *must* be clean to accept a PR.\\n\\nCI is run through GitHub Actions from `.github/workflows/ci.yml`.\\n\\nContributions are welcome through pull requests.\\n'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.scrape_github_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ffb45040-dfc0-4ce1-9c50-5be9a89dc691",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': '.dockerignore',\n",
       "  'path': '.dockerignore',\n",
       "  'sha': 'c69283ec552ca5f4615060c6aabcc5607d6d6962',\n",
       "  'size': 54,\n",
       "  'url': 'https://api.github.com/repos/huggingface/text-generation-inference/contents/.dockerignore?ref=main',\n",
       "  'html_url': 'https://github.com/huggingface/text-generation-inference/blob/main/.dockerignore',\n",
       "  'git_url': 'https://api.github.com/repos/huggingface/text-generation-inference/git/blobs/c69283ec552ca5f4615060c6aabcc5607d6d6962',\n",
       "  'download_url': 'https://raw.githubusercontent.com/huggingface/text-generation-inference/main/.dockerignore',\n",
       "  'type': 'file',\n",
       "  '_links': {'self': 'https://api.github.com/repos/huggingface/text-generation-inference/contents/.dockerignore?ref=main',\n",
       "   'git': 'https://api.github.com/repos/huggingface/text-generation-inference/git/blobs/c69283ec552ca5f4615060c6aabcc5607d6d6962',\n",
       "   'html': 'https://github.com/huggingface/text-generation-inference/blob/main/.dockerignore'}},\n",
       " {'name': '.github',\n",
       "  'path': '.github',\n",
       "  'sha': '50f7555ba3c9f9d9942d473edf0542eefe2d188e',\n",
       "  'size': 0,\n",
       "  'url': 'https://api.github.com/repos/huggingface/text-generation-inference/contents/.github?ref=main',\n",
       "  'html_url': 'https://github.com/huggingface/text-generation-inference/tree/main/.github',\n",
       "  'git_url': 'https://api.github.com/repos/huggingface/text-generation-inference/git/trees/50f7555ba3c9f9d9942d473edf0542eefe2d188e',\n",
       "  'download_url': None,\n",
       "  'type': 'dir',\n",
       "  '_links': {'self': 'https://api.github.com/repos/huggingface/text-generation-inference/contents/.github?ref=main',\n",
       "   'git': 'https://api.github.com/repos/huggingface/text-generation-inference/git/trees/50f7555ba3c9f9d9942d473edf0542eefe2d188e',\n",
       "   'html': 'https://github.com/huggingface/text-generation-inference/tree/main/.github'}},\n",
       " {'name': '.gitignore',\n",
       "  'path': '.gitignore',\n",
       "  'sha': '19604d4267793bfd516f312cceaf50555041f31f',\n",
       "  'size': 35,\n",
       "  'url': 'https://api.github.com/repos/huggingface/text-generation-inference/contents/.gitignore?ref=main',\n",
       "  'html_url': 'https://github.com/huggingface/text-generation-inference/blob/main/.gitignore',\n",
       "  'git_url': 'https://api.github.com/repos/huggingface/text-generation-inference/git/blobs/19604d4267793bfd516f312cceaf50555041f31f',\n",
       "  'download_url': 'https://raw.githubusercontent.com/huggingface/text-generation-inference/main/.gitignore',\n",
       "  'type': 'file',\n",
       "  '_links': {'self': 'https://api.github.com/repos/huggingface/text-generation-inference/contents/.gitignore?ref=main',\n",
       "   'git': 'https://api.github.com/repos/huggingface/text-generation-inference/git/blobs/19604d4267793bfd516f312cceaf50555041f31f',\n",
       "   'html': 'https://github.com/huggingface/text-generation-inference/blob/main/.gitignore'}},\n",
       " {'name': 'Cargo.lock',\n",
       "  'path': 'Cargo.lock',\n",
       "  'sha': '48595077fe5d1de845678d713c8c598198e70c17',\n",
       "  'size': 86174,\n",
       "  'url': 'https://api.github.com/repos/huggingface/text-generation-inference/contents/Cargo.lock?ref=main',\n",
       "  'html_url': 'https://github.com/huggingface/text-generation-inference/blob/main/Cargo.lock',\n",
       "  'git_url': 'https://api.github.com/repos/huggingface/text-generation-inference/git/blobs/48595077fe5d1de845678d713c8c598198e70c17',\n",
       "  'download_url': 'https://raw.githubusercontent.com/huggingface/text-generation-inference/main/Cargo.lock',\n",
       "  'type': 'file',\n",
       "  '_links': {'self': 'https://api.github.com/repos/huggingface/text-generation-inference/contents/Cargo.lock?ref=main',\n",
       "   'git': 'https://api.github.com/repos/huggingface/text-generation-inference/git/blobs/48595077fe5d1de845678d713c8c598198e70c17',\n",
       "   'html': 'https://github.com/huggingface/text-generation-inference/blob/main/Cargo.lock'}},\n",
       " {'name': 'Cargo.toml',\n",
       "  'path': 'Cargo.toml',\n",
       "  'sha': 'd34b10bfd5965c9f76a01f2a98c8dc955d904e2a',\n",
       "  'size': 357,\n",
       "  'url': 'https://api.github.com/repos/huggingface/text-generation-inference/contents/Cargo.toml?ref=main',\n",
       "  'html_url': 'https://github.com/huggingface/text-generation-inference/blob/main/Cargo.toml',\n",
       "  'git_url': 'https://api.github.com/repos/huggingface/text-generation-inference/git/blobs/d34b10bfd5965c9f76a01f2a98c8dc955d904e2a',\n",
       "  'download_url': 'https://raw.githubusercontent.com/huggingface/text-generation-inference/main/Cargo.toml',\n",
       "  'type': 'file',\n",
       "  '_links': {'self': 'https://api.github.com/repos/huggingface/text-generation-inference/contents/Cargo.toml?ref=main',\n",
       "   'git': 'https://api.github.com/repos/huggingface/text-generation-inference/git/blobs/d34b10bfd5965c9f76a01f2a98c8dc955d904e2a',\n",
       "   'html': 'https://github.com/huggingface/text-generation-inference/blob/main/Cargo.toml'}},\n",
       " {'name': 'Dockerfile',\n",
       "  'path': 'Dockerfile',\n",
       "  'sha': '483270a88268faeb28f34fa95b461002674a1bc6',\n",
       "  'size': 5610,\n",
       "  'url': 'https://api.github.com/repos/huggingface/text-generation-inference/contents/Dockerfile?ref=main',\n",
       "  'html_url': 'https://github.com/huggingface/text-generation-inference/blob/main/Dockerfile',\n",
       "  'git_url': 'https://api.github.com/repos/huggingface/text-generation-inference/git/blobs/483270a88268faeb28f34fa95b461002674a1bc6',\n",
       "  'download_url': 'https://raw.githubusercontent.com/huggingface/text-generation-inference/main/Dockerfile',\n",
       "  'type': 'file',\n",
       "  '_links': {'self': 'https://api.github.com/repos/huggingface/text-generation-inference/contents/Dockerfile?ref=main',\n",
       "   'git': 'https://api.github.com/repos/huggingface/text-generation-inference/git/blobs/483270a88268faeb28f34fa95b461002674a1bc6',\n",
       "   'html': 'https://github.com/huggingface/text-generation-inference/blob/main/Dockerfile'}},\n",
       " {'name': 'LICENSE',\n",
       "  'path': 'LICENSE',\n",
       "  'sha': '7d0e80345c787d5080678c89b3834b50e399c3f5',\n",
       "  'size': 11342,\n",
       "  'url': 'https://api.github.com/repos/huggingface/text-generation-inference/contents/LICENSE?ref=main',\n",
       "  'html_url': 'https://github.com/huggingface/text-generation-inference/blob/main/LICENSE',\n",
       "  'git_url': 'https://api.github.com/repos/huggingface/text-generation-inference/git/blobs/7d0e80345c787d5080678c89b3834b50e399c3f5',\n",
       "  'download_url': 'https://raw.githubusercontent.com/huggingface/text-generation-inference/main/LICENSE',\n",
       "  'type': 'file',\n",
       "  '_links': {'self': 'https://api.github.com/repos/huggingface/text-generation-inference/contents/LICENSE?ref=main',\n",
       "   'git': 'https://api.github.com/repos/huggingface/text-generation-inference/git/blobs/7d0e80345c787d5080678c89b3834b50e399c3f5',\n",
       "   'html': 'https://github.com/huggingface/text-generation-inference/blob/main/LICENSE'}},\n",
       " {'name': 'Makefile',\n",
       "  'path': 'Makefile',\n",
       "  'sha': '032a49de44d806ff459698a1662e66eab23412c1',\n",
       "  'size': 1135,\n",
       "  'url': 'https://api.github.com/repos/huggingface/text-generation-inference/contents/Makefile?ref=main',\n",
       "  'html_url': 'https://github.com/huggingface/text-generation-inference/blob/main/Makefile',\n",
       "  'git_url': 'https://api.github.com/repos/huggingface/text-generation-inference/git/blobs/032a49de44d806ff459698a1662e66eab23412c1',\n",
       "  'download_url': 'https://raw.githubusercontent.com/huggingface/text-generation-inference/main/Makefile',\n",
       "  'type': 'file',\n",
       "  '_links': {'self': 'https://api.github.com/repos/huggingface/text-generation-inference/contents/Makefile?ref=main',\n",
       "   'git': 'https://api.github.com/repos/huggingface/text-generation-inference/git/blobs/032a49de44d806ff459698a1662e66eab23412c1',\n",
       "   'html': 'https://github.com/huggingface/text-generation-inference/blob/main/Makefile'}},\n",
       " {'name': 'README.md',\n",
       "  'path': 'README.md',\n",
       "  'sha': '756d7e35e7a9e220d765615f805b66de02ab2748',\n",
       "  'size': 8261,\n",
       "  'url': 'https://api.github.com/repos/huggingface/text-generation-inference/contents/README.md?ref=main',\n",
       "  'html_url': 'https://github.com/huggingface/text-generation-inference/blob/main/README.md',\n",
       "  'git_url': 'https://api.github.com/repos/huggingface/text-generation-inference/git/blobs/756d7e35e7a9e220d765615f805b66de02ab2748',\n",
       "  'download_url': 'https://raw.githubusercontent.com/huggingface/text-generation-inference/main/README.md',\n",
       "  'type': 'file',\n",
       "  '_links': {'self': 'https://api.github.com/repos/huggingface/text-generation-inference/contents/README.md?ref=main',\n",
       "   'git': 'https://api.github.com/repos/huggingface/text-generation-inference/git/blobs/756d7e35e7a9e220d765615f805b66de02ab2748',\n",
       "   'html': 'https://github.com/huggingface/text-generation-inference/blob/main/README.md'}},\n",
       " {'name': 'aml',\n",
       "  'path': 'aml',\n",
       "  'sha': '22465090b980e3af47cb0ab2971821cafbf9d59e',\n",
       "  'size': 0,\n",
       "  'url': 'https://api.github.com/repos/huggingface/text-generation-inference/contents/aml?ref=main',\n",
       "  'html_url': 'https://github.com/huggingface/text-generation-inference/tree/main/aml',\n",
       "  'git_url': 'https://api.github.com/repos/huggingface/text-generation-inference/git/trees/22465090b980e3af47cb0ab2971821cafbf9d59e',\n",
       "  'download_url': None,\n",
       "  'type': 'dir',\n",
       "  '_links': {'self': 'https://api.github.com/repos/huggingface/text-generation-inference/contents/aml?ref=main',\n",
       "   'git': 'https://api.github.com/repos/huggingface/text-generation-inference/git/trees/22465090b980e3af47cb0ab2971821cafbf9d59e',\n",
       "   'html': 'https://github.com/huggingface/text-generation-inference/tree/main/aml'}},\n",
       " {'name': 'assets',\n",
       "  'path': 'assets',\n",
       "  'sha': '5b3c530a2ba67bf5dec74769f83ffe3b9ea4abad',\n",
       "  'size': 0,\n",
       "  'url': 'https://api.github.com/repos/huggingface/text-generation-inference/contents/assets?ref=main',\n",
       "  'html_url': 'https://github.com/huggingface/text-generation-inference/tree/main/assets',\n",
       "  'git_url': 'https://api.github.com/repos/huggingface/text-generation-inference/git/trees/5b3c530a2ba67bf5dec74769f83ffe3b9ea4abad',\n",
       "  'download_url': None,\n",
       "  'type': 'dir',\n",
       "  '_links': {'self': 'https://api.github.com/repos/huggingface/text-generation-inference/contents/assets?ref=main',\n",
       "   'git': 'https://api.github.com/repos/huggingface/text-generation-inference/git/trees/5b3c530a2ba67bf5dec74769f83ffe3b9ea4abad',\n",
       "   'html': 'https://github.com/huggingface/text-generation-inference/tree/main/assets'}},\n",
       " {'name': 'benchmark',\n",
       "  'path': 'benchmark',\n",
       "  'sha': 'f8b19b6dee415c81a6607d6b1d6c4a9e18cbc6b5',\n",
       "  'size': 0,\n",
       "  'url': 'https://api.github.com/repos/huggingface/text-generation-inference/contents/benchmark?ref=main',\n",
       "  'html_url': 'https://github.com/huggingface/text-generation-inference/tree/main/benchmark',\n",
       "  'git_url': 'https://api.github.com/repos/huggingface/text-generation-inference/git/trees/f8b19b6dee415c81a6607d6b1d6c4a9e18cbc6b5',\n",
       "  'download_url': None,\n",
       "  'type': 'dir',\n",
       "  '_links': {'self': 'https://api.github.com/repos/huggingface/text-generation-inference/contents/benchmark?ref=main',\n",
       "   'git': 'https://api.github.com/repos/huggingface/text-generation-inference/git/trees/f8b19b6dee415c81a6607d6b1d6c4a9e18cbc6b5',\n",
       "   'html': 'https://github.com/huggingface/text-generation-inference/tree/main/benchmark'}},\n",
       " {'name': 'clients',\n",
       "  'path': 'clients',\n",
       "  'sha': 'a25d2550933bea8aaceb314b501ce4605a597e2a',\n",
       "  'size': 0,\n",
       "  'url': 'https://api.github.com/repos/huggingface/text-generation-inference/contents/clients?ref=main',\n",
       "  'html_url': 'https://github.com/huggingface/text-generation-inference/tree/main/clients',\n",
       "  'git_url': 'https://api.github.com/repos/huggingface/text-generation-inference/git/trees/a25d2550933bea8aaceb314b501ce4605a597e2a',\n",
       "  'download_url': None,\n",
       "  'type': 'dir',\n",
       "  '_links': {'self': 'https://api.github.com/repos/huggingface/text-generation-inference/contents/clients?ref=main',\n",
       "   'git': 'https://api.github.com/repos/huggingface/text-generation-inference/git/trees/a25d2550933bea8aaceb314b501ce4605a597e2a',\n",
       "   'html': 'https://github.com/huggingface/text-generation-inference/tree/main/clients'}},\n",
       " {'name': 'docs',\n",
       "  'path': 'docs',\n",
       "  'sha': 'd5c6c53148901be15410592fa4f6e3a3ef41a966',\n",
       "  'size': 0,\n",
       "  'url': 'https://api.github.com/repos/huggingface/text-generation-inference/contents/docs?ref=main',\n",
       "  'html_url': 'https://github.com/huggingface/text-generation-inference/tree/main/docs',\n",
       "  'git_url': 'https://api.github.com/repos/huggingface/text-generation-inference/git/trees/d5c6c53148901be15410592fa4f6e3a3ef41a966',\n",
       "  'download_url': None,\n",
       "  'type': 'dir',\n",
       "  '_links': {'self': 'https://api.github.com/repos/huggingface/text-generation-inference/contents/docs?ref=main',\n",
       "   'git': 'https://api.github.com/repos/huggingface/text-generation-inference/git/trees/d5c6c53148901be15410592fa4f6e3a3ef41a966',\n",
       "   'html': 'https://github.com/huggingface/text-generation-inference/tree/main/docs'}},\n",
       " {'name': 'k6',\n",
       "  'path': 'k6',\n",
       "  'sha': '4eab4f328a7fc8174859b42d01fccea52c0388ac',\n",
       "  'size': 0,\n",
       "  'url': 'https://api.github.com/repos/huggingface/text-generation-inference/contents/k6?ref=main',\n",
       "  'html_url': 'https://github.com/huggingface/text-generation-inference/tree/main/k6',\n",
       "  'git_url': 'https://api.github.com/repos/huggingface/text-generation-inference/git/trees/4eab4f328a7fc8174859b42d01fccea52c0388ac',\n",
       "  'download_url': None,\n",
       "  'type': 'dir',\n",
       "  '_links': {'self': 'https://api.github.com/repos/huggingface/text-generation-inference/contents/k6?ref=main',\n",
       "   'git': 'https://api.github.com/repos/huggingface/text-generation-inference/git/trees/4eab4f328a7fc8174859b42d01fccea52c0388ac',\n",
       "   'html': 'https://github.com/huggingface/text-generation-inference/tree/main/k6'}},\n",
       " {'name': 'launcher',\n",
       "  'path': 'launcher',\n",
       "  'sha': '45ab8b34d43727b4ffabb2df307c5a1f87921bda',\n",
       "  'size': 0,\n",
       "  'url': 'https://api.github.com/repos/huggingface/text-generation-inference/contents/launcher?ref=main',\n",
       "  'html_url': 'https://github.com/huggingface/text-generation-inference/tree/main/launcher',\n",
       "  'git_url': 'https://api.github.com/repos/huggingface/text-generation-inference/git/trees/45ab8b34d43727b4ffabb2df307c5a1f87921bda',\n",
       "  'download_url': None,\n",
       "  'type': 'dir',\n",
       "  '_links': {'self': 'https://api.github.com/repos/huggingface/text-generation-inference/contents/launcher?ref=main',\n",
       "   'git': 'https://api.github.com/repos/huggingface/text-generation-inference/git/trees/45ab8b34d43727b4ffabb2df307c5a1f87921bda',\n",
       "   'html': 'https://github.com/huggingface/text-generation-inference/tree/main/launcher'}},\n",
       " {'name': 'proto',\n",
       "  'path': 'proto',\n",
       "  'sha': '20841948a747a65ff8b289b165b2a2e079157890',\n",
       "  'size': 0,\n",
       "  'url': 'https://api.github.com/repos/huggingface/text-generation-inference/contents/proto?ref=main',\n",
       "  'html_url': 'https://github.com/huggingface/text-generation-inference/tree/main/proto',\n",
       "  'git_url': 'https://api.github.com/repos/huggingface/text-generation-inference/git/trees/20841948a747a65ff8b289b165b2a2e079157890',\n",
       "  'download_url': None,\n",
       "  'type': 'dir',\n",
       "  '_links': {'self': 'https://api.github.com/repos/huggingface/text-generation-inference/contents/proto?ref=main',\n",
       "   'git': 'https://api.github.com/repos/huggingface/text-generation-inference/git/trees/20841948a747a65ff8b289b165b2a2e079157890',\n",
       "   'html': 'https://github.com/huggingface/text-generation-inference/tree/main/proto'}},\n",
       " {'name': 'router',\n",
       "  'path': 'router',\n",
       "  'sha': '40220f4fe246a8fe51a1f7ee80e48481fbf6eba0',\n",
       "  'size': 0,\n",
       "  'url': 'https://api.github.com/repos/huggingface/text-generation-inference/contents/router?ref=main',\n",
       "  'html_url': 'https://github.com/huggingface/text-generation-inference/tree/main/router',\n",
       "  'git_url': 'https://api.github.com/repos/huggingface/text-generation-inference/git/trees/40220f4fe246a8fe51a1f7ee80e48481fbf6eba0',\n",
       "  'download_url': None,\n",
       "  'type': 'dir',\n",
       "  '_links': {'self': 'https://api.github.com/repos/huggingface/text-generation-inference/contents/router?ref=main',\n",
       "   'git': 'https://api.github.com/repos/huggingface/text-generation-inference/git/trees/40220f4fe246a8fe51a1f7ee80e48481fbf6eba0',\n",
       "   'html': 'https://github.com/huggingface/text-generation-inference/tree/main/router'}},\n",
       " {'name': 'rust-toolchain.toml',\n",
       "  'path': 'rust-toolchain.toml',\n",
       "  'sha': 'd6ecf0c44b6f0bdc63681eef5e2dd8f1781ed140',\n",
       "  'size': 65,\n",
       "  'url': 'https://api.github.com/repos/huggingface/text-generation-inference/contents/rust-toolchain.toml?ref=main',\n",
       "  'html_url': 'https://github.com/huggingface/text-generation-inference/blob/main/rust-toolchain.toml',\n",
       "  'git_url': 'https://api.github.com/repos/huggingface/text-generation-inference/git/blobs/d6ecf0c44b6f0bdc63681eef5e2dd8f1781ed140',\n",
       "  'download_url': 'https://raw.githubusercontent.com/huggingface/text-generation-inference/main/rust-toolchain.toml',\n",
       "  'type': 'file',\n",
       "  '_links': {'self': 'https://api.github.com/repos/huggingface/text-generation-inference/contents/rust-toolchain.toml?ref=main',\n",
       "   'git': 'https://api.github.com/repos/huggingface/text-generation-inference/git/blobs/d6ecf0c44b6f0bdc63681eef5e2dd8f1781ed140',\n",
       "   'html': 'https://github.com/huggingface/text-generation-inference/blob/main/rust-toolchain.toml'}},\n",
       " {'name': 'sagemaker-entrypoint.sh',\n",
       "  'path': 'sagemaker-entrypoint.sh',\n",
       "  'sha': '711e3721b300b4279306bdc1a9377e5c420ec498',\n",
       "  'size': 377,\n",
       "  'url': 'https://api.github.com/repos/huggingface/text-generation-inference/contents/sagemaker-entrypoint.sh?ref=main',\n",
       "  'html_url': 'https://github.com/huggingface/text-generation-inference/blob/main/sagemaker-entrypoint.sh',\n",
       "  'git_url': 'https://api.github.com/repos/huggingface/text-generation-inference/git/blobs/711e3721b300b4279306bdc1a9377e5c420ec498',\n",
       "  'download_url': 'https://raw.githubusercontent.com/huggingface/text-generation-inference/main/sagemaker-entrypoint.sh',\n",
       "  'type': 'file',\n",
       "  '_links': {'self': 'https://api.github.com/repos/huggingface/text-generation-inference/contents/sagemaker-entrypoint.sh?ref=main',\n",
       "   'git': 'https://api.github.com/repos/huggingface/text-generation-inference/git/blobs/711e3721b300b4279306bdc1a9377e5c420ec498',\n",
       "   'html': 'https://github.com/huggingface/text-generation-inference/blob/main/sagemaker-entrypoint.sh'}},\n",
       " {'name': 'server',\n",
       "  'path': 'server',\n",
       "  'sha': '0dfc41696489049d98563b1c21632336dd0c7e04',\n",
       "  'size': 0,\n",
       "  'url': 'https://api.github.com/repos/huggingface/text-generation-inference/contents/server?ref=main',\n",
       "  'html_url': 'https://github.com/huggingface/text-generation-inference/tree/main/server',\n",
       "  'git_url': 'https://api.github.com/repos/huggingface/text-generation-inference/git/trees/0dfc41696489049d98563b1c21632336dd0c7e04',\n",
       "  'download_url': None,\n",
       "  'type': 'dir',\n",
       "  '_links': {'self': 'https://api.github.com/repos/huggingface/text-generation-inference/contents/server?ref=main',\n",
       "   'git': 'https://api.github.com/repos/huggingface/text-generation-inference/git/trees/0dfc41696489049d98563b1c21632336dd0c7e04',\n",
       "   'html': 'https://github.com/huggingface/text-generation-inference/tree/main/server'}},\n",
       " {'name': 'supported_models.json',\n",
       "  'path': 'supported_models.json',\n",
       "  'sha': '8e9dd9e03bdc410e66b637ac535de17b5ce8b776',\n",
       "  'size': 188,\n",
       "  'url': 'https://api.github.com/repos/huggingface/text-generation-inference/contents/supported_models.json?ref=main',\n",
       "  'html_url': 'https://github.com/huggingface/text-generation-inference/blob/main/supported_models.json',\n",
       "  'git_url': 'https://api.github.com/repos/huggingface/text-generation-inference/git/blobs/8e9dd9e03bdc410e66b637ac535de17b5ce8b776',\n",
       "  'download_url': 'https://raw.githubusercontent.com/huggingface/text-generation-inference/main/supported_models.json',\n",
       "  'type': 'file',\n",
       "  '_links': {'self': 'https://api.github.com/repos/huggingface/text-generation-inference/contents/supported_models.json?ref=main',\n",
       "   'git': 'https://api.github.com/repos/huggingface/text-generation-inference/git/blobs/8e9dd9e03bdc410e66b637ac535de17b5ce8b776',\n",
       "   'html': 'https://github.com/huggingface/text-generation-inference/blob/main/supported_models.json'}}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.get_repo_contents('./huggingface/text-generation-inference')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b753f4c-a5a5-4e69-b3f7-59f9bc5afbc9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
